{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "# from dataloader import create_dataloaders\n",
    "from dataloader_ER import create_dataloaders,create_dataloaders_WebTable\n",
    "# from lora_model import LORAEngine,LORAEngineDeberta,LORAEngineDebertaMultiClass\n",
    "from lora_model_CTA import LORAEngineDebertaMultiClass\n",
    "\n",
    "# from influence import IFEngine\n",
    "from influence_hyperinf import IFEngine\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path=\"roberta-large\"\n",
    "# model_name_or_path=\"/home/yanmy/roberta-base\"\n",
    "model_name_or_path=\"../../model/deberta-v3-base\"\n",
    "# model_name_or_path=\"../../model/roberta-large\"\n",
    "task=\"WebTable\"\n",
    "# task = \"qnli\"\n",
    "noise_ratio=0\n",
    "batch_size=64\n",
    "# target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "# target_modules = ['value']\n",
    "target_modules=[\"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "\n",
    "device=\"cuda:7\"\n",
    "num_epochs=10\n",
    "lr=3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f176063e6a774a8da2cd4bd59b09c7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1f7ec5a2cf4eb2a1940ca95259533e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8272fdecbbe45cabfc99faf2ead5d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader_outputs = create_dataloaders_WebTable(model_name_or_path=model_name_or_path,\n",
    "                                           task=task,\n",
    "                                           batch_size=batch_size,\n",
    "                                           train_file = '../datasets/webtable_IF/train.csv',\n",
    "                                           valid_file = '../datasets/webtable_IF/valid.csv',\n",
    "                                           test_file = '../datasets/webtable_IF/test.csv',\n",
    "                                           max_length = 256,\n",
    "                                        #    select_index=np.arange(0,5000,1)\n",
    "                                           )\n",
    "\n",
    "train_dataloader, eval_dataloader,test_dataloader, tokenized_datasets, collate_fn, num_labels, int2label = dataloader_outputs\n",
    "\n",
    "\n",
    "# lora_engine = LORAEngine(model_name_or_path=model_name_or_path,\n",
    "#                             target_modules=target_modules,\n",
    "#                             train_dataloader=train_dataloader,\n",
    "#                             eval_dataloader=eval_dataloader,\n",
    "#                             device=device,\n",
    "#                             num_epochs=num_epochs,\n",
    "#                             lr=lr,\n",
    "#                             low_rank=8, \n",
    "#                             task=task)\n",
    "\n",
    "lora_engine = LORAEngineDebertaMultiClass(model_name_or_path=model_name_or_path,\n",
    "                            target_modules=target_modules,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            # eval_dataloader=eval_dataloader,\n",
    "                            eval_dataloader=eval_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            device=device,\n",
    "                            num_epochs=num_epochs,\n",
    "                            lr=lr,\n",
    "                            num_labels = num_labels,\n",
    "                            low_rank=16, \n",
    "                            task=task,\n",
    "                            save_path = '../models/mrpc',\n",
    "                            valid_each_epoch=False,\n",
    "                            cal_grad_per_sample = True,\n",
    "                            grad_epoch = [5,7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ../../model/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 458,709 || all params: 185,044,650 || trainable%: 0.2479\n",
      "Total Steps:4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/704 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 704/704 [00:52<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.4880777296017516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:50<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 1.7886094497516751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:44<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 1.2420205415480516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:45<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 0.9862400226464326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:46<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 0.8376742519268935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:44<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 0.7673470885069533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [02:09<00:00,  5.44it/s]\n",
      "100%|██████████| 65/65 [00:11<00:00,  5.57it/s]\n",
      "100%|██████████| 807/807 [00:56<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Result on Test Data: Micro F1 = {'f1': 0.6574697955390335}, Macro F1 = {'f1': 0.2502817605279058}\n"
     ]
    }
   ],
   "source": [
    "lora_engine.build_LORA_model()\n",
    "tr_grad_dict,val_grad_dict = lora_engine.train_LORA_model()\n",
    "import torch\n",
    "torch.save(tr_grad_dict,'../output/CTA/webtable/tr_grad_dict_all.pkl')\n",
    "torch.save(val_grad_dict,'../output/CTA/webtable/val_grad_dict_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_engine.save_lora('../models/ER-semi-text-c-LoRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_grad_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(tr_grad_dict,'../output/ER/semi-text-c/tr_grad_dict_all.pkl')\n",
    "torch.save(val_grad_dict,'../output/ER/semi-text-c/val_grad_dict_all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分别计算不同epoch下的分数，然后累加分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3322955/154564480.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tr_grad_dict_all = torch.load('../output/ER/semi-text-c/tr_grad_dict_all.pkl')\n",
      "/tmp/ipykernel_3322955/154564480.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_grad_dict_all = torch.load('../output/ER/semi-text-c/val_grad_dict_all.pkl')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "tr_grad_dict_all = torch.load('../output/ER/semi-text-c/tr_grad_dict_all.pkl')\n",
    "val_grad_dict_all = torch.load('../output/ER/semi-text-c/val_grad_dict_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from influence_batch import IFEngine\n",
    "from tqdm.notebook import tqdm\n",
    "noise_index = None\n",
    "IF_device = 'cuda:7' ## 避免爆显存\n",
    "def compute_val_grad_avg(val_grad_dict):\n",
    "    # Compute the avg gradient on the validation dataset\n",
    "    n_val = len(val_grad_dict)\n",
    "    val_grad_avg_dict={}\n",
    "    for weight_name in val_grad_dict[0]:\n",
    "        if weight_name.__contains__('base_model'):\n",
    "            val_grad_avg_dict[weight_name]=torch.zeros(val_grad_dict[0][weight_name].shape)\n",
    "            for val_id in val_grad_dict:\n",
    "                val_grad_avg_dict[weight_name] += val_grad_dict[val_id][weight_name] / n_val\n",
    "        # else:\n",
    "        #     val_grad_avg_dict[weight_name] = val_grad_dict[weight_name]\n",
    "    # val_grad_avg_dict['ids'] = val_grad_dict['ids']\n",
    "    return val_grad_avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict_all = tr_grad_dict\n",
    "val_grad_dict_all = val_grad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_grad_dict_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25df9f6f9de342bda9970b9d69d8d50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6639ed8371134bc9a6ba625704ce0c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ae40f8b1b54a6a91877d5c486b622b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.0962071418762207\n",
      "Time taken for Hessian-free:  2.1457672119140625e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29345079dd30449d8bd2314841cec648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Datainf:  1.935877799987793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac77158c8a34a8d86d321770d61fb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LiSSA:  9.297013282775879\n"
     ]
    }
   ],
   "source": [
    "result_df = {}\n",
    "for epoch in tr_grad_dict_all.keys():\n",
    "    tr_grad_dict = tr_grad_dict_all[epoch]\n",
    "    val_grad_dict_avg = compute_val_grad_avg(val_grad_dict_all[epoch]) ## 不把所有的val_grad_dict放到gpu中，节省显存\n",
    "    for key in tqdm(tr_grad_dict): ## 尝试转为fp16计算\n",
    "        for kk in tr_grad_dict[key]:\n",
    "            # if kk!='ids':\n",
    "                tr_grad_dict[key][kk] = tr_grad_dict[key][kk].to(IF_device)\n",
    "            # tr_grad_dict[key].pop('ids')\n",
    "\n",
    "    for key in tqdm(val_grad_dict_avg):\n",
    "        # for kk in val_grad_dict[key]:\n",
    "            # if kk!='ids':\n",
    "                val_grad_dict_avg[key] = val_grad_dict_avg[key].to(IF_device)\n",
    "    influence_engine = IFEngine(weight_list=val_grad_dict_avg.keys())\n",
    "    influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict_avg, noise_index)\n",
    "\n",
    "    influence_engine.compute_hvps(compute_accurate=False,compute_LiSSA=True)\n",
    "    influence_engine.compute_IF()\n",
    "    result_df[epoch] = influence_engine.IF_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(result_df,'../output/CTA/webtable/IF_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =  pd.DataFrame(result_df[5]['iterative']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = pd.DataFrame(result_df[2]['iterative']).T\n",
    "result_4 = pd.DataFrame(result_df[4]['iterative']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_sum_column(df):\n",
    "    \"\"\"\n",
    "    对输入的DataFrame添加一列，列的值为所有列名包含'model'的列的元素之和。\n",
    "\n",
    "    参数:\n",
    "    df (pd.DataFrame): 输入的DataFrame对象\n",
    "\n",
    "    返回:\n",
    "    pd.DataFrame: 添加了新列后的DataFrame对象\n",
    "    \"\"\"\n",
    "    sum_cols = [col for col in df.columns if 'model' in col]\n",
    "    df['sum_model_cols'] = df[sum_cols].sum(axis=1)\n",
    "    return df\n",
    "# result_2 = add_sum_column(result_2)\n",
    "# result_4 = add_sum_column(result_4)\n",
    "result =  pd.DataFrame(result_df[5]['iterative']).T\n",
    "result = add_sum_column(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample_IF = np.zeros(45000)\n",
    "for index,row in result.iterrows():\n",
    "    ids = row['ids']\n",
    "    sum = row['sum_model_cols']\n",
    "    for id in ids:\n",
    "        sample_IF[id] += sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample_IF = np.zeros(45000)\n",
    "for index,row in result_2.iterrows():\n",
    "    ids = row['ids']\n",
    "    sum = row['sum_model_cols']\n",
    "    for id in ids:\n",
    "        sample_IF[id] += sum\n",
    "for index,row in result_4.iterrows():\n",
    "    ids = row['ids']\n",
    "    sum = row['sum_model_cols']\n",
    "    for id in ids:\n",
    "        sample_IF[id] += sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_index = np.argsort(sample_IF)[:4500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3336774/2073020919.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  result_df = torch.load('../output/ER/semi-text-c/IF_dict.pkl')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "result_df = torch.load('../output/ER/semi-text-c/IF_dict.pkl')\n",
    "# sample_IF = {}\n",
    "# # for i in range(4500):\n",
    "# #     sample_IF[i] = 0\n",
    "# for epoch in result_df.keys():\n",
    "#     for method in ['iterative']:\n",
    "#         for batch_id in result_df[epoch][method].keys(): ## 选出来所有的batch\n",
    "#             IF_score = result_df[epoch][method][batch_id]\n",
    "#             ids = IF_score['ids']\n",
    "result_2 = pd.DataFrame(result_df[2]['iterative']).T\n",
    "result_4 = pd.DataFrame(result_df[4]['iterative']).T\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_sum_column(df):\n",
    "    \"\"\"\n",
    "    对输入的DataFrame添加一列，列的值为所有列名包含'model'的列的元素之和。\n",
    "\n",
    "    参数:\n",
    "    df (pd.DataFrame): 输入的DataFrame对象\n",
    "\n",
    "    返回:\n",
    "    pd.DataFrame: 添加了新列后的DataFrame对象\n",
    "    \"\"\"\n",
    "    sum_cols = [col for col in df.columns if 'model' in col]\n",
    "    df['sum_model_cols'] = df[sum_cols].sum(axis=1)\n",
    "    return df\n",
    "result_2 = add_sum_column(result_2)\n",
    "result_4 = add_sum_column(result_4)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "sample_IF = np.zeros(4500)\n",
    "for index,row in result_2.iterrows():\n",
    "    ids = row['ids']\n",
    "    sum = row['sum_model_cols']\n",
    "    for id in ids:\n",
    "        sample_IF[id] += sum\n",
    "for index,row in result_4.iterrows():\n",
    "    ids = row['ids']\n",
    "    sum = row['sum_model_cols']\n",
    "    for id in ids:\n",
    "        sample_IF[id] += sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_index = np.argsort(sample_IF)[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "# from dataloader import create_dataloaders\n",
    "from dataloader_ER import create_dataloaders\n",
    "# from lora_model import LORAEngine,LORAEngineDeberta,LORAEngineDebertaMultiClass\n",
    "from lora_model_vmap import LORAEngineDebertaMultiClass\n",
    "\n",
    "# from influence import IFEngine\n",
    "from influence_hyperinf import IFEngine\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# model_name_or_path=\"roberta-large\"\n",
    "# model_name_or_path=\"/home/yanmy/roberta-base\"\n",
    "model_name_or_path=\"../../model/deberta-v3-base\"\n",
    "# model_name_or_path=\"../../model/roberta-large\"\n",
    "task=\"ER\"\n",
    "# task = \"qnli\"\n",
    "noise_ratio=0\n",
    "batch_size=16\n",
    "# target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "# target_modules = ['value']\n",
    "target_modules=[\"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "\n",
    "device=\"cuda:7\"\n",
    "num_epochs=15\n",
    "lr=3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771a3c5fc2d84c82ad8cf1c73dc9deca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2101eaa892f042d9a90d39d8b1a7dccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b583a95b895145afadeeca488f72dfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader_outputs = create_dataloaders(model_name_or_path=model_name_or_path,\n",
    "                                           task=task,\n",
    "                                           noise_ratio=noise_ratio,\n",
    "                                           batch_size=batch_size,\n",
    "                                           train_file = '../ER/semi-text-c/train.json',\n",
    "                                           valid_file = '../ER/semi-text-c/valid.json',\n",
    "                                           test_file = '../ER/semi-text-c/test.json',\n",
    "                                           max_length=256,\n",
    "                                           select_index = select_index)\n",
    "# train_dataloader, eval_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "train_dataloader, eval_dataloader,test_dataloader, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "\n",
    "\n",
    "# lora_engine = LORAEngine(model_name_or_path=model_name_or_path,\n",
    "#                             target_modules=target_modules,\n",
    "#                             train_dataloader=train_dataloader,\n",
    "#                             eval_dataloader=eval_dataloader,\n",
    "#                             device=device,\n",
    "#                             num_epochs=num_epochs,\n",
    "#                             lr=lr,\n",
    "#                             low_rank=8, \n",
    "#                             task=task)\n",
    "\n",
    "lora_engine = LORAEngineDebertaMultiClass(model_name_or_path=model_name_or_path,\n",
    "                            target_modules=target_modules,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            # eval_dataloader=eval_dataloader,\n",
    "                            eval_dataloader=eval_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            device=device,\n",
    "                            num_epochs=num_epochs,\n",
    "                            lr=lr,\n",
    "                            low_rank=16, \n",
    "                            task=task,\n",
    "                            save_path = '../models/mrpc',\n",
    "                            valid_each_epoch=False,\n",
    "                            cal_grad_per_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ../../model/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 184,720,132 || trainable%: 0.1605\n",
      "Total Steps:480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 0.6829902809113264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 0.6839223299175501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 0.6768771205097437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 0.6237391531467438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 0.47199080791324377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 0.3934737551026046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 0.31626943429000676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 0.2618878852808848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 0.20863939123228192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 0.23196130606811494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Training Loss = 0.16344814986223355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Training Loss = 0.1570405598031357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Training Loss = 0.14662564324680716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Training Loss = 0.15161741798510775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Training Loss = 0.1355522975209169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262/262 [00:18<00:00, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Result on Test Data: {'f1': 0.7880910683012259}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_engine.build_LORA_model()\n",
    "lora_engine.train_LORA_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'f1': 0.8182701652089407} 1000\n",
    "{'f1': 0.7937062937062938} 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =  pd.DataFrame(result_df[5]['proposed']).T\n",
    "result = add_sum_column(result)\n",
    "\n",
    "import numpy as np\n",
    "sample_IF = np.zeros(45000)\n",
    "for index,row in result.iterrows():\n",
    "    ids = row['ids']\n",
    "    sum = row['sum_model_cols']\n",
    "    for id in ids:\n",
    "        sample_IF[id] += sum\n",
    "select_index = np.argsort(sample_IF)[:9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847633494b4d462c963888dc80944df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145dd35f167b4e2cbca8b89977f1a224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96732bd7d66842769bd667177127782b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader_outputs = create_dataloaders_WebTable(model_name_or_path=model_name_or_path,\n",
    "                                           task=task,\n",
    "                                           batch_size=batch_size,\n",
    "                                           train_file = '../datasets/webtable_IF/train.csv',\n",
    "                                           valid_file = '../datasets/webtable_IF/valid.csv',\n",
    "                                           test_file = '../datasets/webtable_IF/test.csv',\n",
    "                                           max_length = 256,\n",
    "                                           select_index=select_index\n",
    "                                           )\n",
    "\n",
    "train_dataloader, eval_dataloader,test_dataloader, tokenized_datasets, collate_fn, num_labels, int2label = dataloader_outputs\n",
    "\n",
    "\n",
    "# lora_engine = LORAEngine(model_name_or_path=model_name_or_path,\n",
    "#                             target_modules=target_modules,\n",
    "#                             train_dataloader=train_dataloader,\n",
    "#                             eval_dataloader=eval_dataloader,\n",
    "#                             device=device,\n",
    "#                             num_epochs=num_epochs,\n",
    "#                             lr=lr,\n",
    "#                             low_rank=8, \n",
    "#                             task=task)\n",
    "\n",
    "lora_engine = LORAEngineDebertaMultiClass(model_name_or_path=model_name_or_path,\n",
    "                            target_modules=target_modules,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            # eval_dataloader=eval_dataloader,\n",
    "                            eval_dataloader=eval_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            device=device,\n",
    "                            num_epochs=10,\n",
    "                            lr=lr,\n",
    "                            num_labels = num_labels,\n",
    "                            low_rank=16, \n",
    "                            task=task,\n",
    "                            save_path = '../models/mrpc',\n",
    "                            valid_each_epoch=False,\n",
    "                            cal_grad_per_sample = False,\n",
    "                            grad_epoch = [5,7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ../../model/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 458,709 || all params: 185,044,650 || trainable%: 0.2479\n",
      "Total Steps:1410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/141 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 141/141 [00:08<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 4.622079008859946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:08<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 3.692977391236217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:08<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 3.171968994411171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.7281834115373327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.4334014992341926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 2.236975460187763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 2.0978693463278155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 2.004238104143887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:08<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 1.93690180863049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:08<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 1.8906240175801812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 807/807 [00:56<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Result on Test Data: Micro F1 = {'f1': 0.4569973667905824}, Macro F1 = {'f1': 0.09202929732558915}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_engine.build_LORA_model()\n",
    "lora_engine.train_LORA_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTA数据需要重构？\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
