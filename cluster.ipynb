{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import  KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from FlagEmbedding import FlagModel\n",
    "import time\n",
    "import submodlib\n",
    "from submodlib.functions.facilityLocation import FacilityLocationFunction\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--pt_data_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--json_data_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--json_save_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--sent_type\", type=int, default=0)\n",
    "    parser.add_argument(\"--ppl_type\", type=int, default=0)\n",
    "    parser.add_argument(\"--cluster_method\", type=str, default='kmeans')\n",
    "    parser.add_argument(\"--reduce_method\", type=str, default='tsne')\n",
    "    parser.add_argument(\"--sample_num\", type=int, default=10)\n",
    "    parser.add_argument(\"--kmeans_num_clusters\", type=int, default=100)\n",
    "    parser.add_argument(\"--low_th\", type=int, default=1)\n",
    "    parser.add_argument(\"--up_th\", type=int, default=99)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def do_clustering(high_dim_vectors,cluster_method='kmeans',kmeans_num_clusters=100):\n",
    "\n",
    "    clustering_algorithm = cluster_method\n",
    "    if clustering_algorithm == 'kmeans':\n",
    "        clustering = KMeans(n_clusters=kmeans_num_clusters, random_state=0).fit(high_dim_vectors)\n",
    "    \n",
    "    return clustering\n",
    "\n",
    "def do_reduce_dim(high_dim_vectors):\n",
    "    # Perform t-SNE for visualization\n",
    "    # if args.reduce_method == 'tsne':\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    low_dim_vectors = tsne.fit_transform(high_dim_vectors)\n",
    "    return low_dim_vectors\n",
    "\n",
    "def sample_middle_confidence_data(cluster_labels, confidences, n, low_th=25, up_th=75):\n",
    "    num_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "    # Get the indices for each cluster\n",
    "    cluster_indices = {i: np.where(cluster_labels == i)[0] for i in range(num_clusters)}\n",
    "    \n",
    "    # Create a dictionary to store the indices of the middle level confidence samples\n",
    "    middle_confidence_samples = {}\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        # Get the sorted indices for this cluster\n",
    "        sorted_indices = cluster_indices[i]\n",
    "        \n",
    "        # If there are less than n samples in this class, just return all of them\n",
    "        if len(sorted_indices) < n:\n",
    "            middle_confidence_samples[i] = sorted_indices\n",
    "            continue\n",
    "\n",
    "        # Get the confidences for this cluster\n",
    "        cluster_confidences = confidences[sorted_indices]\n",
    "        lower_threshold = np.percentile(cluster_confidences, low_th)\n",
    "        upper_threshold = np.percentile(cluster_confidences, up_th)\n",
    "\n",
    "        # Get the indices of the samples within the middle level confidence range\n",
    "        middle_indices = sorted_indices[(cluster_confidences >= lower_threshold) & (cluster_confidences <= upper_threshold)]\n",
    "        \n",
    "        # If there are less than n samples in the middle range, use all of them\n",
    "        if len(middle_indices) < n:\n",
    "            middle_confidence_samples[i] = middle_indices\n",
    "        else:\n",
    "            # Calculate step size for even sampling\n",
    "            step_size = len(middle_indices) // n\n",
    "            # Select evenly from the middle level confidence samples\n",
    "            middle_confidence_samples[i] = middle_indices[::step_size][:n]\n",
    "\n",
    "    return middle_confidence_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings by SentBert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse LLM File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data = pd.read_json('/data/home/wangys/transfer-er/Pipeline/Amazon-Google/LLM_file/Amazon-Google-Train-Match-P1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_input_length(row):\n",
    "    input = row['instruction']\n",
    "    output = input.split('at the final judgement.')[1].split('Take these examples as reference:')[0]\n",
    "    return output\n",
    "All_Data['instruction'] = All_Data.apply(cut_input_length,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2106371/2444698609.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = row[0]\n",
      "/tmp/ipykernel_2106371/2444698609.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = eval(row[-1])['Output']\n"
     ]
    }
   ],
   "source": [
    "# text = All_Data.iloc[0,0]\n",
    "left_list = []\n",
    "right_list = []\n",
    "label_list = []\n",
    "for index,row in All_Data.iterrows():\n",
    "    text = row[0]\n",
    "    Entity_1 = text.split('\\n\\nEntity 1:')[1].split('\\n\\nEntity 2')[0]\n",
    "    Entity_2 = text.split('\\n\\nEntity 2:')[1].split('\\n\\nTake these examples as reference:')[0] \n",
    "    label = eval(row[-1])['Output']\n",
    "    left_list.append(Entity_1)\n",
    "    right_list.append(Entity_2)\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 15/15 [00:01<00:00,  7.63it/s]\n",
      "Inference Embeddings: 100%|██████████| 15/15 [00:02<00:00,  7.26it/s]\n",
      "Inference Embeddings: 100%|██████████| 15/15 [00:00<00:00, 79.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'\n",
    "model = FlagModel('../sentence_transformer_model/bge-large-en-1.5/', \n",
    "                  use_fp16=True)\n",
    "embedding_a = model.encode(left_list)\n",
    "embedding_b = model.encode(right_list)\n",
    "embedding_c = model.encode(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppl = pd.read_json('/data/home/wangys/DataSelection-IF/ppl/ppl_qwen2.5-7B-AG-Short.json')\n",
    "ppl = pd.read_csv('ppl/ppl_qwen2.5-0.5B-AG-short.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3663, 3072)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ppl_list = ppl.iloc[:,0].to_list()\n",
    "pt_data = np.concatenate([embedding_a,embedding_b,embedding_c],axis=1)\n",
    "pt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "{'Output': 'mismatch'}    741\n",
       "{'Output': 'match'}       250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# args = parse_args()\n",
    "# print(args)\n",
    "\n",
    "# pt_data = torch.load(args.pt_data_path, map_location=torch.device('cpu'))\n",
    "# with open(args.json_data_path, \"r\") as f:\n",
    "#     json_data = json.load(f)\n",
    "file_path = '/data/home/wangys/transfer-er/Pipeline/Amazon-Google/LLM_file/Amazon-Google-Train-Match-P1-wo-RAG.json'\n",
    "json_data = pd.read_json(file_path)\n",
    "\n",
    "# emb_list = []\n",
    "# ppl_list = []\n",
    "# for i in tqdm(range(len(json_data))):\n",
    "#     sent_emb_list = pt_data[i]\n",
    "#     # sent_emb_list = data_i['sent_emb']\n",
    "#     emb_list.append(sent_emb_list)\n",
    "#     ppl_list.append(ppl_list[i])\n",
    "high_dim_vectors = pt_data\n",
    "\n",
    "# high_dim_vectors = torch.cat(emb_list,0).numpy()\n",
    "ppl_array = np.array(ppl_list)\n",
    "\n",
    "clustering = do_clustering(high_dim_vectors,kmeans_num_clusters=100)\n",
    "cluster_labels = clustering.labels_\n",
    "\n",
    "def get_json_sample(middle_confidence_samples):\n",
    "    \n",
    "    json_samples = []\n",
    "    for k in middle_confidence_samples.keys():\n",
    "        ids_list = middle_confidence_samples[k].tolist()\n",
    "        # for id_i in ids_list:\n",
    "            # ori_sample = json_data[id_i]\n",
    "        json_samples.extend(ids_list)\n",
    "    \n",
    "\n",
    "    return json_samples\n",
    "\n",
    "# middle_confidence_samples = sample_middle_confidence_data(cluster_labels, ppl_array, n = 10, low_th=25, up_th = 75)\n",
    "middle_confidence_samples = sample_middle_confidence_data(cluster_labels, ppl_array, n = 10, low_th=25, up_th = 75)\n",
    "\n",
    "new_data = get_json_sample(middle_confidence_samples)\n",
    "json_data.iloc[new_data]['output'].value_counts()\n",
    "# print('New data len \\n',len(new_data))\n",
    "# with open(args.json_save_path, \"w\") as fw:\n",
    "#     json.dump(new_data, fw, indent=4)\n",
    "# pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(json_data.iloc[new_data].to_dict(orient='records'), open('train/AG-train-init.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FL score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_indices = {i: np.where(cluster_labels == i)[0] for i in range(100)}\n",
    "# cluster_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "# def cluster_vectors(high_dim_vectors_cluster, indexes, batch_size):\n",
    "#     \"\"\"\n",
    "#     根据给定的高维向量矩阵、索引以及期望的每个聚类元素个数进行聚类划分。\n",
    "\n",
    "#     参数:\n",
    "#     high_dim_vectors_cluster (numpy.ndarray): m * n的高维向量矩阵，m为元素个数，n为向量维度。\n",
    "#     indexes (numpy.ndarray): 对应高维向量矩阵中元素的索引列表，长度为k。\n",
    "#     batch_size (int): 期望每个聚类包含的元素个数。\n",
    "\n",
    "#     返回:\n",
    "#     dict: 聚类划分后的结果，键为聚类编号（从0到k - 1），值为对应聚类包含的元素索引列表。\n",
    "#     \"\"\"\n",
    "#     # 获取对应索引的向量\n",
    "#     selected_vectors = high_dim_vectors_cluster[indexes]\n",
    "\n",
    "#     # 计算这些向量之间的余弦相似度矩阵\n",
    "#     similarity_matrix = cosine_similarity(selected_vectors)\n",
    "\n",
    "#     # 使用层次聚类（AgglomerativeClustering）基于余弦相似度进行聚类划分\n",
    "#     k = len(indexes)\n",
    "#     clustering_model = AgglomerativeClustering(n_clusters=k, metric='cosine', linkage='average')\n",
    "#     clustering_model.fit(similarity_matrix)\n",
    "\n",
    "#     # 根据聚类结果分配所有的m个元素到对应的聚类中\n",
    "#     cluster_assignments = {i: [] for i in range(k)}\n",
    "#     for i in range(len(high_dim_vectors_cluster)):\n",
    "#         vector = high_dim_vectors_cluster[i].reshape(1, -1)\n",
    "#         similarities = cosine_similarity(vector, selected_vectors)[0]\n",
    "#         closest_cluster = np.argmax(similarities)\n",
    "#         cluster_assignments[closest_cluster].append(i)\n",
    "\n",
    "#     # 调整每个聚类中的元素个数尽量接近batch_size\n",
    "#     for cluster_id in range(k):\n",
    "#         cluster_indices = cluster_assignments[cluster_id]\n",
    "#         if len(cluster_indices) > batch_size:\n",
    "#             # 如果当前聚类元素个数大于batch_size，可考虑进一步划分等策略（这里简单取前batch_size个元素示例）\n",
    "#             cluster_assignments[cluster_id] = cluster_indices[:batch_size]\n",
    "#         elif len(cluster_indices) < batch_size:\n",
    "#             # 如果小于batch_size，可以从其他聚类补充等（这里暂不实现复杂补充逻辑，仅打印提示）\n",
    "#             print(f\"Cluster {cluster_id} has less than {batch_size} elements.\")\n",
    "\n",
    "#     return cluster_assignments\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def cluster_vectors(high_dim_vectors_cluster, indexes, batch_size):\n",
    "    \"\"\"\n",
    "    根据给定的高维向量矩阵、索引以及期望的每个聚类元素个数进行聚类划分。\n",
    "\n",
    "    参数:\n",
    "    high_dim_vectors_cluster (numpy.ndarray): m * n的高维向量矩阵，m为元素个数，n为向量维度。\n",
    "    indexes (numpy.ndarray): 对应高维向量矩阵中元素的索引列表，长度为k。\n",
    "    batch_size (int): 期望每个聚类包含的元素个数。\n",
    "\n",
    "    返回:\n",
    "    tuple: 包含两个元素，第一个元素是聚类划分后的结果（字典形式，键为聚类编号，值为对应聚类包含的元素索引列表），\n",
    "           第二个元素是覆盖率（float类型，表示已分配元素占总元素的比例）。\n",
    "    \"\"\"\n",
    "    # 获取对应索引的向量\n",
    "    selected_vectors = high_dim_vectors_cluster[indexes]\n",
    "\n",
    "    # 计算这些向量之间的余弦相似度矩阵\n",
    "    similarity_matrix = cosine_similarity(selected_vectors)\n",
    "\n",
    "    # 使用层次聚类（AgglomerativeClustering）基于余弦相似度进行聚类划分\n",
    "    k = len(indexes)\n",
    "    clustering_model = AgglomerativeClustering(n_clusters=k, metric='cosine', linkage='average')\n",
    "    clustering_model.fit(similarity_matrix)\n",
    "\n",
    "    # 根据聚类结果分配所有的m个元素到对应的聚类中\n",
    "    cluster_assignments = {i: [] for i in range(k)}\n",
    "    for i in range(len(high_dim_vectors_cluster)):\n",
    "        vector = high_dim_vectors_cluster[i].reshape(1, -1)\n",
    "        similarities = cosine_similarity(vector, selected_vectors)[0]\n",
    "        closest_cluster = np.argmax(similarities)\n",
    "        cluster_assignments[closest_cluster].append(i)\n",
    "\n",
    "    # 检查未分配的元素，并根据距离的就近原则分配到对应的聚类中\n",
    "    all_indices = set(range(len(high_dim_vectors_cluster)))\n",
    "    assigned_indices = set([index for sublist in cluster_assignments.values() for index in sublist])\n",
    "    unassigned_indices = all_indices - assigned_indices\n",
    "    for index in unassigned_indices:\n",
    "        vector = high_dim_vectors_cluster[index].reshape(1, -1)\n",
    "        distances = []\n",
    "        for cluster_id in range(k):\n",
    "            cluster_vectors = np.array([high_dim_vectors_cluster[i].reshape(1, -1) for i in cluster_assignments[cluster_id]])\n",
    "            mean_cluster_vector = np.mean(cluster_vectors, axis=0)\n",
    "            distance = cosine_similarity(vector, mean_cluster_vector)[0][0]\n",
    "            distances.append(distance)\n",
    "        closest_cluster = np.argmin(distances)\n",
    "        cluster_assignments[closest_cluster].append(index)\n",
    "\n",
    "    # 调整每个聚类中的元素个数尽量接近batch_size（这里简单处理，可根据实际优化）\n",
    "    for cluster_id in range(k):\n",
    "        cluster_indices = cluster_assignments[cluster_id]\n",
    "        if len(cluster_indices) > batch_size:\n",
    "            cluster_assignments[cluster_id] = cluster_indices[:batch_size]\n",
    "        elif len(cluster_indices) < batch_size:\n",
    "            while len(cluster_indices) < batch_size and unassigned_indices:\n",
    "                # 从未分配元素中找距离当前聚类最近的补充进来\n",
    "                index_to_add = None\n",
    "                min_distance = float('inf')\n",
    "                for unassigned_index in unassigned_indices:\n",
    "                    unassigned_vector = high_dim_vectors_cluster[unassigned_index].reshape(1, -1)\n",
    "                    mean_cluster_vector = np.mean([high_dim_vectors_cluster[i].reshape(1, -1) for i in cluster_indices], axis=0)\n",
    "                    distance = cosine_similarity(unassigned_vector, mean_cluster_vector)[0][0]\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        index_to_add = unassigned_index\n",
    "                if index_to_add is not None:\n",
    "                    cluster_assignments[cluster_id].append(index_to_add)\n",
    "                    unassigned_indices.remove(index_to_add)\n",
    "                    cluster_indices = cluster_assignments[cluster_id]\n",
    "\n",
    "    # 计算覆盖率\n",
    "    coverage = len(assigned_indices) / len(high_dim_vectors_cluster)\n",
    "\n",
    "    return cluster_assignments, coverage\n",
    "def cosine_similarity_clustering(high_dim_vectors_cluster, indices, k, batch_size):\n",
    "    \"\"\"\n",
    "    Perform k clustering based on cosine similarity, ensuring each cluster has batch_size elements.\n",
    "\n",
    "    Parameters:\n",
    "    - high_dim_vectors_cluster (np.ndarray): An m x n matrix containing m high-dimensional vectors.\n",
    "    - indices (list): A list of k indices corresponding to m rows in the matrix.\n",
    "    - k (int): Number of clusters.\n",
    "    - batch_size (int): Desired number of elements per cluster.\n",
    "\n",
    "    Returns:\n",
    "    - clusters (list of lists): A list where each sublist contains the indices of the elements in a cluster.\n",
    "    \"\"\"\n",
    "    m, n = high_dim_vectors_cluster.shape\n",
    "    if len(indices) != k:\n",
    "        raise ValueError(\"Number of provided indices must match the number of clusters (k).\")\n",
    "\n",
    "    # Step 1: Initialize cluster centers using the given indices\n",
    "    cluster_centers = high_dim_vectors_cluster[indices]\n",
    "\n",
    "    # Step 2: Compute cosine similarity between all elements and cluster centers\n",
    "    similarity_matrix = cosine_similarity(high_dim_vectors_cluster, cluster_centers)\n",
    "\n",
    "    # Step 3: Assign elements to clusters greedily (allow duplicates if needed)\n",
    "    clusters = [[] for _ in range(k)]\n",
    "\n",
    "    for _ in range(batch_size):  # Ensure each cluster has batch_size elements\n",
    "        for cluster_idx in range(k):\n",
    "            # Find the most similar element for the current cluster\n",
    "            best_idx = -1\n",
    "            best_similarity = -1\n",
    "            for i in range(m):\n",
    "                if similarity_matrix[i, cluster_idx] > best_similarity:\n",
    "                    best_idx = i\n",
    "                    best_similarity = similarity_matrix[i, cluster_idx]\n",
    "\n",
    "            if best_idx != -1:\n",
    "                clusters[cluster_idx].append(best_idx)\n",
    "\n",
    "    # Step 4: Handle remaining elements by assigning to closest clusters\n",
    "    remaining_elements = [i for i in range(m)]\n",
    "    for i in remaining_elements:\n",
    "        # Assign to the cluster with the highest similarity\n",
    "        best_cluster = np.argmax(similarity_matrix[i])\n",
    "        clusters[best_cluster].append(i)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fla(X, number_all, number_select):\n",
    "    start_time = time.time()\n",
    "\n",
    "    Y = X\n",
    "    obj = FacilityLocationFunction(n=number_all, mode=\"dense\", data=Y, metric=\"cosine\")\n",
    "    greedyList = obj.maximize(budget=number_select, optimizer='LazyGreedy', stopIfZeroGain=False, stopIfNegativeGain=False, verbose=False)\n",
    "    idx_list = [tuple_i[0] for tuple_i in greedyList]\n",
    "\n",
    "    print('FLA time used:',(time.time()-start_time),'(second)')\n",
    "    return idx_list,greedyList\n",
    "\n",
    "# idx_list,greedyList = do_fla(high_dim_vectors,high_dim_vectors.shape[0],number_select=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating FLA score per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.03470182418823242 (second)\n",
      "FLA time used: 0.021735668182373047 (second)\n",
      "FLA time used: 0.07947826385498047 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 15 of 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.05884528160095215 (second)\n",
      "FLA time used: 0.0444488525390625 (second)\n",
      "FLA time used: 0.04488778114318848 (second)\n",
      "FLA time used: 0.025129318237304688 (second)\n",
      "FLA time used: 0.025299072265625 (second)\n",
      "FLA time used: 0.024254798889160156 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 9 of 9]1]3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.025971651077270508 (second)\n",
      "FLA time used: 0.006575345993041992 (second)\n",
      "FLA time used: 0.006723165512084961 (second)\n",
      "FLA time used: 0.010684967041015625 (second)\n",
      "FLA time used: 0.010772466659545898 (second)\n",
      "FLA time used: 0.001813650131225586 (second)\n",
      "FLA time used: 0.001905679702758789 (second)\n",
      "FLA time used: 0.010607242584228516 (second)\n",
      "FLA time used: 0.010812044143676758 (second)\n",
      "FLA time used: 0.003981351852416992 (second)\n",
      "FLA time used: 0.004069328308105469 (second)\n",
      "FLA time used: 0.01546335220336914 (second)\n",
      "FLA time used: 0.015787124633789062 (second)\n",
      "FLA time used: 0.003015756607055664 (second)\n",
      "FLA time used: 0.003118276596069336 (second)\n",
      "FLA time used: 0.007592916488647461 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 5 of 5]9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.0077342987060546875 (second)\n",
      "FLA time used: 0.0013020038604736328 (second)\n",
      "FLA time used: 0.0013506412506103516 (second)\n",
      "FLA time used: 0.005677700042724609 (second)\n",
      "FLA time used: 0.005803585052490234 (second)\n",
      "FLA time used: 0.01500248908996582 (second)\n",
      "FLA time used: 0.015356779098510742 (second)\n",
      "FLA time used: 0.004811763763427734 (second)\n",
      "FLA time used: 0.0049021244049072266 (second)\n",
      "FLA time used: 0.003743410110473633 (second)\n",
      "FLA time used: 0.0038466453552246094 (second)\n",
      "FLA time used: 0.005674839019775391 (second)\n",
      "FLA time used: 0.005761384963989258 (second)\n",
      "FLA time used: 0.011024236679077148 (second)\n",
      "FLA time used: 0.01123189926147461 (second)\n",
      "FLA time used: 0.02607274055480957 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.026555538177490234 (second)\n",
      "FLA time used: 0.002797842025756836 (second)\n",
      "FLA time used: 0.0029125213623046875 (second)\n",
      "FLA time used: 0.014018774032592773 (second)\n",
      "FLA time used: 0.014412403106689453 (second)\n",
      "FLA time used: 0.006602287292480469 (second)\n",
      "FLA time used: 0.006728649139404297 (second)\n",
      "FLA time used: 0.0034666061401367188 (second)\n",
      "FLA time used: 0.003596067428588867 (second)\n",
      "FLA time used: 0.00941324234008789 (second)\n",
      "FLA time used: 0.00963449478149414 (second)\n",
      "FLA time used: 0.0012993812561035156 (second)\n",
      "FLA time used: 0.0013842582702636719 (second)\n",
      "FLA time used: 0.007947921752929688 (second)\n",
      "FLA time used: 0.008139371871948242 (second)\n",
      "FLA time used: 0.0035178661346435547 (second)\n",
      "FLA time used: 0.0035893917083740234 (second)\n",
      "FLA time used: 0.004247188568115234 (second)\n",
      "FLA time used: 0.004420042037963867 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 24 of 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.011903762817382812 (second)\n",
      "FLA time used: 0.011785268783569336 (second)\n",
      "FLA time used: 0.018543004989624023 (second)\n",
      "FLA time used: 0.019147157669067383 (second)\n",
      "FLA time used: 0.06536650657653809 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 16 of 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.06603288650512695 (second)\n",
      "FLA time used: 0.03353166580200195 (second)\n",
      "FLA time used: 0.03420567512512207 (second)\n",
      "FLA time used: 0.005736112594604492 (second)\n",
      "FLA time used: 0.005803108215332031 (second)\n",
      "FLA time used: 0.012792110443115234 (second)\n",
      "FLA time used: 0.013059377670288086 (second)\n",
      "FLA time used: 0.005064725875854492 (second)\n",
      "FLA time used: 0.005178689956665039 (second)\n",
      "FLA time used: 0.0053637027740478516 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 4 of 4]7]1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.005509853363037109 (second)\n",
      "FLA time used: 0.003549337387084961 (second)\n",
      "FLA time used: 0.003591775894165039 (second)\n",
      "FLA time used: 0.00724029541015625 (second)\n",
      "FLA time used: 0.0073964595794677734 (second)\n",
      "FLA time used: 0.01016378402709961 (second)\n",
      "FLA time used: 0.010435104370117188 (second)\n",
      "FLA time used: 0.005662202835083008 (second)\n",
      "FLA time used: 0.0058345794677734375 (second)\n",
      "FLA time used: 0.01056218147277832 (second)\n",
      "FLA time used: 0.010864734649658203 (second)\n",
      "FLA time used: 0.0012898445129394531 (second)\n",
      "FLA time used: 0.0013856887817382812 (second)\n",
      "FLA time used: 0.0065648555755615234 (second)\n",
      "FLA time used: 0.006730794906616211 (second)\n",
      "FLA time used: 0.00694584846496582 (second)\n",
      "FLA time used: 0.007107257843017578 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 33 of 33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.021218061447143555 (second)\n",
      "FLA time used: 0.020635128021240234 (second)\n",
      "FLA time used: 0.011864900588989258 (second)\n",
      "FLA time used: 0.012236833572387695 (second)\n",
      "FLA time used: 0.005670785903930664 (second)\n",
      "FLA time used: 0.005761861801147461 (second)\n",
      "FLA time used: 0.01071786880493164 (second)\n",
      "FLA time used: 0.010842323303222656 (second)\n",
      "FLA time used: 0.01960611343383789 (second)\n",
      "FLA time used: 0.020132780075073242 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 8 of 8]3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.003046751022338867 (second)\n",
      "FLA time used: 0.0031049251556396484 (second)\n",
      "FLA time used: 0.006361722946166992 (second)\n",
      "FLA time used: 0.006455898284912109 (second)\n",
      "FLA time used: 0.010609149932861328 (second)\n",
      "FLA time used: 0.010889530181884766 (second)\n",
      "FLA time used: 0.0013031959533691406 (second)\n",
      "FLA time used: 0.0013930797576904297 (second)\n",
      "FLA time used: 0.014563798904418945 (second)\n",
      "FLA time used: 0.01485753059387207 (second)\n",
      "FLA time used: 0.014114618301391602 (second)\n",
      "FLA time used: 0.014365196228027344 (second)\n",
      "FLA time used: 0.006266355514526367 (second)\n",
      "FLA time used: 0.00641942024230957 (second)\n",
      "FLA time used: 0.005699872970581055 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 4 of 4]1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.005776166915893555 (second)\n",
      "FLA time used: 0.0013072490692138672 (second)\n",
      "FLA time used: 0.0013880729675292969 (second)\n",
      "FLA time used: 0.0012912750244140625 (second)\n",
      "FLA time used: 0.0013971328735351562 (second)\n",
      "FLA time used: 0.00478053092956543 (second)\n",
      "FLA time used: 0.004904031753540039 (second)\n",
      "FLA time used: 0.0012943744659423828 (second)\n",
      "FLA time used: 0.0013556480407714844 (second)\n",
      "FLA time used: 0.004239797592163086 (second)\n",
      "FLA time used: 0.004352092742919922 (second)\n",
      "FLA time used: 0.0075910091400146484 (second)\n",
      "FLA time used: 0.00770115852355957 (second)\n",
      "FLA time used: 0.0019850730895996094 (second)\n",
      "FLA time used: 0.0020904541015625 (second)\n",
      "FLA time used: 0.005676984786987305 (second)\n",
      "FLA time used: 0.005790233612060547 (second)\n",
      "FLA time used: 0.0013053417205810547 (second)\n",
      "FLA time used: 0.0013928413391113281 (second)\n",
      "FLA time used: 0.009044647216796875 (second)\n",
      "FLA time used: 0.009319305419921875 (second)\n",
      "FLA time used: 0.0013189315795898438 (second)\n",
      "FLA time used: 0.0013897418975830078 (second)\n",
      "FLA time used: 0.0034856796264648438 (second)\n",
      "FLA time used: 0.0035779476165771484 (second)\n",
      "FLA time used: 0.02680373191833496 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.02725839614868164 (second)\n",
      "FLA time used: 0.0056498050689697266 (second)\n",
      "FLA time used: 0.005904436111450195 (second)\n",
      "FLA time used: 0.0056302547454833984 (second)\n",
      "FLA time used: 0.005812883377075195 (second)\n",
      "FLA time used: 0.003474712371826172 (second)\n",
      "FLA time used: 0.003582477569580078 (second)\n",
      "FLA time used: 0.0034973621368408203 (second)\n",
      "FLA time used: 0.0035991668701171875 (second)\n",
      "FLA time used: 0.0034105777740478516 (second)\n",
      "FLA time used: 0.0033402442932128906 (second)\n",
      "FLA time used: 0.0012984275817871094 (second)\n",
      "FLA time used: 0.001432180404663086 (second)\n",
      "FLA time used: 0.0035028457641601562 (second)\n",
      "FLA time used: 0.0035598278045654297 (second)\n",
      "FLA time used: 0.0075795650482177734 (second)\n",
      "FLA time used: 0.007736682891845703 (second)\n",
      "FLA time used: 0.0013082027435302734 (second)\n",
      "FLA time used: 0.001386880874633789 (second)\n",
      "FLA time used: 0.007258176803588867 (second)\n",
      "FLA time used: 0.0074310302734375 (second)\n",
      "FLA time used: 0.0035257339477539062 (second)\n",
      "FLA time used: 0.0036094188690185547 (second)\n",
      "FLA time used: 0.0012993812561035156 (second)\n",
      "FLA time used: 0.0013875961303710938 (second)\n",
      "FLA time used: 0.01186823844909668 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 6 of 6]0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.01216578483581543 (second)\n",
      "FLA time used: 0.013186454772949219 (second)\n",
      "FLA time used: 0.013508319854736328 (second)\n",
      "FLA time used: 0.014136314392089844 (second)\n",
      "FLA time used: 0.014513254165649414 (second)\n",
      "FLA time used: 0.001804351806640625 (second)\n",
      "FLA time used: 0.0018932819366455078 (second)\n",
      "FLA time used: 0.0035054683685302734 (second)\n",
      "FLA time used: 0.003589630126953125 (second)\n",
      "FLA time used: 0.004260063171386719 (second)\n",
      "FLA time used: 0.00438690185546875 (second)\n",
      "FLA time used: 0.005991697311401367 (second)\n",
      "FLA time used: 0.006071329116821289 (second)\n",
      "FLA time used: 0.007220745086669922 (second)\n",
      "FLA time used: 0.007379770278930664 (second)\n",
      "FLA time used: 0.0042400360107421875 (second)\n",
      "FLA time used: 0.004358530044555664 (second)\n",
      "FLA time used: 0.002382993698120117 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 3 of 3]4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.0024406909942626953 (second)\n",
      "FLA time used: 0.0030214786529541016 (second)\n",
      "FLA time used: 0.0031092166900634766 (second)\n",
      "FLA time used: 0.03331470489501953 (second)\n",
      "FLA time used: 0.03332400321960449 (second)\n",
      "FLA time used: 0.007585048675537109 (second)\n",
      "FLA time used: 0.007714986801147461 (second)\n",
      "FLA time used: 0.013160943984985352 (second)\n",
      "FLA time used: 0.013429641723632812 (second)\n",
      "FLA time used: 0.009419918060302734 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 6 of 6]9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.009672880172729492 (second)\n",
      "FLA time used: 0.025509357452392578 (second)\n",
      "FLA time used: 0.026093244552612305 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 72 of 72]1% [Iteration 23 of 72]"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 8\n",
    "batch_division = {}\n",
    "FL_Score = {}\n",
    "greedyList_All = {}\n",
    "for i in range(100):\n",
    "    batch_division[i] = []\n",
    "    cluster_indice_index = cluster_indices[i]\n",
    "    high_dim_vectors_cluster = high_dim_vectors[cluster_indice_index]\n",
    "    cluster_size = len(cluster_indice_index)\n",
    "    # print(i,cluster_size)\n",
    "    fla_num = int(np.ceil(cluster_size / batch_size))\n",
    "    idx_list,greedyList = do_fla(high_dim_vectors_cluster,high_dim_vectors_cluster.shape[0],number_select=fla_num) ## idx_list is the selected number\n",
    "    \n",
    "    result,coverage = cluster_vectors(high_dim_vectors_cluster,idx_list,batch_size)\n",
    "    for cluster_ind in result.keys():\n",
    "        global_result = [cluster_indice_index[j] for j in result[cluster_ind]] ## 将相对index映射到global index\n",
    "        batch_division[i].append(global_result)\n",
    "    \n",
    "    ### 全局FL分数\n",
    "    _,greedyList_AllElement = do_fla(high_dim_vectors_cluster,high_dim_vectors_cluster.shape[0],number_select=high_dim_vectors_cluster.shape[0]-1)\n",
    "    greedyList_AllElement_dict = {}\n",
    "    for (index,value) in greedyList_AllElement: ## 遍历fla选取\n",
    "        global_index = cluster_indice_index[index] ## 从相对index转为全局index\n",
    "        greedyList_AllElement_dict[global_index] = value \n",
    "    greedyList_All[i] = greedyList_AllElement_dict\n",
    "    # print(result,coverage)\n",
    "    # result = cosine_similarity_clustering(high_dim_vectors_cluster,idx_list,k = fla_num, batch_size=batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = []\n",
    "for key in batch_division.keys():\n",
    "    batch_sampler.extend(batch_division[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_sampler,'ppl/AG-batch.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取phase 2 的Cluster和Grad，计算IF分数，这里考虑使用一个python独立程序来计算\n",
    "## 计算IF Score需要划分eval_dataset，这里需要修改cal_IF_self_divide.py来计算？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=6 python cal_IF_mp.py --yaml_path config.yaml --process_num 3 --total_process_num 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUDA_VISIBLE_DEVICES=4 python cal_IF_mp.py      --yaml_path config.yaml    --process_num 1    --total_process_num 1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command_ppl = 'CUDA_VISIBLE_DEVICES={} python cal_IF_mp.py  \\\n",
    "    --yaml_path {}\\\n",
    "    --process_num {}\\\n",
    "    --total_process_num {}'.format('4','config.yaml',1,1)\n",
    "command_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2106371/1507354576.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sample_IF = torch.load('Influence/AG-p2.pkl')\n"
     ]
    }
   ],
   "source": [
    "sample_IF = torch.load('Influence/AG-p2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2106371/2537682593.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sample_IF = torch.load('Influence/AG-p2.pkl')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def z_score_normalize(sample_IF):\n",
    "    \"\"\"\n",
    "    对sample_IF字典中各个'method'对应的值（键为index、值为float的字典）进行Z-score归一化。\n",
    "\n",
    "    参数:\n",
    "    sample_IF (dict): 包含多个'method'键的字典，每个'method'键对应的值为需要进行归一化处理的字典数据。\n",
    "\n",
    "    返回:\n",
    "    dict: 归一化后的字典，结构与输入的sample_IF一致，其中每个'method'键对应的值都已经完成Z-score归一化。\n",
    "    \"\"\"\n",
    "    for method in sample_IF.keys():\n",
    "        # 获取当前method对应需要归一化的值列表，保持原有顺序\n",
    "        values_list = list(sample_IF[method].values())\n",
    "        # 将列表转换为torch.Tensor\n",
    "        tensor_value = torch.tensor(values_list).unsqueeze(1)  # 添加维度，变为二维张量\n",
    "\n",
    "        # 计算均值和标准差\n",
    "        mean_value = tensor_value.mean()\n",
    "        std_value = tensor_value.std()\n",
    "\n",
    "        # 进行Z-score归一化\n",
    "        normalized_tensor = (tensor_value - mean_value) / std_value\n",
    "\n",
    "        # 将归一化后的结果再转换回列表\n",
    "        normalized_list = normalized_tensor.squeeze(1).tolist()\n",
    "\n",
    "        # 更新原字典中当前method对应的值\n",
    "        index_list = list(sample_IF[method].keys())\n",
    "        normalized_dict = {}\n",
    "        for index, normalized_value in zip(index_list, normalized_list):\n",
    "            normalized_dict[index] = normalized_value\n",
    "\n",
    "        sample_IF[method] = normalized_dict\n",
    "\n",
    "    return sample_IF\n",
    "sample_IF = torch.load('Influence/AG-p2.pkl')\n",
    "sample_IF = z_score_normalize(sample_IF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedyList_All_norm = z_score_normalize(greedyList_All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedyList_All_norm_flatten = {}\n",
    "for key in greedyList_All_norm.keys():\n",
    "    for index in greedyList_All_norm[key]:\n",
    "        greedyList_All_norm_flatten[index] = greedyList_All_norm[key][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2907 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "788   1.0\n",
       "861   1.0\n",
       "933   1.0\n",
       "1010  1.0\n",
       "1283  1.0\n",
       "...   ...\n",
       "1539  1.0\n",
       "1609  1.0\n",
       "1694  1.0\n",
       "2377  1.0\n",
       "2654  1.0\n",
       "\n",
       "[2907 rows x 1 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl_p2 = pd.read_csv('ppl/ppl_qwen2.5-0.5B-AG-short-p2.csv',index_col=0)\n",
    "ppl_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2106371/1546336646.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  total_score[index] += row[0]\n"
     ]
    }
   ],
   "source": [
    "# sample_IF['iterative'\n",
    "# ppl\n",
    "# for i in range(100): ## by cluster division\n",
    "total_score = np.zeros(len(All_Data))\n",
    "## calculate ppl\n",
    "for index,row in ppl.iterrows():\n",
    "    total_score[index] += row[0]\n",
    "## add FL\n",
    "for key in greedyList_All_norm_flatten.keys():\n",
    "    total_score[key] += greedyList_All_norm_flatten[key]\n",
    "## add global_IF\n",
    "for key in sample_IF['iterative']:\n",
    "    total_score[key] -= sample_IF['iterative'][key] ## Influence Score与performance成反比，所以是-="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 遍历cluster,排序\n",
    "cluster_rank = {}\n",
    "for i in range(100): ## 100 is the cluster number, hyper-parameter\n",
    "    cluster_rank[i] = {}\n",
    "    cluster_index = cluster_indices[i]\n",
    "    sorted_index = cluster_index[np.argsort(-total_score[cluster_index])] ## return to global index\n",
    "    cluster_rank[i] = sorted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_choose_index = []\n",
    "cluster_per_budget = 5\n",
    "for i in range(100):\n",
    "    p2_choose_index.extend(cluster_rank[i][:cluster_per_budget])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "{'Output': 'mismatch'}    375\n",
       "{'Output': 'match'}       125\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data.iloc[p2_choose_index]['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(json_data.iloc[p2_choose_index].to_dict(orient='records'), open('train/AG-train-p2.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1:0.7466666666666667 500/3600\n",
    "F1: 0.7670 1000/3600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
