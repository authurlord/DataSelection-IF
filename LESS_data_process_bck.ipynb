{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "def process_datasets(config_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    遍历给定文件夹内的 YAML 配置文件，处理相应的 JSON 文件，并生成 JSONL 格式的输出。\n",
    "\n",
    "    Args:\n",
    "        config_folder (str): 包含 YAML 配置文件的文件夹路径。\n",
    "        output_folder (str): 用于保存新生成的 JSONL 文件的文件夹路径。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    for root, _, files in os.walk(config_folder):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.yaml') and 'config_' in file_name:\n",
    "                yaml_path = os.path.join(root, file_name)\n",
    "                \n",
    "                print(f\"\\nProcessing YAML file: {yaml_path}\")\n",
    "                try:\n",
    "                    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "                        yaml_data = yaml.safe_load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading YAML file {yaml_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if 'train_file_path' in yaml_data:\n",
    "                    json_path = yaml_data['train_file_path']\n",
    "                    dataset_name = yaml_data.get('dataset', 'unknown_dataset')\n",
    "                    task_name = yaml_data.get('task', 'unknown_task')\n",
    "                    \n",
    "                    full_dataset_name = f\"{task_name}_{dataset_name}\"\n",
    "                    \n",
    "                    if not os.path.exists(json_path):\n",
    "                        print(f\"Warning: JSON file not found at {json_path}. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    print(f\"  -> Reading JSON file: {json_path}\")\n",
    "                    try:\n",
    "                        df = pd.read_json(json_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading JSON file {json_path}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 检查所需的列是否存在\n",
    "                    if 'instruction' not in df.columns or 'output' not in df.columns:\n",
    "                        print(f\"Warning: JSON file {json_path} is missing 'instruction' or 'output' column. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    # 创建新的 JSONL 文件名\n",
    "                    full_output_file = os.path.join(output_folder, f\"{full_dataset_name}_full.jsonl\")\n",
    "                    subset_output_file = os.path.join(output_folder, f\"{full_dataset_name}_subset.jsonl\")\n",
    "                    \n",
    "                    # 生成完整数据集\n",
    "                    full_data_list = []\n",
    "                    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"  -> Formatting data\"):\n",
    "                        new_entry = {\n",
    "                            \"dataset\": full_dataset_name,\n",
    "                            \"id\": f\"{full_dataset_name}_{index}\",\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": row['instruction']\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"assistant\",\n",
    "                                    \"content\": row['output']\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                        full_data_list.append(new_entry)\n",
    "                    \n",
    "                    # 保存完整数据集\n",
    "                    with open(full_output_file, 'w', encoding='utf-8') as f:\n",
    "                        for item in full_data_list:\n",
    "                            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                    print(f\"  -> Successfully created full JSONL file: {full_output_file}\")\n",
    "                    \n",
    "                    # 随机抽取1%的数据样本\n",
    "                    sample_size = max(1, int(len(full_data_list) * 0.3))\n",
    "                    subset_data_list = random.sample(full_data_list, sample_size)\n",
    "                    \n",
    "                    # 保存子集\n",
    "                    with open(subset_output_file, 'w', encoding='utf-8') as f:\n",
    "                        for item in subset_data_list:\n",
    "                            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                    print(f\"  -> Successfully created 1% subset JSONL file: {subset_output_file}\")\n",
    "def merge_jsonl_files(input_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    将指定文件夹内的所有 full.jsonl 和 subset.jsonl 文件合并。\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): 包含要合并的 JSONL 文件的文件夹路径。\n",
    "        output_folder (str): 用于保存合并后的文件的文件夹路径。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    # 合并 full.jsonl 文件\n",
    "    full_output_path = os.path.join(output_folder, 'full.jsonl')\n",
    "    full_file_list = [f for f in os.listdir(input_folder) if f.endswith('_full.jsonl')]\n",
    "    \n",
    "    if not full_file_list:\n",
    "        print(\"No files ending with '_full.jsonl' were found. Skipping full file merge.\")\n",
    "    else:\n",
    "        print(f\"Found {len(full_file_list)} full files to merge.\")\n",
    "        with open(full_output_path, 'w', encoding='utf-8') as outfile:\n",
    "            for file_name in full_file_list:\n",
    "                file_path = os.path.join(input_folder, file_name)\n",
    "                print(f\"  -> Merging: {file_name}\")\n",
    "                with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                    for line in infile:\n",
    "                        # 逐行读取并写入到新的文件中\n",
    "                        outfile.write(line)\n",
    "        print(f\"Successfully merged all full files into: {full_output_path}\")\n",
    "        \n",
    "    # ---\n",
    "\n",
    "    # 合并 subset.jsonl 文件\n",
    "    subset_output_path = os.path.join(output_folder, 'subset.jsonl')\n",
    "    subset_file_list = [f for f in os.listdir(input_folder) if f.endswith('_subset.jsonl')]\n",
    "\n",
    "    if not subset_file_list:\n",
    "        print(\"No files ending with '_subset.jsonl' were found. Skipping subset file merge.\")\n",
    "    else:\n",
    "        print(f\"Found {len(subset_file_list)} subset files to merge.\")\n",
    "        with open(subset_output_path, 'w', encoding='utf-8') as outfile:\n",
    "            for file_name in subset_file_list:\n",
    "                file_path = os.path.join(input_folder, file_name)\n",
    "                print(f\"  -> Merging: {file_name}\")\n",
    "                with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                    for line in infile:\n",
    "                        # 逐行读取并写入到新的文件中\n",
    "                        outfile.write(line)\n",
    "        print(f\"Successfully merged all subset files into: {subset_output_path}\")\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_datasets_test(config_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    Traverses YAML configuration files in a given folder, processes the corresponding\n",
    "    JSON test files, and generates a 1% random subset in JSONL format,\n",
    "    following the 'task-dataset-test-subset.jsonl' naming convention.\n",
    "\n",
    "    Args:\n",
    "        config_folder (str): The path to the folder containing the YAML configuration files.\n",
    "        output_folder (str): The path to the folder where the new JSONL files will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    for root, _, files in os.walk(config_folder):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.yaml') and 'config_' in file_name:\n",
    "                yaml_path = os.path.join(root, file_name)\n",
    "\n",
    "                print(f\"\\nProcessing YAML file: {yaml_path}\")\n",
    "                try:\n",
    "                    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "                        yaml_data = yaml.safe_load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading YAML file {yaml_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if 'test_file_path' in yaml_data:\n",
    "                    json_path = yaml_data['test_file_path']\n",
    "                    dataset_name = yaml_data.get('dataset', 'unknown_dataset')\n",
    "                    task_name = yaml_data.get('task', 'unknown_task')\n",
    "\n",
    "                    # Create a unified dataset name from task and dataset\n",
    "                    unified_dataset_name = f\"{task_name}_{dataset_name}\"\n",
    "                    \n",
    "                    if not os.path.exists(json_path):\n",
    "                        print(f\"Warning: JSON file not found at {json_path}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    print(f\"  -> Reading JSON file: {json_path}\")\n",
    "                    try:\n",
    "                        df = pd.read_json(json_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading JSON file {json_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # Check for required columns\n",
    "                    if 'instruction' not in df.columns or 'output' not in df.columns:\n",
    "                        print(f\"Warning: JSON file {json_path} is missing 'instruction' or 'output' column. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Generate the output file name according to the specified standard\n",
    "                    output_file_name = f\"{task_name}-{dataset_name}-test-subset.jsonl\"\n",
    "                    output_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "                    # Generate the full data list for sampling\n",
    "                    full_data_list = []\n",
    "                    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"  -> Formatting data\"):\n",
    "                        new_entry = {\n",
    "                            \"dataset\": unified_dataset_name,\n",
    "                            \"id\": f\"{unified_dataset_name}_{index}\",\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": row['instruction']\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"assistant\",\n",
    "                                    \"content\": row['output']\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                        full_data_list.append(new_entry)\n",
    "\n",
    "                    # Randomly sample 1% of the data\n",
    "                    if not full_data_list:\n",
    "                        print(f\"Warning: No data found in {json_path}. Skipping subset creation.\")\n",
    "                        continue\n",
    "\n",
    "                    sample_size = max(1, int(len(full_data_list) * 0.01))\n",
    "                    subset_data_list = random.sample(full_data_list, sample_size)\n",
    "\n",
    "                    # Save the subset to the new file\n",
    "                    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                        for item in subset_data_list:\n",
    "                            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                    print(f\"  -> Successfully created 1% subset JSONL file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing YAML file: script/config_CTA_WebTable.yaml\n",
      "  -> Reading JSON file: train/CTA/WebTable/WebTable-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 17709/17709 [00:00<00:00, 26900.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/CTA-WebTable-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_RE_RE.yaml\n",
      "  -> Reading JSON file: train/RE/RE/RE-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 2072/2072 [00:00<00:00, 28819.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/RE-RE-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_wdc.yaml\n",
      "  -> Reading JSON file: train/ER/wdc/wdc-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 4398/4398 [00:00<00:00, 29090.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER-wdc-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_semi-text-w.yaml\n",
      "  -> Reading JSON file: train/ER/semi-text-w/semi-text-w-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 1846/1846 [00:00<00:00, 28974.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER-semi-text-w-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_semi-text-c.yaml\n",
      "  -> Reading JSON file: train/ER/semi-text-c/semi-text-c-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 4179/4179 [00:00<00:00, 29397.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER-semi-text-c-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_abt-buy.yaml\n",
      "  -> Reading JSON file: train/ER/abt-buy/abt-buy-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 1916/1916 [00:00<00:00, 29547.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER-abt-buy-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_amazon-google.yaml\n",
      "  -> Reading JSON file: train/ER/amazon-google/amazon-google-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 2289/2289 [00:00<00:00, 29094.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER-amazon-google-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_walmart-amazon.yaml\n",
      "  -> Reading JSON file: train/ER/walmart-amazon/walmart-amazon-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 2049/2049 [00:00<00:00, 29233.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER-walmart-amazon-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DC_hospital.yaml\n",
      "  -> Reading JSON file: train/DC/hospital/hospital-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 508/508 [00:00<00:00, 26829.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DC-hospital-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DC_rayyan.yaml\n",
      "  -> Reading JSON file: train/DC/rayyan/rayyan-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 1117/1117 [00:00<00:00, 28315.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DC-rayyan-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DC_beer.yaml\n",
      "  -> Reading JSON file: train/DC/beer/beer-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 3364/3364 [00:00<00:00, 19387.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DC-beer-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DI_walmart.yaml\n",
      "  -> Reading JSON file: train/DI/walmart/walmart-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 104/104 [00:00<00:00, 24750.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DI-walmart-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DI_amazon.yaml\n",
      "  -> Reading JSON file: train/DI/amazon/amazon-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 816/816 [00:00<00:00, 27456.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DI-amazon-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_SM_CMS.yaml\n",
      "  -> Reading JSON file: train/SM/CMS/CMS-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 5127/5127 [00:00<00:00, 24988.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/SM-CMS-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_AVE_oa_mine.yaml\n",
      "  -> Reading JSON file: train/AVE/oa_mine/oa_mine-test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 2451/2451 [00:00<00:00, 28955.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/AVE-oa_mine-test-subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_CTA_SimTab.yaml\n",
      "  -> Reading JSON file: /data/home/wangys/MELD/dataset/CTA/SimTab_test_few.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Formatting data: 100%|██████████| 7610/7610 [00:00<00:00, 29142.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/CTA-SimTab-test-subset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_datasets_test('script/','../LESS/LESS_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_6687</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_1554</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_7171</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_14472</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_349</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29921</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_6183</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29922</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_3096</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29923</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_3508</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29924</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_5772</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29925</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_637</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29926 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dataset                  id  \\\n",
       "0      CTA_WebTable   CTA_WebTable_6687   \n",
       "1      CTA_WebTable   CTA_WebTable_1554   \n",
       "2      CTA_WebTable   CTA_WebTable_7171   \n",
       "3      CTA_WebTable  CTA_WebTable_14472   \n",
       "4      CTA_WebTable    CTA_WebTable_349   \n",
       "...             ...                 ...   \n",
       "29921    CTA_SimTab     CTA_SimTab_6183   \n",
       "29922    CTA_SimTab     CTA_SimTab_3096   \n",
       "29923    CTA_SimTab     CTA_SimTab_3508   \n",
       "29924    CTA_SimTab     CTA_SimTab_5772   \n",
       "29925    CTA_SimTab      CTA_SimTab_637   \n",
       "\n",
       "                                                messages  \n",
       "0      [{'role': 'user', 'content': 'You are an exper...  \n",
       "1      [{'role': 'user', 'content': 'You are an exper...  \n",
       "2      [{'role': 'user', 'content': 'You are an exper...  \n",
       "3      [{'role': 'user', 'content': 'You are an exper...  \n",
       "4      [{'role': 'user', 'content': 'You are an exper...  \n",
       "...                                                  ...  \n",
       "29921  [{'role': 'user', 'content': 'You are an exper...  \n",
       "29922  [{'role': 'user', 'content': 'You are an exper...  \n",
       "29923  [{'role': 'user', 'content': 'You are an exper...  \n",
       "29924  [{'role': 'user', 'content': 'You are an exper...  \n",
       "29925  [{'role': 'user', 'content': 'You are an exper...  \n",
       "\n",
       "[29926 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('../LESS/LESS_data/subset.jsonl',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradient-calculation': 1577.7494127750397, 'IF-Score': 241.13814163208008}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "time_dict = np.load('Influence_single/CTA/WebTable/time.npy',allow_pickle=True).item()\n",
    "time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29926])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "result = torch.load('../LESS/LESS_output/CTA_SimTab/influence_score.pt')\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0221,  0.0072,  0.0150,  ...,  0.0252, -0.0096,  0.0194],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_708</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_614</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_3391</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_3724</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_2936</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_12957</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_9556</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_941</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_1813</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_5379</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>992 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dataset                  id  \\\n",
       "0      CTA_SimTab      CTA_SimTab_708   \n",
       "1      CTA_SimTab      CTA_SimTab_614   \n",
       "2      CTA_SimTab     CTA_SimTab_3391   \n",
       "3      CTA_SimTab     CTA_SimTab_3724   \n",
       "4      CTA_SimTab     CTA_SimTab_2936   \n",
       "..            ...                 ...   \n",
       "987  CTA_WebTable  CTA_WebTable_12957   \n",
       "988  CTA_WebTable   CTA_WebTable_9556   \n",
       "989    CTA_SimTab      CTA_SimTab_941   \n",
       "990    CTA_SimTab     CTA_SimTab_1813   \n",
       "991  CTA_WebTable   CTA_WebTable_5379   \n",
       "\n",
       "                                              messages  \n",
       "0    [{'role': 'user', 'content': 'You are an exper...  \n",
       "1    [{'role': 'user', 'content': 'You are an exper...  \n",
       "2    [{'role': 'user', 'content': 'You are an exper...  \n",
       "3    [{'role': 'user', 'content': 'You are an exper...  \n",
       "4    [{'role': 'user', 'content': 'You are an exper...  \n",
       "..                                                 ...  \n",
       "987  [{'role': 'user', 'content': 'You are an exper...  \n",
       "988  [{'role': 'user', 'content': 'You are an exper...  \n",
       "989  [{'role': 'user', 'content': 'You are an exper...  \n",
       "990  [{'role': 'user', 'content': 'You are an exper...  \n",
       "991  [{'role': 'user', 'content': 'You are an exper...  \n",
       "\n",
       "[992 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_json('../LESS/LESS_output/CTA_SimTab/LESS_subset.jsonl',lines=True)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1026305/2719693357.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  instruction = row[-1][0]['content']\n",
      "/tmp/ipykernel_1026305/2719693357.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  output = row[-1][1]['content']\n"
     ]
    }
   ],
   "source": [
    "task = 'CTA'\n",
    "dataset = 'SimTab'\n",
    "pd.read_json('LESS_output/{task}_{dataset}/LESS_subset.jsonl',lines=True)\n",
    "def content_extraction(row):\n",
    "    instruction = row[-1][0]['content']\n",
    "    input = ''\n",
    "    output = row[-1][1]['content']\n",
    "    return instruction,input,output\n",
    "content_subset = file.apply(content_extraction,axis=1,result_type='expand')\n",
    "content_subset.columns = ['instruction','input','output']\n",
    "json.dump(content_subset.to_dict(orient='records'), open(f'../DataSelection-IF/train/{task}/{dataset}/train-select-w-LESS.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
