{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from dataloader import create_dataloaders\n",
    "# from dataloader_ER import create_dataloaders\n",
    "# from lora_model import LORAEngine,LORAEngineDeberta,LORAEngineDebertaMultiClass\n",
    "from lora_model_vmap import LORAEngineDebertaMultiClass\n",
    "\n",
    "# from influence import IFEngine\n",
    "from influence_hyperinf import IFEngine\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path=\"roberta-large\"\n",
    "# model_name_or_path=\"/home/yanmy/roberta-base\"\n",
    "model_name_or_path=\"../../model/deberta-v3-base\"\n",
    "# model_name_or_path=\"../../model/roberta-large\"\n",
    "task=\"mrpc\"\n",
    "# task = \"qnli\"\n",
    "noise_ratio=0\n",
    "batch_size=32\n",
    "# target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "# target_modules = ['value']\n",
    "target_modules=[\"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "\n",
    "device=\"cuda:0\"\n",
    "num_epochs=3\n",
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT\n",
    "# model_name_or_path=\"roberta-large\"\n",
    "# model_name_or_path=\"/home/yanmy/roberta-base\"\n",
    "# model_name_or_path=\"../../model/deberta-v3-base\"\n",
    "model_name_or_path = '/data/home/wangys/model/bert-base-uncased/'\n",
    "# model_name_or_path=\"../../model/roberta-large\"\n",
    "task=\"mrpc\"\n",
    "# task = \"qnli\"\n",
    "noise_ratio=0\n",
    "batch_size=32\n",
    "# target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "target_modules = ['value']\n",
    "# target_modules=[\"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "\n",
    "device=\"cuda:0\"\n",
    "num_epochs=3\n",
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning models\n",
    "# dataloader_outputs = create_dataloaders(model_name_or_path=model_name_or_path,\n",
    "#                                            task=task,\n",
    "#                                            noise_ratio=noise_ratio,\n",
    "#                                            batch_size=batch_size,\n",
    "#                                            train_file = '../ER/semi-text-c-FUSER/train.json',\n",
    "#                                            valid_file = '../ER/semi-text-c-FUSER/valid.json',\n",
    "#                                            test_file = '../ER/semi-text-c-FUSER/test.json')\n",
    "\n",
    "\n",
    "dataloader_outputs = create_dataloaders(model_name_or_path=model_name_or_path,\n",
    "                                           task=task,\n",
    "                                           noise_ratio=noise_ratio,\n",
    "                                           batch_size=batch_size,\n",
    "                                        #    train_file = '../ER/semi-text-c/train.json',\n",
    "                                        #    valid_file = '../ER/semi-text-c/valid.json',\n",
    "                                        #    test_file = '../ER/semi-text-c/test.json',\n",
    "                                           max_length=256)\n",
    "# train_dataloader, eval_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "train_dataloader, eval_dataloader,test_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "\n",
    "\n",
    "# lora_engine = LORAEngine(model_name_or_path=model_name_or_path,\n",
    "#                             target_modules=target_modules,\n",
    "#                             train_dataloader=train_dataloader,\n",
    "#                             eval_dataloader=eval_dataloader,\n",
    "#                             device=device,\n",
    "#                             num_epochs=num_epochs,\n",
    "#                             lr=lr,\n",
    "#                             low_rank=8, \n",
    "#                             task=task)\n",
    "\n",
    "lora_engine = LORAEngineDebertaMultiClass(model_name_or_path=model_name_or_path,\n",
    "                            target_modules=target_modules,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            # eval_dataloader=eval_dataloader,\n",
    "                            eval_dataloader=eval_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            device=device,\n",
    "                            num_epochs=num_epochs,\n",
    "                            lr=lr,\n",
    "                            low_rank=16, \n",
    "                            task=task,\n",
    "                            save_path = '../models/mrpc',\n",
    "                            valid_each_epoch=True,\n",
    "                            cal_grad_per_sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2451, 2325, 3258,  243, 3213, 2449, 1249, 3573, 1120, 2242, 1643,  370,\n",
      "        1781,  229, 1843, 1983, 2358, 2732, 3303, 2340,   96, 1886, 1372, 2255,\n",
      "        3243,  217, 2429, 1166, 2277, 1968,  729,  453])\n"
     ]
    }
   ],
   "source": [
    "for step,batch in enumerate(train_dataloader):\n",
    "    print(batch['id'])\n",
    "    break\n",
    "    # print(batch['input_ids'][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "lora_engine.eval_LORA_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ../../model/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 184,720,132 || trainable%: 0.1605\n",
      "Total Steps:345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/115 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 115/115 [00:05<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 0.5401092377693757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 22.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: {'f1': 0.9192982456140351}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:09<00:00, 11.51it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.69it/s]\n",
      "100%|██████████| 115/115 [00:08<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 0.3194727790744408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 22.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: {'f1': 0.9125214408233276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.41it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.66it/s]\n",
      "100%|██████████| 115/115 [00:08<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 0.20453087376511614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 22.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: {'f1': 0.9200710479573713}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:10<00:00, 11.43it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.70it/s]\n",
      "100%|██████████| 54/54 [00:02<00:00, 22.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Result on Test Data: {'f1': 0.9184805804524114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_engine.build_LORA_model()\n",
    "tr_grad_dict,val_grad_dict = lora_engine.train_LORA_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(tr_grad_dict,'../output/tr_grad_dict_all.pkl')\n",
    "torch.save(val_grad_dict,'../output/val_grad_dict_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_engine.save('../models/mrpc-all')\n",
    "# lora_engine.load('../models/qnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_grad_dict[0][0]['base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_B.default.weight'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-17 03:14:17,312] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "lora_engine.save_lora('../models/mrpc-deberta-lora')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_engine.load_pretrained_network('../models/mrpc-deberta-lora')\n",
    "lora_engine.eval_LORA_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict, val_grad_dict = lora_engine.compute_gradient_iterative(tokenized_datasets, collate_fn, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tr_grad_dict = torch.load('../output/tr_grad_dict_all.pkl')\n",
    "val_grad_dict = torch.load('../output/val_grad_dict_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle, os\n",
    "import torch\n",
    "import numpy as np\n",
    "def compute_hvp_and_IF(tr_grad_dict, val_grad_avg_dict, weight_name, device='cuda:3', lambda_const_param=10, n_iteration=30):\n",
    "    '''\n",
    "    Compute the influence function score by our method HyperINF\n",
    "    '''\n",
    "\n",
    "    def schulz_inverse_stable(A, damping_factor=0, max_iterations=20, tol=1e-6):\n",
    "        n = A.shape[0]\n",
    "        I = torch.eye(n, device=A.device)\n",
    "        A_damped = A + damping_factor * I  # Apply damping for stability\n",
    "        X = torch.eye(n, device=A.device) * 0.00005  # Initial estimate of inverse matrix\n",
    "\n",
    "        for _ in range(max_iterations):\n",
    "            X = X @ (2 * I - A_damped @ X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    start_time = time()\n",
    "    hvp_iterative_dict = {}\n",
    "    if_tmp_dict = {}\n",
    "\n",
    "    # 将相关变量放到指定设备上\n",
    "    # tr_grad_dict_layer = {tr_id: tr_grad_dict[tr_id][weight_name].to(device) for tr_id in tr_grad_dict}\n",
    "    # val_grad_avg_dict_layer = val_grad_avg_dict[weight_name].to(device)\n",
    "    \n",
    "    tr_grad_dict_layer = {tr_id: tr_grad_dict[tr_id][weight_name] for tr_id in tr_grad_dict}\n",
    "    val_grad_avg_dict_layer = val_grad_avg_dict[weight_name]\n",
    "    # print(val_grad_avg_dict_layer['base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_B.default.weight'].device)\n",
    "\n",
    "    for tr_id in tr_grad_dict:\n",
    "        if_tmp_value = 0\n",
    "\n",
    "        # lambda_const computation\n",
    "        S = torch.zeros(len(tr_grad_dict.keys())).to(device)\n",
    "        for tr_id_inner in tr_grad_dict:\n",
    "            tmp_grad = tr_grad_dict_layer[tr_id_inner]\n",
    "            S[tr_id_inner] = torch.mean(tmp_grad**2)\n",
    "        lambda_const = torch.mean(S) / lambda_const_param  # layer-wise lambda\n",
    "\n",
    "        # iterative hvp computation\n",
    "        G_l = torch.zeros((tr_grad_dict_layer[0].T @ tr_grad_dict_layer[0]).shape).to(device)\n",
    "        for tr_id_inner in tr_grad_dict:\n",
    "            tmp_grad = tr_grad_dict_layer[tr_id_inner]\n",
    "            G_l += tmp_grad.T @ tmp_grad / len(tr_grad_dict)\n",
    "        \n",
    "        G_l = G_l + lambda_const * torch.eye(G_l.shape[0], device=G_l.device)\n",
    "        # G_l_inv = torch.inverse(G_l)\n",
    "        G_l_inv = schulz_inverse_stable(G_l, damping_factor=0.001, max_iterations=n_iteration, tol=1e-6)\n",
    "        # print(val_grad_avg_dict_layer.device,G_l_inv.device)\n",
    "        hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n",
    "\n",
    "        if_tmp_value += torch.sum(hvp_iterative_dict[weight_name] * tr_grad_dict_layer[tr_id])\n",
    "        if_tmp_dict[tr_id] = -if_tmp_value.cpu()\n",
    "\n",
    "    # 从设备中删除变量\n",
    "    del tr_grad_dict_layer\n",
    "    del val_grad_avg_dict_layer\n",
    "\n",
    "    IF_dict = pd.Series(if_tmp_dict, dtype=float).to_numpy()\n",
    "    time_taken = time() - start_time\n",
    "\n",
    "    print(\"Time taken for HyperINF: \", time_taken)\n",
    "    return hvp_iterative_dict, IF_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_grad_avg(val_grad_dict):\n",
    "    # Compute the avg gradient on the validation dataset\n",
    "    n_val = len(val_grad_dict)\n",
    "    val_grad_avg_dict={}\n",
    "    for weight_name in val_grad_dict[0]:\n",
    "        if weight_name.__contains__('base_model'):\n",
    "            val_grad_avg_dict[weight_name]=torch.zeros(val_grad_dict[0][weight_name].shape)\n",
    "            for val_id in val_grad_dict:\n",
    "                val_grad_avg_dict[weight_name] += val_grad_dict[val_id][weight_name] / n_val\n",
    "        # else:\n",
    "        #     val_grad_avg_dict[weight_name] = val_grad_dict[weight_name]\n",
    "    # val_grad_avg_dict['ids'] = val_grad_dict['ids']\n",
    "    return val_grad_avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_grad_avg_dict = {}\n",
    "for epoch in val_grad_dict.keys():\n",
    "    val_grad_avg_dict[epoch] = compute_val_grad_avg(val_grad_dict[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e072f1fa1b54860b5fceda4629f9701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6e2af198b44491910d0b40a4042113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c947c9f0271428b8f34cc50d10da63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aeedbf6649e435db4a119b651f2ea18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638baa23feda4dc99274994858929cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0dc9edd65436da14592ef23f56be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IF_device = 'cuda:3' ## 避免爆显存\n",
    "# tr_grad_dict_IF = tr_grad_dict.copy()\n",
    "# val_grad_dict_IF = val_grad_dict.copy()\n",
    "# ## to cuda\n",
    "for epoch in tr_grad_dict.keys():\n",
    "    for key in tqdm(tr_grad_dict[epoch]): ## 尝试转为fp16计算\n",
    "        for kk in tr_grad_dict[epoch][key]:\n",
    "            tr_grad_dict[epoch][key][kk] = tr_grad_dict[epoch][key][kk].to(IF_device)\n",
    "\n",
    "    for key in tqdm(val_grad_avg_dict[epoch].keys()):\n",
    "        # for kk in val_grad_avg_dict[epoch][key]:\n",
    "        val_grad_avg_dict[epoch][key] = val_grad_avg_dict[epoch][key].to(IF_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_grad_avg_dict[0]['base_model.model.classifier.modules_to_save.default.weight'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_grad_avg_dict[0]['base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_B.default.weight'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38191a92731f4e3e8e8a4cbae9f62209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3718876838684082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3316936492919922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3280389308929443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3344578742980957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.328200340270996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3270502090454102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3264994621276855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3270211219787598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3302111625671387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3287537097930908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3288626670837402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3270528316497803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.336942195892334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3272490501403809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3297312259674072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3369574546813965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3337726593017578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3300435543060303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.3837590217590332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.4092977046966553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.4087483882904053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.4112942218780518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.4087378978729248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.4120762348175049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3177074/675338865.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(val_grad_avg_dict_layer @ G_l_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  1.4067926406860352\n"
     ]
    }
   ],
   "source": [
    "IF_dict_all = {}\n",
    "for weight_name in tqdm(val_grad_avg_dict[0].keys()):\n",
    "    _,IF = compute_hvp_and_IF(tr_grad_dict=tr_grad_dict[0],\n",
    "                            val_grad_avg_dict=val_grad_avg_dict[0],\n",
    "                            weight_name=weight_name,\n",
    "                            device=IF_device)\n",
    "    IF_dict_all[weight_name] = IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
