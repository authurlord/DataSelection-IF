{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    "\n",
    "# Dataset id from huggingface.co/dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['The text delves into the intricate storytelling mechanisms of video games across various genres. It explores how developers use narrative techniques such as branching storylines and non-linear progression to engage players deeply in their gaming experience. The analysis includes a comparison with traditional forms of storytelling, like novels and films, highlighting unique features found only in interactive media.',\n",
       "  'In recent years, there has been a substantial shift in the way we communicate and access information, driven largely by advancements in internet technology and telecommunications. These improvements have fundamentally changed how we interact with each other and with various forms of media, transforming daily activities such as shopping, banking, education, entertainment, and more into experiences that are increasingly digital-centric. This trend is not merely a product of consumer demand but also reflects significant innovations from leading tech companies who continue to push the boundaries of what’s possible in data transmission speeds, reliability, device integration, cybersecurity measures, and the development of smarter devices capable of integrating seamlessly with internet services. Furthermore, this shift has implications for policy-making at local and global levels, raising new questions about privacy rights, digital literacy, and equitable access to these technologies worldwide.'],\n",
       " 'label': [19, 9]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = \"/data/home/wangys/DataSelection-IF/datasets/synthetic\"\n",
    "\n",
    "# Load raw dataset\n",
    "train_dataset = load_dataset(dataset_id, split='train')\n",
    "\n",
    "split_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "split_dataset['train'][5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/900 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 900/900 [00:00<00:00, 4268.76 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 4397.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Model id to load the tokenizer\n",
    "model_id = \"../../model/ModernBERT-base/\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Tokenize helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True,padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize dataset\n",
    "if \"label\" in split_dataset[\"train\"].features.keys():\n",
    "    split_dataset =  split_dataset.rename_column(\"label\", \"labels\") # to match Trainer\n",
    "tokenized_dataset = split_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertForSequenceClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at ../../model/ModernBERT-base/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Model id to load the tokenizer\n",
    "# model_id = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "# Prepare model labels - useful for inference\n",
    "labels = tokenized_dataset[\"train\"].features[\"labels\"].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "# Download the model from huggingface.co/models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/bert24/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 01:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.511188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.721274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.704200</td>\n",
       "      <td>0.400104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.704200</td>\n",
       "      <td>0.492610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.481440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=0.34063020186954074, metrics={'train_runtime': 99.4246, 'train_samples_per_second': 45.26, 'train_steps_per_second': 11.315, 'total_flos': 1120294678932000.0, 'train_loss': 0.34063020186954074, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"ModernBERT-domain-classifier\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    bf16=False, # bfloat16 training\n",
    "    optim=\"adamw_torch_fused\", # improved optimizer\n",
    "    # logging & evaluation strategies\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    # load_best_model_at_end=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: business-and-industrial\n",
      "Actual Label: arts-and-entertainment\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: computers-and-electronics\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: finance\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: real-estate\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: sports\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: food-and-drink\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: people-and-society\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: games\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: pets-and-animals\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: real-estate\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: real-estate\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: pets-and-animals\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: internet-and-telecom\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: autos-and-vehicles\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: travel-and-transportation\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: travel-and-transportation\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: law-and-government\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: home-and-garden\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: science\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: science\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: games\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: games\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: sensitive-subjects\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: people-and-society\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: arts-and-entertainment\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: shopping\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: people-and-society\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: arts-and-entertainment\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: health\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: travel-and-transportation\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: people-and-society\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: home-and-garden\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: home-and-garden\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: books-and-literature\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: real-estate\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: games\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: pets-and-animals\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: health\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: health\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: health\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: people-and-society\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: law-and-government\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: food-and-drink\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: finance\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: autos-and-vehicles\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: hobbies-and-leisure\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: science\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: adult\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: pets-and-animals\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: finance\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: business-and-industrial\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: beauty-and-fitness\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: computers-and-electronics\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: travel-and-transportation\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: games\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: business-and-industrial\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: law-and-government\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: health\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: sports\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: business-and-industrial\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: shopping\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: beauty-and-fitness\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: jobs-and-education\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: arts-and-entertainment\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: food-and-drink\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: business-and-industrial\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: science\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: computers-and-electronics\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: law-and-government\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: food-and-drink\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: health\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: pets-and-animals\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: internet-and-telecom\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: home-and-garden\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: books-and-literature\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: science\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: shopping\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: sports\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: online-communities\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: travel-and-transportation\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: internet-and-telecom\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: online-communities\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: science\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: home-and-garden\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: science\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: health\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: travel-and-transportation\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: business-and-industrial\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: health\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: travel-and-transportation\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: food-and-drink\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: law-and-government\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: real-estate\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: travel-and-transportation\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: computers-and-electronics\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: people-and-society\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: computers-and-electronics\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: science\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: books-and-literature\n",
      "Predicted Label: business-and-industrial\n",
      "Actual Label: home-and-garden\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# Process the prediction results (predictions, label_ids, metrics)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "for i in range(100):\n",
    "    predicted_label = id2label[str(predicted_labels[i])]\n",
    "    example_data = split_dataset['test']\n",
    "\n",
    "\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    print(f\"Actual Label: {id2label[str(example_data[i]['labels'])]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
