{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import math\n",
    "from submodlib.functions.facilityLocation import FacilityLocationFunction\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def do_fla(X, number_all, number_select):\n",
    "    start_time = time.time()\n",
    "\n",
    "    Y = X\n",
    "    obj = FacilityLocationFunction(n=number_all, mode=\"dense\", data=Y, metric=\"euclidean\")\n",
    "    greedyList = obj.maximize(budget=number_select, optimizer='LazyGreedy', stopIfZeroGain=False, stopIfNegativeGain=False, verbose=False)\n",
    "    idx_list = [tuple_i[0] for tuple_i in greedyList]\n",
    "\n",
    "    print('FLA time used:',(time.time()-start_time)/60,'(min)')\n",
    "    return idx_list\n",
    "\n",
    "def do_ifd(args, json_data):\n",
    "    def sort_key(x):\n",
    "        # Check if the value is nan\n",
    "        if math.isnan(x[args.key_name]):\n",
    "            return (0, 0) \n",
    "        return (1, x[args.key_name]) \n",
    "    \n",
    "    # The types for the ifd filtering\n",
    "    # 1. Given the ifd_num, choose the highese ifd_num samples\n",
    "    # 2. Not given the ifd_num, choose between bounds\n",
    "\n",
    "    # Both the above needs to define the ifd_lupper\n",
    "    if args.ifd_lupper != 0:\n",
    "        filtered_data = [x for x in json_data if (isinstance(x[args.key_name], (int, float)) and x[args.key_name] < args.ifd_lupper)]\n",
    "    else:\n",
    "        filtered_data = json_data\n",
    "\n",
    "    if args.ifd_num != 0:\n",
    "        new_data = sorted(filtered_data, key=sort_key, reverse=True)\n",
    "        new_data = new_data[:args.ifd_num]\n",
    "    else:\n",
    "        new_data = [x for x in filtered_data if (isinstance(x[args.key_name], (int, float)) and x[args.key_name] > args.ifd_lower)]\n",
    "    \n",
    "    print('Data length after ifd:', len(new_data))\n",
    "    return new_data\n",
    "\n",
    "def combine_sentences(args, filtered_data):\n",
    "    sent_all = []\n",
    "    for dict_i in filtered_data:\n",
    "\n",
    "        if 'input' in dict_i.keys():\n",
    "            instruction_i = dict_i['instruction'] + '\\n' + dict_i['input'] + '\\n'\n",
    "        else:\n",
    "            instruction_i = dict_i['instruction'] + '\\n'\n",
    "\n",
    "        if args.emb_type == 'whole':\n",
    "            sent_all.append(instruction_i+dict_i['output'])\n",
    "        elif args.emb_type == 'instruction':\n",
    "            sent_all.append(instruction_i)\n",
    "        elif args.emb_type == 'response':\n",
    "            sent_all.append(dict_i['output'])\n",
    "\n",
    "    return sent_all\n",
    "\n",
    "# Function to get embeddings for a list of sentences\n",
    "def get_sentence_embeddings(sentences, model_name, batch_size):\n",
    "\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Initialize the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    all_sentence_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "        # Process sentences in batches\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        # Tokenize sentences and convert to input format expected by the model\n",
    "        encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "        # Move encoded input to the same device as the model\n",
    "        encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}\n",
    "\n",
    "        # Get model's output (without any specific head)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "        all_sentence_embeddings.append(sentence_embeddings)\n",
    "\n",
    "    # Concatenate all embeddings from each batch\n",
    "    all_sentence_embeddings = torch.cat(all_sentence_embeddings, dim=0)\n",
    "\n",
    "    return all_sentence_embeddings.cpu()\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--json_data_path\", type=str, default='')\n",
    "    parser.add_argument(\"--json_save_path\", type=str, default='')\n",
    "\n",
    "    parser.add_argument(\"--ifd_num\", type=int, default=520)\n",
    "    parser.add_argument(\"--ifd_lower\", type=float, default=0.9, help='the lower bound for ifd filtering')\n",
    "    parser.add_argument(\"--ifd_lupper\", type=float, default=1.0, help='the upper bound for ifd filtering')\n",
    "    parser.add_argument(\"--key_name\", type=str, default='ifd_ppl',help='ifd_ppl')\n",
    "\n",
    "    parser.add_argument(\"--emb_model\", type=str, default='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    parser.add_argument(\"--emb_type\", type=str, default='whole', help='whole, instruction, response')\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "\n",
    "    parser.add_argument(\"--fla_num\", type=int, default=100)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.json_data_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # filter by ifd\n",
    "    filtered_data = do_ifd(args,json_data)\n",
    "\n",
    "    # get the embedding\n",
    "    sentences = combine_sentences(args, filtered_data)\n",
    "    embeddings = get_sentence_embeddings(sentences, args.emb_model, args.batch_size)\n",
    "\n",
    "    # do fla\n",
    "    X = embeddings.numpy()\n",
    "    fla_idxs = do_fla(X, len(sentences),args.fla_num)\n",
    "\n",
    "    final_json_data_ori = [filtered_data[i] for i in fla_idxs]\n",
    "    with open(args.json_save_path, 'w') as file:\n",
    "        json.dump(final_json_data_ori, file, indent=4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "model = FlagModel('../sentence_transformer_model/bge-large-en-1.5/', \n",
    "                  use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 18/18 [00:08<00:00,  2.19it/s]\n",
      "Inference Embeddings: 100%|██████████| 18/18 [00:03<00:00,  4.81it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m embedding_a \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(sent_1)\n\u001b[1;32m      6\u001b[0m embedding_b \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(sent_2)\n\u001b[0;32m----> 7\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m([sent_1,sent_2])\n",
      "File \u001b[0;32m~/anaconda3/envs/deepspeed/lib/python3.12/site-packages/numpy/__init__.py:333\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved in NumPy 1.25.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTester was removed in NumPy 1.25.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_json('ER/semi-text-c/train.json')\n",
    "sent_1 = train.iloc[:,0].to_list()\n",
    "sent_2 = train.iloc[:,1].to_list()\n",
    "embedding_a = model.encode(sent_1)\n",
    "embedding_b = model.encode(sent_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 1024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 2048)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = np.concatenate([embedding_a,embedding_b],axis=1)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fla(X, number_all, number_select):\n",
    "    start_time = time.time()\n",
    "\n",
    "    Y = X\n",
    "    obj = FacilityLocationFunction(n=number_all, mode=\"clustered\", data=Y, metric=\"cosine\")\n",
    "    greedyList = obj.maximize(budget=number_select, optimizer='LazyGreedy', stopIfZeroGain=False, stopIfNegativeGain=False, verbose=False)\n",
    "    idx_list = [tuple_i[0] for tuple_i in greedyList]\n",
    "\n",
    "    print('FLA time used:',(time.time()-start_time),'(second)')\n",
    "    return idx_list,greedyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fla(X, number_all, number_select):\n",
    "    start_time = time.time()\n",
    "\n",
    "    Y = X\n",
    "    obj = FacilityLocationFunction(n=number_all, mode=\"dense\", data=Y, metric=\"cosine\")\n",
    "    greedyList = obj.maximize(budget=number_select, optimizer='LazyGreedy', stopIfZeroGain=False, stopIfNegativeGain=False, verbose=False)\n",
    "    idx_list = [tuple_i[0] for tuple_i in greedyList]\n",
    "\n",
    "    print('FLA time used:',(time.time()-start_time),'(second)')\n",
    "    return idx_list,greedyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 27.0569167137146 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 500 of 500]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fla_idxs,greedyList = do_fla(embedding_b, embedding_b.shape[0],500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
