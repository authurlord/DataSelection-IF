{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import  KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from FlagEmbedding import FlagModel\n",
    "import time\n",
    "import submodlib\n",
    "from submodlib.functions.facilityLocation import FacilityLocationFunction\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--pt_data_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--json_data_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--json_save_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--sent_type\", type=int, default=0)\n",
    "    parser.add_argument(\"--ppl_type\", type=int, default=0)\n",
    "    parser.add_argument(\"--cluster_method\", type=str, default='kmeans')\n",
    "    parser.add_argument(\"--reduce_method\", type=str, default='tsne')\n",
    "    parser.add_argument(\"--sample_num\", type=int, default=10)\n",
    "    parser.add_argument(\"--kmeans_num_clusters\", type=int, default=100)\n",
    "    parser.add_argument(\"--low_th\", type=int, default=1)\n",
    "    parser.add_argument(\"--up_th\", type=int, default=99)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def do_clustering(high_dim_vectors,cluster_method='kmeans',kmeans_num_clusters=100):\n",
    "\n",
    "    clustering_algorithm = cluster_method\n",
    "    if clustering_algorithm == 'kmeans':\n",
    "        clustering = KMeans(n_clusters=kmeans_num_clusters, random_state=0).fit(high_dim_vectors)\n",
    "    \n",
    "    return clustering\n",
    "\n",
    "def do_reduce_dim(high_dim_vectors):\n",
    "    # Perform t-SNE for visualization\n",
    "    # if args.reduce_method == 'tsne':\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    low_dim_vectors = tsne.fit_transform(high_dim_vectors)\n",
    "    return low_dim_vectors\n",
    "\n",
    "def sample_middle_confidence_data(cluster_labels, confidences, n, low_th=25, up_th=75):\n",
    "    num_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "    # Get the indices for each cluster\n",
    "    cluster_indices = {i: np.where(cluster_labels == i)[0] for i in range(num_clusters)}\n",
    "    \n",
    "    # Create a dictionary to store the indices of the middle level confidence samples\n",
    "    middle_confidence_samples = {}\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        # Get the sorted indices for this cluster\n",
    "        sorted_indices = cluster_indices[i]\n",
    "        \n",
    "        # If there are less than n samples in this class, just return all of them\n",
    "        if len(sorted_indices) < n:\n",
    "            middle_confidence_samples[i] = sorted_indices\n",
    "            continue\n",
    "\n",
    "        # Get the confidences for this cluster\n",
    "        cluster_confidences = confidences[sorted_indices]\n",
    "        lower_threshold = np.percentile(cluster_confidences, low_th)\n",
    "        upper_threshold = np.percentile(cluster_confidences, up_th)\n",
    "\n",
    "        # Get the indices of the samples within the middle level confidence range\n",
    "        middle_indices = sorted_indices[(cluster_confidences >= lower_threshold) & (cluster_confidences <= upper_threshold)]\n",
    "        \n",
    "        # If there are less than n samples in the middle range, use all of them\n",
    "        if len(middle_indices) < n:\n",
    "            middle_confidence_samples[i] = middle_indices\n",
    "        else:\n",
    "            # Calculate step size for even sampling\n",
    "            step_size = len(middle_indices) // n\n",
    "            # Select evenly from the middle level confidence samples\n",
    "            middle_confidence_samples[i] = middle_indices[::step_size][:n]\n",
    "\n",
    "    return middle_confidence_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings by SentBert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse LLM File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data = pd.read_json('/data/home/wangys/transfer-er/Pipeline/Amazon-Google/LLM_file/Amazon-Google-Train-Match-P1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_input_length(row):\n",
    "    input = row['instruction']\n",
    "    output = input.split('at the final judgement.')[1].split('Take these examples as reference:')[0]\n",
    "    return output\n",
    "All_Data['instruction'] = All_Data.apply(cut_input_length,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2106371/2444698609.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = row[0]\n",
      "/tmp/ipykernel_2106371/2444698609.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = eval(row[-1])['Output']\n"
     ]
    }
   ],
   "source": [
    "# text = All_Data.iloc[0,0]\n",
    "left_list = []\n",
    "right_list = []\n",
    "label_list = []\n",
    "for index,row in All_Data.iterrows():\n",
    "    text = row[0]\n",
    "    Entity_1 = text.split('\\n\\nEntity 1:')[1].split('\\n\\nEntity 2')[0]\n",
    "    Entity_2 = text.split('\\n\\nEntity 2:')[1].split('\\n\\nTake these examples as reference:')[0] \n",
    "    label = eval(row[-1])['Output']\n",
    "    left_list.append(Entity_1)\n",
    "    right_list.append(Entity_2)\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 15/15 [00:01<00:00,  7.63it/s]\n",
      "Inference Embeddings: 100%|██████████| 15/15 [00:02<00:00,  7.26it/s]\n",
      "Inference Embeddings: 100%|██████████| 15/15 [00:00<00:00, 79.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'\n",
    "model = FlagModel('../sentence_transformer_model/bge-large-en-1.5/', \n",
    "                  use_fp16=True)\n",
    "embedding_a = model.encode(left_list)\n",
    "embedding_b = model.encode(right_list)\n",
    "embedding_c = model.encode(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppl = pd.read_json('/data/home/wangys/DataSelection-IF/ppl/ppl_qwen2.5-7B-AG-Short.json')\n",
    "ppl = pd.read_csv('ppl/ppl_qwen2.5-0.5B-AG-short.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3663, 3072)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ppl_list = ppl.iloc[:,0].to_list()\n",
    "pt_data = np.concatenate([embedding_a,embedding_b,embedding_c],axis=1)\n",
    "pt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "{'Output': 'mismatch'}    741\n",
       "{'Output': 'match'}       250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# args = parse_args()\n",
    "# print(args)\n",
    "\n",
    "# pt_data = torch.load(args.pt_data_path, map_location=torch.device('cpu'))\n",
    "# with open(args.json_data_path, \"r\") as f:\n",
    "#     json_data = json.load(f)\n",
    "file_path = '/data/home/wangys/transfer-er/Pipeline/Amazon-Google/LLM_file/Amazon-Google-Train-Match-P1-wo-RAG.json'\n",
    "json_data = pd.read_json(file_path)\n",
    "\n",
    "# emb_list = []\n",
    "# ppl_list = []\n",
    "# for i in tqdm(range(len(json_data))):\n",
    "#     sent_emb_list = pt_data[i]\n",
    "#     # sent_emb_list = data_i['sent_emb']\n",
    "#     emb_list.append(sent_emb_list)\n",
    "#     ppl_list.append(ppl_list[i])\n",
    "high_dim_vectors = pt_data\n",
    "\n",
    "# high_dim_vectors = torch.cat(emb_list,0).numpy()\n",
    "ppl_array = np.array(ppl_list)\n",
    "\n",
    "clustering = do_clustering(high_dim_vectors,kmeans_num_clusters=100)\n",
    "cluster_labels = clustering.labels_\n",
    "\n",
    "def get_json_sample(middle_confidence_samples):\n",
    "    \n",
    "    json_samples = []\n",
    "    for k in middle_confidence_samples.keys():\n",
    "        ids_list = middle_confidence_samples[k].tolist()\n",
    "        # for id_i in ids_list:\n",
    "            # ori_sample = json_data[id_i]\n",
    "        json_samples.extend(ids_list)\n",
    "    \n",
    "\n",
    "    return json_samples\n",
    "\n",
    "# middle_confidence_samples = sample_middle_confidence_data(cluster_labels, ppl_array, n = 10, low_th=25, up_th = 75)\n",
    "middle_confidence_samples = sample_middle_confidence_data(cluster_labels, ppl_array, n = 10, low_th=25, up_th = 75)\n",
    "\n",
    "new_data = get_json_sample(middle_confidence_samples)\n",
    "json_data.iloc[new_data]['output'].value_counts()\n",
    "# print('New data len \\n',len(new_data))\n",
    "# with open(args.json_save_path, \"w\") as fw:\n",
    "#     json.dump(new_data, fw, indent=4)\n",
    "# pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(json_data.iloc[new_data].to_dict(orient='records'), open('train/AG-train-init.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FL score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_indices = {i: np.where(cluster_labels == i)[0] for i in range(100)}\n",
    "# cluster_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "# def cluster_vectors(high_dim_vectors_cluster, indexes, batch_size):\n",
    "#     \"\"\"\n",
    "#     根据给定的高维向量矩阵、索引以及期望的每个聚类元素个数进行聚类划分。\n",
    "\n",
    "#     参数:\n",
    "#     high_dim_vectors_cluster (numpy.ndarray): m * n的高维向量矩阵，m为元素个数，n为向量维度。\n",
    "#     indexes (numpy.ndarray): 对应高维向量矩阵中元素的索引列表，长度为k。\n",
    "#     batch_size (int): 期望每个聚类包含的元素个数。\n",
    "\n",
    "#     返回:\n",
    "#     dict: 聚类划分后的结果，键为聚类编号（从0到k - 1），值为对应聚类包含的元素索引列表。\n",
    "#     \"\"\"\n",
    "#     # 获取对应索引的向量\n",
    "#     selected_vectors = high_dim_vectors_cluster[indexes]\n",
    "\n",
    "#     # 计算这些向量之间的余弦相似度矩阵\n",
    "#     similarity_matrix = cosine_similarity(selected_vectors)\n",
    "\n",
    "#     # 使用层次聚类（AgglomerativeClustering）基于余弦相似度进行聚类划分\n",
    "#     k = len(indexes)\n",
    "#     clustering_model = AgglomerativeClustering(n_clusters=k, metric='cosine', linkage='average')\n",
    "#     clustering_model.fit(similarity_matrix)\n",
    "\n",
    "#     # 根据聚类结果分配所有的m个元素到对应的聚类中\n",
    "#     cluster_assignments = {i: [] for i in range(k)}\n",
    "#     for i in range(len(high_dim_vectors_cluster)):\n",
    "#         vector = high_dim_vectors_cluster[i].reshape(1, -1)\n",
    "#         similarities = cosine_similarity(vector, selected_vectors)[0]\n",
    "#         closest_cluster = np.argmax(similarities)\n",
    "#         cluster_assignments[closest_cluster].append(i)\n",
    "\n",
    "#     # 调整每个聚类中的元素个数尽量接近batch_size\n",
    "#     for cluster_id in range(k):\n",
    "#         cluster_indices = cluster_assignments[cluster_id]\n",
    "#         if len(cluster_indices) > batch_size:\n",
    "#             # 如果当前聚类元素个数大于batch_size，可考虑进一步划分等策略（这里简单取前batch_size个元素示例）\n",
    "#             cluster_assignments[cluster_id] = cluster_indices[:batch_size]\n",
    "#         elif len(cluster_indices) < batch_size:\n",
    "#             # 如果小于batch_size，可以从其他聚类补充等（这里暂不实现复杂补充逻辑，仅打印提示）\n",
    "#             print(f\"Cluster {cluster_id} has less than {batch_size} elements.\")\n",
    "\n",
    "#     return cluster_assignments\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def cluster_vectors(high_dim_vectors_cluster, indexes, batch_size):\n",
    "    \"\"\"\n",
    "    根据给定的高维向量矩阵、索引以及期望的每个聚类元素个数进行聚类划分。\n",
    "\n",
    "    参数:\n",
    "    high_dim_vectors_cluster (numpy.ndarray): m * n的高维向量矩阵，m为元素个数，n为向量维度。\n",
    "    indexes (numpy.ndarray): 对应高维向量矩阵中元素的索引列表，长度为k。\n",
    "    batch_size (int): 期望每个聚类包含的元素个数。\n",
    "\n",
    "    返回:\n",
    "    tuple: 包含两个元素，第一个元素是聚类划分后的结果（字典形式，键为聚类编号，值为对应聚类包含的元素索引列表），\n",
    "           第二个元素是覆盖率（float类型，表示已分配元素占总元素的比例）。\n",
    "    \"\"\"\n",
    "    # 获取对应索引的向量\n",
    "    selected_vectors = high_dim_vectors_cluster[indexes]\n",
    "\n",
    "    # 计算这些向量之间的余弦相似度矩阵\n",
    "    similarity_matrix = cosine_similarity(selected_vectors)\n",
    "\n",
    "    # 使用层次聚类（AgglomerativeClustering）基于余弦相似度进行聚类划分\n",
    "    k = len(indexes)\n",
    "    clustering_model = AgglomerativeClustering(n_clusters=k, metric='cosine', linkage='average')\n",
    "    clustering_model.fit(similarity_matrix)\n",
    "\n",
    "    # 根据聚类结果分配所有的m个元素到对应的聚类中\n",
    "    cluster_assignments = {i: [] for i in range(k)}\n",
    "    for i in range(len(high_dim_vectors_cluster)):\n",
    "        vector = high_dim_vectors_cluster[i].reshape(1, -1)\n",
    "        similarities = cosine_similarity(vector, selected_vectors)[0]\n",
    "        closest_cluster = np.argmax(similarities)\n",
    "        cluster_assignments[closest_cluster].append(i)\n",
    "\n",
    "    # 检查未分配的元素，并根据距离的就近原则分配到对应的聚类中\n",
    "    all_indices = set(range(len(high_dim_vectors_cluster)))\n",
    "    assigned_indices = set([index for sublist in cluster_assignments.values() for index in sublist])\n",
    "    unassigned_indices = all_indices - assigned_indices\n",
    "    for index in unassigned_indices:\n",
    "        vector = high_dim_vectors_cluster[index].reshape(1, -1)\n",
    "        distances = []\n",
    "        for cluster_id in range(k):\n",
    "            cluster_vectors = np.array([high_dim_vectors_cluster[i].reshape(1, -1) for i in cluster_assignments[cluster_id]])\n",
    "            mean_cluster_vector = np.mean(cluster_vectors, axis=0)\n",
    "            distance = cosine_similarity(vector, mean_cluster_vector)[0][0]\n",
    "            distances.append(distance)\n",
    "        closest_cluster = np.argmin(distances)\n",
    "        cluster_assignments[closest_cluster].append(index)\n",
    "\n",
    "    # 调整每个聚类中的元素个数尽量接近batch_size（这里简单处理，可根据实际优化）\n",
    "    for cluster_id in range(k):\n",
    "        cluster_indices = cluster_assignments[cluster_id]\n",
    "        if len(cluster_indices) > batch_size:\n",
    "            cluster_assignments[cluster_id] = cluster_indices[:batch_size]\n",
    "        elif len(cluster_indices) < batch_size:\n",
    "            while len(cluster_indices) < batch_size and unassigned_indices:\n",
    "                # 从未分配元素中找距离当前聚类最近的补充进来\n",
    "                index_to_add = None\n",
    "                min_distance = float('inf')\n",
    "                for unassigned_index in unassigned_indices:\n",
    "                    unassigned_vector = high_dim_vectors_cluster[unassigned_index].reshape(1, -1)\n",
    "                    mean_cluster_vector = np.mean([high_dim_vectors_cluster[i].reshape(1, -1) for i in cluster_indices], axis=0)\n",
    "                    distance = cosine_similarity(unassigned_vector, mean_cluster_vector)[0][0]\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        index_to_add = unassigned_index\n",
    "                if index_to_add is not None:\n",
    "                    cluster_assignments[cluster_id].append(index_to_add)\n",
    "                    unassigned_indices.remove(index_to_add)\n",
    "                    cluster_indices = cluster_assignments[cluster_id]\n",
    "\n",
    "    # 计算覆盖率\n",
    "    coverage = len(assigned_indices) / len(high_dim_vectors_cluster)\n",
    "\n",
    "    return cluster_assignments, coverage\n",
    "def cosine_similarity_clustering(high_dim_vectors_cluster, indices, k, batch_size):\n",
    "    \"\"\"\n",
    "    Perform k clustering based on cosine similarity, ensuring each cluster has batch_size elements.\n",
    "\n",
    "    Parameters:\n",
    "    - high_dim_vectors_cluster (np.ndarray): An m x n matrix containing m high-dimensional vectors.\n",
    "    - indices (list): A list of k indices corresponding to m rows in the matrix.\n",
    "    - k (int): Number of clusters.\n",
    "    - batch_size (int): Desired number of elements per cluster.\n",
    "\n",
    "    Returns:\n",
    "    - clusters (list of lists): A list where each sublist contains the indices of the elements in a cluster.\n",
    "    \"\"\"\n",
    "    m, n = high_dim_vectors_cluster.shape\n",
    "    if len(indices) != k:\n",
    "        raise ValueError(\"Number of provided indices must match the number of clusters (k).\")\n",
    "\n",
    "    # Step 1: Initialize cluster centers using the given indices\n",
    "    cluster_centers = high_dim_vectors_cluster[indices]\n",
    "\n",
    "    # Step 2: Compute cosine similarity between all elements and cluster centers\n",
    "    similarity_matrix = cosine_similarity(high_dim_vectors_cluster, cluster_centers)\n",
    "\n",
    "    # Step 3: Assign elements to clusters greedily (allow duplicates if needed)\n",
    "    clusters = [[] for _ in range(k)]\n",
    "\n",
    "    for _ in range(batch_size):  # Ensure each cluster has batch_size elements\n",
    "        for cluster_idx in range(k):\n",
    "            # Find the most similar element for the current cluster\n",
    "            best_idx = -1\n",
    "            best_similarity = -1\n",
    "            for i in range(m):\n",
    "                if similarity_matrix[i, cluster_idx] > best_similarity:\n",
    "                    best_idx = i\n",
    "                    best_similarity = similarity_matrix[i, cluster_idx]\n",
    "\n",
    "            if best_idx != -1:\n",
    "                clusters[cluster_idx].append(best_idx)\n",
    "\n",
    "    # Step 4: Handle remaining elements by assigning to closest clusters\n",
    "    remaining_elements = [i for i in range(m)]\n",
    "    for i in remaining_elements:\n",
    "        # Assign to the cluster with the highest similarity\n",
    "        best_cluster = np.argmax(similarity_matrix[i])\n",
    "        clusters[best_cluster].append(i)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fla(X, number_all, number_select):\n",
    "    start_time = time.time()\n",
    "\n",
    "    Y = X\n",
    "    obj = FacilityLocationFunction(n=number_all, mode=\"dense\", data=Y, metric=\"cosine\")\n",
    "    greedyList = obj.maximize(budget=number_select, optimizer='LazyGreedy', stopIfZeroGain=False, stopIfNegativeGain=False, verbose=False)\n",
    "    idx_list = [tuple_i[0] for tuple_i in greedyList]\n",
    "\n",
    "    print('FLA time used:',(time.time()-start_time),'(second)')\n",
    "    return idx_list,greedyList\n",
    "\n",
    "# idx_list,greedyList = do_fla(high_dim_vectors,high_dim_vectors.shape[0],number_select=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating FLA score per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.03293466567993164 (second)\n",
      "FLA time used: 0.06013751029968262 (second)\n",
      "FLA time used: 0.04508638381958008 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 13 of 13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.025324106216430664 (second)\n",
      "FLA time used: 0.02434849739074707 (second)\n",
      "FLA time used: 0.006682872772216797 (second)\n",
      "FLA time used: 0.011463642120361328 (second)\n",
      "FLA time used: 0.0018830299377441406 (second)\n",
      "FLA time used: 0.010698556900024414 (second)\n",
      "FLA time used: 0.004029273986816406 (second)\n",
      "FLA time used: 0.015811920166015625 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 7 of 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.003081798553466797 (second)\n",
      "FLA time used: 0.007641315460205078 (second)\n",
      "FLA time used: 0.001377105712890625 (second)\n",
      "FLA time used: 0.0057103633880615234 (second)\n",
      "FLA time used: 0.016421794891357422 (second)\n",
      "FLA time used: 0.004853963851928711 (second)\n",
      "FLA time used: 0.003812074661254883 (second)\n",
      "FLA time used: 0.005659580230712891 (second)\n",
      "FLA time used: 0.011415719985961914 (second)\n",
      "FLA time used: 0.03092813491821289 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.002870321273803711 (second)\n",
      "FLA time used: 0.014646768569946289 (second)\n",
      "FLA time used: 0.0067365169525146484 (second)\n",
      "FLA time used: 0.00362396240234375 (second)\n",
      "FLA time used: 0.00952768325805664 (second)\n",
      "FLA time used: 0.0013699531555175781 (second)\n",
      "FLA time used: 0.007970094680786133 (second)\n",
      "FLA time used: 0.0035521984100341797 (second)\n",
      "FLA time used: 0.0042722225189208984 (second)\n",
      "FLA time used: 0.011423826217651367 (second)\n",
      "FLA time used: 0.018591642379760742 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 8 of 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.06824469566345215 (second)\n",
      "FLA time used: 0.03386688232421875 (second)\n",
      "FLA time used: 0.0058138370513916016 (second)\n",
      "FLA time used: 0.01276707649230957 (second)\n",
      "FLA time used: 0.005139827728271484 (second)\n",
      "FLA time used: 0.0054569244384765625 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 4 of 4]1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.003573894500732422 (second)\n",
      "FLA time used: 0.0074176788330078125 (second)\n",
      "FLA time used: 0.010229110717773438 (second)\n",
      "FLA time used: 0.005696773529052734 (second)\n",
      "FLA time used: 0.010645627975463867 (second)\n",
      "FLA time used: 0.0014615058898925781 (second)\n",
      "FLA time used: 0.006741046905517578 (second)\n",
      "FLA time used: 0.007024049758911133 (second)\n",
      "FLA time used: 0.020309925079345703 (second)\n",
      "FLA time used: 0.011874914169311523 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 6 of 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.005729198455810547 (second)\n",
      "FLA time used: 0.01073908805847168 (second)\n",
      "FLA time used: 0.01968097686767578 (second)\n",
      "FLA time used: 0.003448963165283203 (second)\n",
      "FLA time used: 0.006515026092529297 (second)\n",
      "FLA time used: 0.010833501815795898 (second)\n",
      "FLA time used: 0.0013551712036132812 (second)\n",
      "FLA time used: 0.01481175422668457 (second)\n",
      "FLA time used: 0.014276981353759766 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 7 of 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.006352424621582031 (second)\n",
      "FLA time used: 0.0057904720306396484 (second)\n",
      "FLA time used: 0.0013895034790039062 (second)\n",
      "FLA time used: 0.0013952255249023438 (second)\n",
      "FLA time used: 0.004823446273803711 (second)\n",
      "FLA time used: 0.0014522075653076172 (second)\n",
      "FLA time used: 0.004313468933105469 (second)\n",
      "FLA time used: 0.007636547088623047 (second)\n",
      "FLA time used: 0.0020546913146972656 (second)\n",
      "FLA time used: 0.005656242370605469 (second)\n",
      "FLA time used: 0.0013895034790039062 (second)\n",
      "FLA time used: 0.00905609130859375 (second)\n",
      "FLA time used: 0.0013880729675292969 (second)\n",
      "FLA time used: 0.0035202503204345703 (second)\n",
      "FLA time used: 0.027387619018554688 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.005674600601196289 (second)\n",
      "FLA time used: 0.0056531429290771484 (second)\n",
      "FLA time used: 0.003545999526977539 (second)\n",
      "FLA time used: 0.0035483837127685547 (second)\n",
      "FLA time used: 0.003294229507446289 (second)\n",
      "FLA time used: 0.0013337135314941406 (second)\n",
      "FLA time used: 0.0040149688720703125 (second)\n",
      "FLA time used: 0.007798671722412109 (second)\n",
      "FLA time used: 0.0013804435729980469 (second)\n",
      "FLA time used: 0.0073888301849365234 (second)\n",
      "FLA time used: 0.00354766845703125 (second)\n",
      "FLA time used: 0.0013811588287353516 (second)\n",
      "FLA time used: 0.012080192565917969 (second)\n",
      "FLA time used: 0.013245105743408203 (second)\n",
      "FLA time used: 0.014189958572387695 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 7 of 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.0018758773803710938 (second)\n",
      "FLA time used: 0.0035369396209716797 (second)\n",
      "FLA time used: 0.0042836666107177734 (second)\n",
      "FLA time used: 0.005993843078613281 (second)\n",
      "FLA time used: 0.007248640060424805 (second)\n",
      "FLA time used: 0.004332065582275391 (second)\n",
      "FLA time used: 0.0024979114532470703 (second)\n",
      "FLA time used: 0.003124237060546875 (second)\n",
      "FLA time used: 0.033794403076171875 (second)\n",
      "FLA time used: 0.007601499557495117 (second)\n",
      "FLA time used: 0.013195991516113281 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 7 of 7]1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.009653806686401367 (second)\n",
      "FLA time used: 0.025946855545043945 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 10 of 10]"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "batch_division = {}\n",
    "for i in range(100):\n",
    "    batch_division[i] = []\n",
    "    cluster_indice_index = cluster_indices[i]\n",
    "    high_dim_vectors_cluster = high_dim_vectors[cluster_indice_index]\n",
    "    cluster_size = len(cluster_indice_index)\n",
    "    # print(i,cluster_size)\n",
    "    fla_num = int(np.ceil(cluster_size / batch_size))\n",
    "    idx_list,greedyList = do_fla(high_dim_vectors_cluster,high_dim_vectors_cluster.shape[0],number_select=fla_num) ## idx_list is the selected number\n",
    "    result,coverage = cluster_vectors(high_dim_vectors_cluster,idx_list,batch_size)\n",
    "    for cluster_ind in result.keys():\n",
    "        global_result = [cluster_indice_index[j] for j in result[cluster_ind]] ## 将相对index映射到global index\n",
    "        batch_division[i].append(global_result)\n",
    "    # print(result,coverage)\n",
    "    # result = cosine_similarity_clustering(high_dim_vectors_cluster,idx_list,k = fla_num, batch_size=batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = []\n",
    "for key in batch_division.keys():\n",
    "    batch_sampler.extend(batch_division[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(batch_sampler,'ppl/AG-batch.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取phase 2 的Cluster和Grad，计算IF分数，这里考虑使用一个python独立程序来计算\n",
    "## 计算IF Score需要划分eval_dataset，这里需要修改cal_IF_self_divide.py来计算？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
