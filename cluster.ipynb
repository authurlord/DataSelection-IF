{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import  KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from FlagEmbedding import FlagModel\n",
    "import time\n",
    "import submodlib\n",
    "from submodlib.functions.facilityLocation import FacilityLocationFunction\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--pt_data_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--json_data_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--json_save_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--sent_type\", type=int, default=0)\n",
    "    parser.add_argument(\"--ppl_type\", type=int, default=0)\n",
    "    parser.add_argument(\"--cluster_method\", type=str, default='kmeans')\n",
    "    parser.add_argument(\"--reduce_method\", type=str, default='tsne')\n",
    "    parser.add_argument(\"--sample_num\", type=int, default=10)\n",
    "    parser.add_argument(\"--kmeans_num_clusters\", type=int, default=100)\n",
    "    parser.add_argument(\"--low_th\", type=int, default=1)\n",
    "    parser.add_argument(\"--up_th\", type=int, default=99)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def do_clustering(high_dim_vectors,cluster_method='kmeans',kmeans_num_clusters=100):\n",
    "\n",
    "    clustering_algorithm = cluster_method\n",
    "    if clustering_algorithm == 'kmeans':\n",
    "        clustering = KMeans(n_clusters=kmeans_num_clusters, random_state=0).fit(high_dim_vectors)\n",
    "    \n",
    "    return clustering\n",
    "\n",
    "def do_reduce_dim(high_dim_vectors):\n",
    "    # Perform t-SNE for visualization\n",
    "    # if args.reduce_method == 'tsne':\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    low_dim_vectors = tsne.fit_transform(high_dim_vectors)\n",
    "    return low_dim_vectors\n",
    "\n",
    "def sample_middle_confidence_data(cluster_labels, confidences, n, low_th=25, up_th=75):\n",
    "    num_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "    # Get the indices for each cluster\n",
    "    cluster_indices = {i: np.where(cluster_labels == i)[0] for i in range(num_clusters)}\n",
    "    \n",
    "    # Create a dictionary to store the indices of the middle level confidence samples\n",
    "    middle_confidence_samples = {}\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        # Get the sorted indices for this cluster\n",
    "        sorted_indices = cluster_indices[i]\n",
    "        \n",
    "        # If there are less than n samples in this class, just return all of them\n",
    "        if len(sorted_indices) < n:\n",
    "            middle_confidence_samples[i] = sorted_indices\n",
    "            continue\n",
    "\n",
    "        # Get the confidences for this cluster\n",
    "        cluster_confidences = confidences[sorted_indices]\n",
    "        lower_threshold = np.percentile(cluster_confidences, low_th)\n",
    "        upper_threshold = np.percentile(cluster_confidences, up_th)\n",
    "\n",
    "        # Get the indices of the samples within the middle level confidence range\n",
    "        middle_indices = sorted_indices[(cluster_confidences >= lower_threshold) & (cluster_confidences <= upper_threshold)]\n",
    "        \n",
    "        # If there are less than n samples in the middle range, use all of them\n",
    "        if len(middle_indices) < n:\n",
    "            middle_confidence_samples[i] = middle_indices\n",
    "        else:\n",
    "            # Calculate step size for even sampling\n",
    "            step_size = len(middle_indices) // n\n",
    "            # Select evenly from the middle level confidence samples\n",
    "            middle_confidence_samples[i] = middle_indices[::step_size][:n]\n",
    "\n",
    "    return middle_confidence_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings by SentBert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse LLM File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data = pd.read_json('/data/home/wangys/transfer-er/Pipeline/Amazon-Google/LLM_file/Amazon-Google-Train-Match-P1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_input_length(row):\n",
    "    input = row['instruction']\n",
    "    output = input.split('at the final judgement.')[1].split('Take these examples as reference:')[0]\n",
    "    return output\n",
    "All_Data['instruction'] = All_Data.apply(cut_input_length,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2106371/2444698609.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = row[0]\n",
      "/tmp/ipykernel_2106371/2444698609.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = eval(row[-1])['Output']\n"
     ]
    }
   ],
   "source": [
    "# text = All_Data.iloc[0,0]\n",
    "left_list = []\n",
    "right_list = []\n",
    "label_list = []\n",
    "for index,row in All_Data.iterrows():\n",
    "    text = row[0]\n",
    "    Entity_1 = text.split('\\n\\nEntity 1:')[1].split('\\n\\nEntity 2')[0]\n",
    "    Entity_2 = text.split('\\n\\nEntity 2:')[1].split('\\n\\nTake these examples as reference:')[0] \n",
    "    label = eval(row[-1])['Output']\n",
    "    left_list.append(Entity_1)\n",
    "    right_list.append(Entity_2)\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 15/15 [00:02<00:00,  6.33it/s]\n",
      "Inference Embeddings: 100%|██████████| 15/15 [00:02<00:00,  7.11it/s]\n",
      "Inference Embeddings: 100%|██████████| 15/15 [00:00<00:00, 38.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'\n",
    "model = FlagModel('../sentence_transformer_model/bge-large-en-1.5/', \n",
    "                  use_fp16=True)\n",
    "embedding_a = model.encode(left_list)\n",
    "embedding_b = model.encode(right_list)\n",
    "embedding_c = model.encode(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppl = pd.read_json('/data/home/wangys/DataSelection-IF/ppl/ppl_qwen2.5-7B-AG-Short.json')\n",
    "ppl = pd.read_csv('ppl/ppl_qwen2.5-0.5B-AG-short.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>4.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>4.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>6.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>4.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>4.468750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3663 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0     3.828125\n",
       "1     5.406250\n",
       "2     5.375000\n",
       "3     4.781250\n",
       "4     4.281250\n",
       "...        ...\n",
       "3658  4.281250\n",
       "3659  4.562500\n",
       "3660  6.031250\n",
       "3661  4.437500\n",
       "3662  4.468750\n",
       "\n",
       "[3663 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3663, 3072)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ppl_list = ppl.iloc[:,0].to_list()\n",
    "pt_data = np.concatenate([embedding_a,embedding_b,embedding_c],axis=1)\n",
    "pt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "{'Output': 'mismatch'}    741\n",
       "{'Output': 'match'}       250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# args = parse_args()\n",
    "# print(args)\n",
    "\n",
    "# pt_data = torch.load(args.pt_data_path, map_location=torch.device('cpu'))\n",
    "# with open(args.json_data_path, \"r\") as f:\n",
    "#     json_data = json.load(f)\n",
    "file_path = '/data/home/wangys/transfer-er/Pipeline/Amazon-Google/LLM_file/Amazon-Google-Train-Match-P1-wo-RAG.json'\n",
    "json_data = pd.read_json(file_path)\n",
    "\n",
    "# emb_list = []\n",
    "# ppl_list = []\n",
    "# for i in tqdm(range(len(json_data))):\n",
    "#     sent_emb_list = pt_data[i]\n",
    "#     # sent_emb_list = data_i['sent_emb']\n",
    "#     emb_list.append(sent_emb_list)\n",
    "#     ppl_list.append(ppl_list[i])\n",
    "high_dim_vectors = pt_data\n",
    "\n",
    "# high_dim_vectors = torch.cat(emb_list,0).numpy()\n",
    "ppl_array = np.array(ppl_list)\n",
    "\n",
    "clustering = do_clustering(high_dim_vectors,kmeans_num_clusters=100)\n",
    "cluster_labels = clustering.labels_\n",
    "\n",
    "def get_json_sample(middle_confidence_samples):\n",
    "    \n",
    "    json_samples = []\n",
    "    for k in middle_confidence_samples.keys():\n",
    "        ids_list = middle_confidence_samples[k].tolist()\n",
    "        # for id_i in ids_list:\n",
    "            # ori_sample = json_data[id_i]\n",
    "        json_samples.extend(ids_list)\n",
    "    \n",
    "\n",
    "    return json_samples\n",
    "\n",
    "# middle_confidence_samples = sample_middle_confidence_data(cluster_labels, ppl_array, n = 10, low_th=25, up_th = 75)\n",
    "middle_confidence_samples = sample_middle_confidence_data(cluster_labels, ppl_array, n = 10, low_th=25, up_th = 75)\n",
    "\n",
    "new_data = get_json_sample(middle_confidence_samples)\n",
    "json_data.iloc[new_data]['output'].value_counts()\n",
    "# print('New data len \\n',len(new_data))\n",
    "# with open(args.json_save_path, \"w\") as fw:\n",
    "#     json.dump(new_data, fw, indent=4)\n",
    "# pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>You are an expert in detecting if two text des...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'mismatch'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction input  \\\n",
       "299   You are an expert in detecting if two text des...         \n",
       "540   You are an expert in detecting if two text des...         \n",
       "1010  You are an expert in detecting if two text des...         \n",
       "1283  You are an expert in detecting if two text des...         \n",
       "1707  You are an expert in detecting if two text des...         \n",
       "...                                                 ...   ...   \n",
       "2238  You are an expert in detecting if two text des...         \n",
       "2538  You are an expert in detecting if two text des...         \n",
       "2665  You are an expert in detecting if two text des...         \n",
       "3258  You are an expert in detecting if two text des...         \n",
       "3370  You are an expert in detecting if two text des...         \n",
       "\n",
       "                      output  \n",
       "299   {'Output': 'mismatch'}  \n",
       "540   {'Output': 'mismatch'}  \n",
       "1010  {'Output': 'mismatch'}  \n",
       "1283  {'Output': 'mismatch'}  \n",
       "1707  {'Output': 'mismatch'}  \n",
       "...                      ...  \n",
       "2238  {'Output': 'mismatch'}  \n",
       "2538  {'Output': 'mismatch'}  \n",
       "2665  {'Output': 'mismatch'}  \n",
       "3258  {'Output': 'mismatch'}  \n",
       "3370  {'Output': 'mismatch'}  \n",
       "\n",
       "[991 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.iloc[new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(json_data.iloc[new_data].to_dict(orient='records'), open('train/AG-train-init.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 52.16716647148132 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 1000 of 1000]"
     ]
    }
   ],
   "source": [
    "def do_fla(X, number_all, number_select):\n",
    "    start_time = time.time()\n",
    "\n",
    "    Y = X\n",
    "    obj = FacilityLocationFunction(n=number_all, mode=\"dense\", data=Y, metric=\"cosine\")\n",
    "    greedyList = obj.maximize(budget=number_select, optimizer='LazyGreedy', stopIfZeroGain=False, stopIfNegativeGain=False, verbose=False)\n",
    "    idx_list = [tuple_i[0] for tuple_i in greedyList]\n",
    "\n",
    "    print('FLA time used:',(time.time()-start_time),'(second)')\n",
    "    return idx_list,greedyList\n",
    "\n",
    "idx_list,greedyList = do_fla(high_dim_vectors,high_dim_vectors.shape[0],number_select=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_indices = {i: np.where(cluster_labels == i)[0] for i in range(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLA time used: 0.13275599479675293 (second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[||||||||||||||||||||]100% [Iteration 32 of 32] ]71% [Iteration 23 of 32]"
     ]
    }
   ],
   "source": [
    "# cluster_indices[0]\n",
    "idx_list,greedyList = do_fla(high_dim_vectors[cluster_indices[1]],high_dim_vectors[cluster_indices[1]].shape[0],number_select=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_dim_vectors[cluster_indices[1]].shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_score = pd.read_json('ppl/ppl_qwen2.5-7B-AG-Short.json').iloc[:,0].to_list()\n",
    "full_score = pd.read_json('ppl/ppl_qwen2.5-7B-AG-Full.json').iloc[:,0].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF = np.array(full_score) / np.array(condition_score)\n",
    "np.sum(IDF < 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
