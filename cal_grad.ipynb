{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Literal, Optional, Sequence\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from llamafactory.data import MultiModalDataCollatorForSeq2Seq, get_dataset, get_dataset_id, get_template_and_fix_tokenizer,SFTDataCollatorWith4DAttentionMask\n",
    "from llamafactory.extras.constants import IGNORE_INDEX\n",
    "from llamafactory.hparams import get_train_args\n",
    "from llamafactory.model import load_model, load_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING|2024-12-21 18:00:10] llamafactory.hparams.parser:162 >> `neat_packing` requires `packing` is True. Change `packing` to True.\n",
      "[WARNING|2024-12-21 18:00:10] llamafactory.hparams.parser:162 >> We recommend enable mixed precision training.\n",
      "[INFO|2024-12-21 18:00:10] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 8, distributed training: False, compute dtype: None\n"
     ]
    }
   ],
   "source": [
    "model_args, data_args, training_args, finetuning_args, _ = get_train_args(\n",
    "    dict(\n",
    "        stage='sft',\n",
    "        model_name_or_path='/data/home/wangys/model/Qwen2.5-0.5B-Instruct',\n",
    "        adapter_name_or_path = '../LLAMA-backup/LLaMA-Factory/saves/qwen-0.5B/Abt-Buy-Match-P1-short-qwen/',\n",
    "        dataset='Abt-Buy-Match-P1-short',\n",
    "        dataset_dir='data',\n",
    "        template='qwen',\n",
    "        cutoff_len=400,\n",
    "        max_samples=None,\n",
    "        train_on_prompt=False,\n",
    "        output_dir=\"output\",\n",
    "        overwrite_cache=False,\n",
    "        do_train=True,\n",
    "        # quantization_bit=8\n",
    "        neat_packing = True,\n",
    "        enable_liger_kernel = True,\n",
    "        preprocessing_num_workers=8\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args, data_args, training_args, finetuning_args, _ = get_train_args(\n",
    "    dict(\n",
    "        stage='sft',\n",
    "        model_name_or_path='/data/home/wangys/model/Mistral-7B-Instruct-v0.2/',\n",
    "        adapter_name_or_path = '../LLAMA-backup/LLaMA-Factory/saves/qwen-0.5B/Abt-Buy-Match-P1-short-qwen/',\n",
    "        dataset='Abt-Buy-Match-P1-short',\n",
    "        dataset_dir='data',\n",
    "        template='mistral',\n",
    "        cutoff_len=400,\n",
    "        max_samples=None,\n",
    "        train_on_prompt=False,\n",
    "        output_dir=\"output\",\n",
    "        overwrite_cache=False,\n",
    "        do_train=True,\n",
    "        # quantization_bit=8\n",
    "        use_unsloth = False,\n",
    "        enable_liger_kernel = True,\n",
    "        preprocessing_num_workers = 1,\n",
    "        neat_packing=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:677] 2024-12-21 18:00:15,661 >> loading configuration file /data/home/wangys/model/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-21 18:00:15,666 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/data/home/wangys/model/Qwen2.5-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,694 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,695 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,696 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,696 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,697 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,698 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-12-21 18:00:15,987 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:677] 2024-12-21 18:00:15,989 >> loading configuration file /data/home/wangys/model/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-21 18:00:15,990 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/data/home/wangys/model/Qwen2.5-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,991 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,992 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,993 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,994 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,994 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-12-21 18:00:15,995 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2024-12-21 18:00:16,238 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-21 18:00:16] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
      "[INFO|2024-12-21 18:00:16] llamafactory.data.loader:157 >> Loading dataset /data/home/wangys/transfer-er/Pipeline/Abt-Buy/LLM_file/Abt-Buy-Train-Match-P1-short.json...\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 4710, 60256, 3425, 10390, 220, 16, 323, 10390, 220, 17, 525, 2432, 476, 35301, 11, 323, 5157, 2878, 279, 2661, 14566, 382, 5598, 304, 4718, 3561, 382, 3798, 25, 508, 6347, 13086, 24976, 2533, 5097, 3561, 3110, 25, 4913, 5097, 788, 1591, 630, 3030, 220, 16, 2974, 13608, 606, 1210, 364, 94333, 1032, 13891, 480, 33612, 13795, 29067, 12, 24, 16, 20, 516, 364, 4684, 1210, 364, 693, 13891, 480, 11602, 3769, 369, 38116, 7420, 36402, 54768, 22, 18, 15, 33414, 11, 17258, 10842, 220, 16, 23, 15, 11, 17258, 10842, 220, 18, 21, 15, 33414, 11, 17258, 10842, 220, 16, 22, 15, 3424, 11, 17258, 10842, 220, 16, 21, 15, 3424, 11, 17258, 10842, 220, 16, 20, 15, 3424, 11, 17258, 10842, 220, 16, 19, 15, 3424, 11, 17258, 10842, 220, 16, 18, 15, 3424, 11, 17258, 10842, 220, 16, 17, 15, 3424, 11, 17258, 10842, 220, 16, 16, 15, 33414, 11, 17258, 10842, 220, 16, 15, 15, 33414, 11, 17258, 10842, 220, 24, 15, 3424, 11, 17258, 10842, 220, 23, 22, 15, 3424, 11, 17258, 10842, 220, 23, 21, 15, 3424, 11, 17258, 10842, 220, 23, 20, 15, 3424, 11, 17258, 10842, 220, 23, 19, 15, 3424, 11, 17258, 10842, 220, 23, 18, 15, 33414, 11, 17258, 10842, 220, 23, 17, 15, 3424, 11, 17258, 10842, 220, 23, 16, 15, 3424, 11, 17258, 10842, 220, 22, 17, 15, 3424, 11, 17258, 10842, 220, 22, 16, 15, 33414, 11, 17258, 10842, 220, 22, 15, 15, 33414, 11, 8030, 19, 20, 15, 15, 3424, 11, 8030, 19, 18, 15, 15, 3424, 11, 8030, 19, 15, 15, 15, 3424, 11, 8030, 18, 20, 15, 15, 3424, 11, 8030, 18, 15, 15, 15, 3424, 11, 8030, 17, 20, 15, 15, 3424, 11, 8030, 17, 15, 15, 15, 3424, 11, 8030, 16, 24, 15, 15, 3424, 11, 8030, 16, 23, 15, 15, 3424, 11, 8030, 16, 22, 15, 15, 3424, 11, 8030, 16, 19, 15, 15, 3424, 11, 8030, 16, 18, 15, 15, 3424, 11, 8030, 16, 17, 15, 15, 3424, 11, 8030, 16, 16, 15, 15, 3424, 11, 8030, 16, 15, 15, 15, 3424, 11, 8030, 24, 20, 15, 3424, 11, 8030, 24, 15, 15, 3424, 11, 8030, 23, 24, 15, 3424, 11, 8030, 23, 23, 15, 3424, 11, 8030, 13608, 5097, 1210, 364, 6347, 8275, 151645, 151643]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      " \n",
      "\n",
      "Judge whether Entity 1 and Entity 2 are match or mismatch, and choose within the given Options.\n",
      "\n",
      "Return in JSON format.\n",
      "\n",
      "Options: [match,mismatch]\n",
      "\n",
      "Output format example:{\"Output\": \"\"}\n",
      "\n",
      "Entity 1:\"{'name': 'Canon Rechargeable Battery Pack BP-915', 'description': 'Rechargeable battery pack for Canon PowerShot SX730 HS, ELPH 180, ELPH 360 HS, ELPH 170 IS, ELPH 160 IS, ELPH 150 IS, ELPH 140 IS, ELPH 130 IS, ELPH 120 IS, ELPH 110 HS, ELPH 100 HS, ELPH 90 IS, ELPH 870 IS, ELPH 860 IS, ELPH 850 IS, ELPH 840 IS, ELPH 830 HS, ELPH 820 IS, ELPH 810 IS, ELPH 720 IS, ELPH 710 HS, ELPH 700 HS, SD4500 IS, SD4300 IS, SD4000 IS, SD3500 IS, SD3000 IS, SD2500 IS, SD2000 IS, SD1900 IS, SD1800 IS, SD1700 IS, SD1400 IS, SD1300 IS, SD1200 IS, SD1100 IS, SD1000 IS, SD950 IS, SD900 IS, SD890 IS, SD880 IS, SD{'Output': 'match'}<|im_end|><|endoftext|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13608, 5097, 1210, 364, 6347, 8275, 151645, -100]\n",
      "labels:\n",
      "{'Output': 'match'}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "stage = 'sft'\n",
    "\n",
    "tokenizer_module = load_tokenizer(model_args)\n",
    "tokenizer = tokenizer_module[\"tokenizer\"]\n",
    "template = get_template_and_fix_tokenizer(tokenizer, data_args)\n",
    "trainset = get_dataset_id(template, model_args, data_args, training_args, stage, **tokenizer_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = trainset['train_dataset']\n",
    "data_collator = SFTDataCollatorWith4DAttentionMask(\n",
    "            template=template, tokenizer=tokenizer, label_pad_token_id=IGNORE_INDEX\n",
    "        )\n",
    "dataloader = DataLoader(trainset, batch_size=8, shuffle=False, collate_fn=data_collator, pin_memory=True)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "total_ppl = 0\n",
    "perplexities = []\n",
    "batch: Dict[str, \"torch.Tensor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainset,'trainset_ER_400.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:677] 2024-12-21 18:05:10,637 >> loading configuration file /data/home/wangys/model/Qwen2.5-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-12-21 18:05:10,641 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/data/home/wangys/model/Qwen2.5-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-21 18:05:10] llamafactory.model.model_utils.packing:157 >> Using block diagonal attention for sequence packing without cross-attention.\n",
      "Applied Liger kernels to Qwen2\n",
      "[INFO|2024-12-21 18:05:10] llamafactory.model.model_utils.liger_kernel:157 >> Liger kernel has been applied to the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3934] 2024-12-21 18:05:10,777 >> loading weights file /data/home/wangys/model/Qwen2.5-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1670] 2024-12-21 18:05:10,787 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2024-12-21 18:05:10,790 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4800] 2024-12-21 18:05:11,376 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-12-21 18:05:11,377 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data/home/wangys/model/Qwen2.5-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1049] 2024-12-21 18:05:11,380 >> loading configuration file /data/home/wangys/model/Qwen2.5-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-12-21 18:05:11,381 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2024-12-21 18:05:11] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2024-12-21 18:05:11] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2024-12-21 18:05:11] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2024-12-21 18:05:11] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2024-12-21 18:05:11] llamafactory.model.adapter:157 >> Loaded adapter(s): ../LLAMA-backup/LLaMA-Factory/saves/qwen-0.5B/Abt-Buy-Match-P1-short-qwen/\n",
      "[INFO|2024-12-21 18:05:11] llamafactory.model.loader:157 >> trainable params: 688,128 || all params: 494,720,896 || trainable%: 0.1391\n"
     ]
    }
   ],
   "source": [
    "model = load_model(tokenizer, model_args, finetuning_args, is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/307 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/307 [00:00<00:45,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "tensor([ 8,  9, 10, 11, 12, 13, 14, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/307 [00:00<00:35,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16, 17, 18, 19, 20, 21, 22, 23])\n",
      "tensor([24, 25, 26, 27, 28, 29, 30, 31])\n",
      "tensor([32, 33, 34, 35, 36, 37, 38, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/307 [00:00<00:30,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([40, 41, 42, 43, 44, 45, 46, 47])\n",
      "tensor([48, 49, 50, 51, 52, 53, 54, 55])\n",
      "tensor([56, 57, 58, 59, 60, 61, 62, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 9/307 [00:00<00:29, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([64, 65, 66, 67, 68, 69, 70, 71])\n",
      "tensor([72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 13/307 [00:01<00:28, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([88, 89, 90, 91, 92, 93, 94, 95])\n",
      "tensor([ 96,  97,  98,  99, 100, 101, 102, 103])\n",
      "tensor([104, 105, 106, 107, 108, 109, 110, 111])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 15/307 [00:01<00:27, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([112, 113, 114, 115, 116, 117, 118, 119])\n",
      "tensor([120, 121, 122, 123, 124, 125, 126, 127])\n",
      "tensor([128, 129, 130, 131, 132, 133, 134, 135])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 19/307 [00:01<00:26, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([136, 137, 138, 139, 140, 141, 142, 143])\n",
      "tensor([144, 145, 146, 147, 148, 149, 150, 151])\n",
      "tensor([152, 153, 154, 155, 156, 157, 158, 159])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 21/307 [00:02<00:26, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([160, 161, 162, 163, 164, 165, 166, 167])\n",
      "tensor([168, 169, 170, 171, 172, 173, 174, 175])\n",
      "tensor([176, 177, 178, 179, 180, 181, 182, 183])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 25/307 [00:02<00:26, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([184, 185, 186, 187, 188, 189, 190, 191])\n",
      "tensor([192, 193, 194, 195, 196, 197, 198, 199])\n",
      "tensor([200, 201, 202, 203, 204, 205, 206, 207])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 27/307 [00:02<00:26, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([208, 209, 210, 211, 212, 213, 214, 215])\n",
      "tensor([216, 217, 218, 219, 220, 221, 222, 223])\n",
      "tensor([224, 225, 226, 227, 228, 229, 230, 231])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 31/307 [00:03<00:25, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([232, 233, 234, 235, 236, 237, 238, 239])\n",
      "tensor([240, 241, 242, 243, 244, 245, 246, 247])\n",
      "tensor([248, 249, 250, 251, 252, 253, 254, 255])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 33/307 [00:03<00:25, 10.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([256, 257, 258, 259, 260, 261, 262, 263])\n",
      "tensor([264, 265, 266, 267, 268, 269, 270, 271])\n",
      "tensor([272, 273, 274, 275, 276, 277, 278, 279])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 37/307 [00:03<00:25, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([280, 281, 282, 283, 284, 285, 286, 287])\n",
      "tensor([288, 289, 290, 291, 292, 293, 294, 295])\n",
      "tensor([296, 297, 298, 299, 300, 301, 302, 303])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 39/307 [00:03<00:25, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([304, 305, 306, 307, 308, 309, 310, 311])\n",
      "tensor([312, 313, 314, 315, 316, 317, 318, 319])\n",
      "tensor([320, 321, 322, 323, 324, 325, 326, 327])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 43/307 [00:04<00:24, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([328, 329, 330, 331, 332, 333, 334, 335])\n",
      "tensor([336, 337, 338, 339, 340, 341, 342, 343])\n",
      "tensor([344, 345, 346, 347, 348, 349, 350, 351])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 45/307 [00:04<00:24, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([352, 353, 354, 355, 356, 357, 358, 359])\n",
      "tensor([360, 361, 362, 363, 364, 365, 366, 367])\n",
      "tensor([368, 369, 370, 371, 372, 373, 374, 375])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 49/307 [00:04<00:24, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([376, 377, 378, 379, 380, 381, 382, 383])\n",
      "tensor([384, 385, 386, 387, 388, 389, 390, 391])\n",
      "tensor([392, 393, 394, 395, 396, 397, 398, 399])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 51/307 [00:04<00:23, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([400, 401, 402, 403, 404, 405, 406, 407])\n",
      "tensor([408, 409, 410, 411, 412, 413, 414, 415])\n",
      "tensor([416, 417, 418, 419, 420, 421, 422, 423])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 52/307 [00:05<00:24, 10.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m grad_dict\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "File \u001b[0;32m~/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_grad_dict = {}\n",
    "model.eval()\n",
    "for step,batch in enumerate(tqdm(dataloader)):\n",
    "    model.zero_grad()\n",
    "    print(batch['ids'])\n",
    "    batch = batch.to(model.device)\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    loss.backward()\n",
    "    grad_dict={}\n",
    "    for k, v in model.named_parameters():\n",
    "        if 'lora_A' in k:\n",
    "            grad_dict[k]=v.grad.cpu()\n",
    "        elif 'lora_B' in k:\n",
    "            # first index of shape indicates low-rank\n",
    "            grad_dict[k]=v.grad.cpu().T\n",
    "        else:\n",
    "            pass\n",
    "    tr_grad_dict[step]=grad_dict\n",
    "    # grad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python zip.py --data_path ER/semi-text-c/train.csv --save_path zip_selected_data.json --budget 1000 --k1 4500 --k2 200 --k3 100 --n_jobs 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_json('/data/home/wangys/DataSelection-IF/ER/semi-text-c/train.json')\n",
    "train.to_csv('ER/semi-text-c/train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/data/home/wangys/DataSelection-IF/ER/semi-text-c/train.csv').iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv('ER/semi-text-c/train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict[0]['base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 'sft'\n",
    "import torch\n",
    "trainset = torch.load('trainset_ER_mistral_512.pkl')\n",
    "tokenizer_module = load_tokenizer(model_args)\n",
    "tokenizer = tokenizer_module[\"tokenizer\"]\n",
    "template = get_template_and_fix_tokenizer(tokenizer, data_args)\n",
    "data_collator = SFTDataCollatorWith4DAttentionMask(\n",
    "            template=template, tokenizer=tokenizer, label_pad_token_id=IGNORE_INDEX\n",
    "        )\n",
    "dataloader = DataLoader(trainset, batch_size=8, shuffle=False, collate_fn=data_collator, pin_memory=True)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "total_ppl = 0\n",
    "perplexities = []\n",
    "batch: Dict[str, \"torch.Tensor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step,batch in enumerate(dataloader):\n",
    "    example = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict = {}\n",
    "model.eval()\n",
    "for step,batch in enumerate(tqdm(dataloader)):\n",
    "    model.zero_grad()\n",
    "    # for key in batch.keys():\n",
    "    #     batch[key] = batch[key][:,-256:]\n",
    "    batch = batch.to(model.device)\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # loss = sentence_logps.mean()  # 计算平均loss\n",
    "    loss = outputs.loss\n",
    "    print(loss.item())\n",
    "    \n",
    "    shift_logits: \"torch.Tensor\" = outputs[\"logits\"][..., :-1, :]\n",
    "    shift_labels: \"torch.Tensor\" = batch[\"labels\"][..., 1:]\n",
    "    loss_mask = shift_labels != IGNORE_INDEX\n",
    "    flatten_logits = shift_logits.contiguous().view(shift_labels.size(0) * shift_labels.size(1), -1)\n",
    "    flatten_labels = shift_labels.contiguous().view(-1)\n",
    "    token_logps: \"torch.Tensor\" = criterion(flatten_logits, flatten_labels)\n",
    "    token_logps = token_logps.contiguous().view(shift_logits.size(0), -1)\n",
    "    sentence_logps = (token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "    print(sentence_logps.mean())\n",
    "    loss.backward(retain_graph=True)\n",
    "    grad_dict={}\n",
    "    for k, v in model.named_parameters():\n",
    "        if 'lora_A' in k:\n",
    "            grad_dict[k]=v.grad.cpu()\n",
    "        elif 'lora_B' in k:\n",
    "            # first index of shape indicates low-rank\n",
    "            grad_dict[k]=v.grad.cpu().T\n",
    "        else:\n",
    "            pass\n",
    "    tr_grad_dict[step]=grad_dict\n",
    "    # del grad_dict\n",
    "    # break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(trainset,'trainset_ER.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = MultiModalDataCollatorForSeq2Seq(\n",
    "            template=template, tokenizer=tokenizer, label_pad_token_id=IGNORE_INDEX\n",
    "        )\n",
    "dataloader = DataLoader(trainset, batch_size=8, shuffle=False, collate_fn=data_collator, pin_memory=True)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "total_ppl = 0\n",
    "perplexities = []\n",
    "batch: Dict[str, \"torch.Tensor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict = {}\n",
    "model.eval()\n",
    "for step,batch in enumerate(tqdm(dataloader)):\n",
    "    model.zero_grad()\n",
    "    # for key in batch.keys():\n",
    "    #     batch[key] = batch[key][:,-256:]\n",
    "    batch = batch.to(model.device)\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # loss = sentence_logps.mean()  # 计算平均loss\n",
    "    loss = outputs.loss\n",
    "    print(loss.item())\n",
    "    \n",
    "    shift_logits: \"torch.Tensor\" = outputs[\"logits\"][..., :-1, :]\n",
    "    shift_labels: \"torch.Tensor\" = batch[\"labels\"][..., 1:]\n",
    "    loss_mask = shift_labels != IGNORE_INDEX\n",
    "    flatten_logits = shift_logits.contiguous().view(shift_labels.size(0) * shift_labels.size(1), -1)\n",
    "    flatten_labels = shift_labels.contiguous().view(-1)\n",
    "    token_logps: \"torch.Tensor\" = criterion(flatten_logits, flatten_labels)\n",
    "    token_logps = token_logps.contiguous().view(shift_logits.size(0), -1)\n",
    "    sentence_logps = (token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "    print(sentence_logps.mean())\n",
    "    loss.backward(retain_graph=True)\n",
    "    grad_dict={}\n",
    "    for k, v in model.named_parameters():\n",
    "        if 'lora_A' in k:\n",
    "            grad_dict[k]=v.grad.cpu()\n",
    "        elif 'lora_B' in k:\n",
    "            # first index of shape indicates low-rank\n",
    "            grad_dict[k]=v.grad.cpu().T\n",
    "        else:\n",
    "            pass\n",
    "    tr_grad_dict[step]=grad_dict\n",
    "    # del grad_dict\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_dict['base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_grad_dict = {}\n",
    "# model.eval()\n",
    "# for batch in tqdm(dataloader):\n",
    "#     model.zero_grad()\n",
    "#     batch = batch.to(model.device)\n",
    "#     outputs = model(**batch)\n",
    "#     shift_logits: \"torch.Tensor\" = outputs[\"logits\"][..., :-1, :]\n",
    "#     shift_labels: \"torch.Tensor\" = batch[\"labels\"][..., 1:]\n",
    "#     loss_mask = shift_labels != IGNORE_INDEX\n",
    "#     flatten_logits = shift_logits.contiguous().view(shift_labels.size(0) * shift_labels.size(1), -1)\n",
    "#     flatten_labels = shift_labels.contiguous().view(-1)\n",
    "#     token_logps: \"torch.Tensor\" = criterion(flatten_logits, flatten_labels)\n",
    "#     token_logps = token_logps.contiguous().view(shift_logits.size(0), -1)\n",
    "#     sentence_logps = (token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "#     loss = sentence_logps.mean()  # 计算平均loss\n",
    "#     loss.backward()\n",
    "#     grad_dict={}\n",
    "#     for k, v in model.named_parameters():\n",
    "#         if 'lora_A' in k:\n",
    "#             grad_dict[k]=v.grad.cpu()\n",
    "#         elif 'lora_B' in k:\n",
    "#             # first index of shape indicates low-rank\n",
    "#             grad_dict[k]=v.grad.cpu().T\n",
    "#         else:\n",
    "#             pass\n",
    "#     # tr_grad_dict[step]=grad_dict\n",
    "#     del grad_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qwen-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args, data_args, training_args, finetuning_args, _ = get_train_args(\n",
    "    dict(\n",
    "        stage='sft',\n",
    "        model_name_or_path='/data/home/wangys/model/Qwen2.5-0.5B-Instruct',\n",
    "        adapter_name_or_path = 'saves/mistral-7b/Abt-Buy-Match-P1-q_proj-qwen',\n",
    "        dataset='Abt-Buy-Match-P1',\n",
    "        dataset_dir='data',\n",
    "        template='qwen',\n",
    "        cutoff_len=512,\n",
    "        max_samples=None,\n",
    "        train_on_prompt=False,\n",
    "        output_dir=\"output\",\n",
    "        overwrite_cache=False,\n",
    "        do_train=True,\n",
    "        # quantization_bit=8\n",
    "        use_unsloth = False,\n",
    "        enable_liger_kernel = True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 'sft'\n",
    "\n",
    "tokenizer_module = load_tokenizer(model_args)\n",
    "tokenizer = tokenizer_module[\"tokenizer\"]\n",
    "template = get_template_and_fix_tokenizer(tokenizer, data_args)\n",
    "trainset = get_dataset(template, model_args, data_args, training_args, stage, **tokenizer_module)[\"train_dataset\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(trainset,'trainset_ER_mistral_512.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model(tokenizer, model_args, finetuning_args, is_trainable=True)\n",
    "data_collator = MultiModalDataCollatorForSeq2Seq(\n",
    "            template=template, tokenizer=tokenizer, label_pad_token_id=IGNORE_INDEX\n",
    "        )\n",
    "dataloader = DataLoader(trainset, batch_size=16, shuffle=False, collate_fn=data_collator, pin_memory=True)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "total_ppl = 0\n",
    "perplexities = []\n",
    "batch: Dict[str, \"torch.Tensor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(trainset,'trainset_ER_qwen.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict = {}\n",
    "model.eval()\n",
    "for step,batch in enumerate(tqdm(dataloader)):\n",
    "    model.zero_grad()\n",
    "    batch = batch.to(model.device)\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    # shift_logits: \"torch.Tensor\" = outputs[\"logits\"][..., :-1, :]\n",
    "    # shift_labels: \"torch.Tensor\" = batch[\"labels\"][..., 1:]\n",
    "    # loss_mask = shift_labels != IGNORE_INDEX\n",
    "    # flatten_logits = shift_logits.contiguous().view(shift_labels.size(0) * shift_labels.size(1), -1)\n",
    "    # flatten_labels = shift_labels.contiguous().view(-1)\n",
    "    # token_logps: \"torch.Tensor\" = criterion(flatten_logits, flatten_labels)\n",
    "    # token_logps = token_logps.contiguous().view(shift_logits.size(0), -1)\n",
    "    # sentence_logps = (token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "    # loss = sentence_logps.mean()  # 计算平均loss\n",
    "    loss.backward()\n",
    "    grad_dict={}\n",
    "    for k, v in model.named_parameters():\n",
    "        if 'lora_A' in k:\n",
    "            grad_dict[k]=v.grad.cpu()\n",
    "        elif 'lora_B' in k:\n",
    "            # first index of shape indicates low-rank\n",
    "            grad_dict[k]=v.grad.cpu().T\n",
    "        else:\n",
    "            pass\n",
    "    tr_grad_dict[step]=grad_dict\n",
    "    # grad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict[0]['base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_dict['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
