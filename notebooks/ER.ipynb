{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "# è‡ªå®šä¹‰ Dataset ç±»\n",
    "class EntityMatchingDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.sentence_1 = dataframe['sentence_1'].tolist()\n",
    "        self.sentence_2 = dataframe['sentence_2'].tolist()\n",
    "        self.labels = dataframe['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1 = self.sentence_1[idx]\n",
    "        text2 = self.sentence_2[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # å°†ä¸¤ä¸ªå¥å­è¿æ¥åœ¨ä¸€èµ·å¹¶è¿›è¡Œ tokenization\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            text2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# è¯»å– CSV æ•°æ®\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GradientHook:\n",
    "    def __init__(self, model):\n",
    "        self.gradients = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "        # ä¸ºæ¯ä¸€å±‚æ³¨å†Œé’©å­\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):  # å¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–å±‚ç±»å‹\n",
    "                hook = module.register_backward_hook(self._save_gradient(name))\n",
    "                self.hooks.append(hook)\n",
    "\n",
    "    def _save_gradient(self, layer_name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            self.gradients[layer_name] = grad_output[0].detach().cpu()  # è·å–æ¯å±‚çš„æ¢¯åº¦\n",
    "        return hook\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gradient_hook = None  # ç”¨äºå­˜å‚¨æ¢¯åº¦é’©å­\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        self.gradient_hook = GradientHook(self.model)  # åœ¨è¯„ä¼°æ—¶å¯ç”¨æ¢¯åº¦é’©å­\n",
    "        result = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)  # æ‰§è¡Œè¯„ä¼°\n",
    "        \n",
    "        gradients = self.gradient_hook.get_gradients()  # è·å–æ¢¯åº¦ä¿¡æ¯\n",
    "        self.gradient_hook.remove_hooks()  # è¯„ä¼°å®Œæ¯•åç§»é™¤é’©å­\n",
    "\n",
    "        # åœ¨è¿”å›ç»“æœä¸­åŠ å…¥æ¢¯åº¦ä¿¡æ¯\n",
    "        result[\"gradients\"] = gradients\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /home/yanmy/model/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-13 19:47:20,947] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanmy/anaconda3/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/yanmy/model/deberta-v3-base - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/yanmy/model/deberta-v3-base - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/yanmy/model/deberta-v3-base - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/yanmy/model/deberta-v3-base - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 2.9041, 'eval_samples_per_second': 659.751, 'eval_steps_per_second': 2.755, 'epoch': 3.0, 'gradients': {}}\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_json('/home/yanmy/Transfer-ER/Unicorn-main/data/Abt-Buy-Fewshot/train.json')\n",
    "valid_df = pd.read_json('/home/yanmy/Transfer-ER/Unicorn-main/data/Abt-Buy-Fewshot/valid.json')\n",
    "test_df = pd.read_json('/home/yanmy/Transfer-ER/Unicorn-main/data/Abt-Buy-Fewshot/test.json')\n",
    "train_df.columns = ['sentence_1', 'sentence_2', 'label']\n",
    "valid_df.columns = ['sentence_1', 'sentence_2', 'label']\n",
    "test_df.columns = ['sentence_1', 'sentence_2', 'label']\n",
    "# åŠ è½½ DeBERTa-v3-base çš„ tokenizer\n",
    "# tokenizer = DebertaTokenizer.from_pretrained('/home/yanmy/model/deberta-v3-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/yanmy/model/deberta-v3-base', padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# å®šä¹‰æ•°æ®é›†\n",
    "train_dataset = EntityMatchingDataset(train_df, tokenizer)\n",
    "valid_dataset = EntityMatchingDataset(valid_df, tokenizer)\n",
    "test_dataset = EntityMatchingDataset(test_df, tokenizer)\n",
    "\n",
    "# åŠ è½½ DeBERTa-v3-base æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/home/yanmy/model/deberta-v3-base', num_labels=2)\n",
    "\n",
    "# é…ç½® LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=16,  # é€šå¸¸ LoRA alpha è®¾ç½®ä¸º 2 å€çš„ rank\n",
    "    target_modules=[\"value_proj\"],  # å¾®è°ƒåˆ†ç±»å±‚\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# å®šä¹‰ f1_score è¯„ä¼°å‡½æ•°\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=128,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨ Trainer è¿›è¡Œè®­ç»ƒ\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=valid_dataset,\n",
    "#     # compute_metrics=compute_metrics  # è®¾ç½® f1_score ä½œä¸ºè¯„ä¼°æŒ‡æ ‡\n",
    "# )\n",
    "\n",
    "# # å¼€å§‹è®­ç»ƒ\n",
    "# trainer.train()\n",
    "\n",
    "# # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°\n",
    "# metrics = trainer.evaluate(test_dataset)\n",
    "# print(metrics)  # è¾“å‡ºåŒ…å« f1_score çš„è¯„ä¼°ç»“æœ\n",
    "\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "# è¿›è¡Œè¯„ä¼°\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb8a8e7bd3f47b7a7b4915034a7a19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(text1, text2):\n",
    "    # è·å–æ¨¡å‹æ‰€åœ¨è®¾å¤‡\n",
    "    device = model.device\n",
    "    \n",
    "    # å°†æ–‡æœ¬è½¬æ¢ä¸ºå¼ é‡\n",
    "    inputs = tokenizer(text1, text2, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # å°†è¾“å…¥æ•°æ®ç§»åˆ°ç›¸åŒçš„è®¾å¤‡ä¸Š\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # è¿›è¡Œæ¨ç†\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # è·å–é¢„æµ‹ç±»åˆ«\n",
    "    predicted_class = logits.argmax().item()\n",
    "    \n",
    "    return predicted_class  # 1: åŒä¸€å®ä½“, 0: ä¸åŒå®ä½“\n",
    "\n",
    "predict_all = []\n",
    "from tqdm.notebook import tqdm\n",
    "for sentence_1, sentence_2, label in tqdm(test_df.values):\n",
    "    prediction = predict(sentence_1, sentence_2)\n",
    "    predict_all.append(prediction)\n",
    "label_all = test_df['label'].tolist()\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true=label_all, y_pred=predict_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
