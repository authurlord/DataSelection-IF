{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cOllL4Ms4Ed"
      },
      "source": [
        "## 1. Create DataLoader, Model with LoRA and Influence Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7o9YoXLls4Ef"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "#### DataLoader\n",
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"ER\": (\"sentence_1\", \"sentence_2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}\n",
        "\n",
        "def flip_label(example, ind, noise_index):\n",
        "    '''\n",
        "    Flip the label to make the noisy datapoints\n",
        "    '''\n",
        "    if ind in noise_index:\n",
        "        example[\"label\"] = 1 - example[\"label\"]\n",
        "    return example\n",
        "\n",
        "def load_noisy_dataset_by_task(task=\"mrpc\", noise_ratio=0.1):\n",
        "    glue_datasets = load_dataset(\"glue\", task)\n",
        "    n_train = len(glue_datasets['train'])\n",
        "    n_val = len(glue_datasets['validation'])\n",
        "    if n_train > 4500 and n_val > 500:\n",
        "        new_n_train_list = np.random.choice(n_train, 4500, replace=False)\n",
        "        new_n_val_list = np.random.choice(n_val, 500, replace=False)\n",
        "        glue_datasets['train'] = glue_datasets['train'].select(new_n_train_list)\n",
        "        glue_datasets['validation'] = glue_datasets['validation'].select(new_n_val_list)\n",
        "\n",
        "    n_train = len(glue_datasets['train'])\n",
        "    n_val = len(glue_datasets['validation'])\n",
        "    if noise_ratio > 0.0:\n",
        "        noise_index = np.random.choice(n_train,\n",
        "                                       size=int(noise_ratio*n_train),\n",
        "                                       replace=False)\n",
        "    else:\n",
        "        noise_index = []\n",
        "\n",
        "    glue_datasets['train'] = glue_datasets['train'].map(flip_label,\n",
        "                                                        with_indices=True,\n",
        "                                                        fn_kwargs={'noise_index':noise_index})\n",
        "    return glue_datasets, noise_index\n",
        "\n",
        "def create_dataloaders(model_name_or_path=\"roberta-large\",\n",
        "                       task=\"mrpc\",\n",
        "                       noise_ratio=0.1,\n",
        "                       batch_size=32):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"right\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    sentence1_key, sentence2_key = task_to_keys[task]\n",
        "    def tokenize_function(examples, max_length=128):\n",
        "        # max_length=None => use the model max length (it's actually the default)\n",
        "        if sentence2_key is None:\n",
        "            outputs = tokenizer(examples[sentence1_key], truncation=True, max_length=max_length)\n",
        "        else:\n",
        "            outputs = tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=max_length)\n",
        "        return outputs\n",
        "\n",
        "    noisy_datasets, noise_index=load_noisy_dataset_by_task(task=task, noise_ratio=noise_ratio)\n",
        "    if sentence2_key is None:\n",
        "        tokenized_datasets = noisy_datasets.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"idx\", sentence1_key],\n",
        "        )\n",
        "    else:\n",
        "        tokenized_datasets = noisy_datasets.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"idx\", sentence1_key, sentence2_key],\n",
        "        )\n",
        "\n",
        "    # We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\n",
        "    # transformers library\n",
        "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
        "\n",
        "    train_dataloader = DataLoader(tokenized_datasets[\"train\"],\n",
        "                                  shuffle=True,\n",
        "                                  collate_fn=collate_fn,\n",
        "                                  batch_size=batch_size)\n",
        "    eval_dataloader = DataLoader(tokenized_datasets[\"validation\"],\n",
        "                                 shuffle=False,\n",
        "                                 collate_fn=collate_fn,\n",
        "                                 batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, eval_dataloader, noise_index, tokenized_datasets, collate_fn\n",
        "\n",
        "def create_dataloaders_ER(model_name_or_path=\"roberta-large\",\n",
        "                       task=\"mrpc\",\n",
        "                       noise_ratio=0.1,\n",
        "                       batch_size=32,\n",
        "                       train_file=\"train.json\",\n",
        "                       valid_file=\"valid.json\",\n",
        "                       test_file=\"test.json\"):\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"right\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    # Define task-specific keys for sentence1 and sentence2\n",
        "    sentence1_key, sentence2_key = task_to_keys[task]\n",
        "    \n",
        "    def tokenize_function(examples, max_length=128):\n",
        "        # Tokenize based on the task, considering whether there is a second sentence\n",
        "        if sentence2_key is None:\n",
        "            outputs = tokenizer(examples[sentence1_key], truncation=True, max_length=max_length)\n",
        "        else:\n",
        "            outputs = tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=max_length)\n",
        "        return outputs\n",
        "    \n",
        "    # Load the dataset from JSON files using pandas\n",
        "    train_df = pd.read_json(train_file)\n",
        "    valid_df = pd.read_json(valid_file)\n",
        "    test_df = pd.read_json(test_file)\n",
        "    train_df.columns = ['sentence_1', 'sentence_2', 'label']\n",
        "    valid_df.columns = ['sentence_1', 'sentence_2', 'label']\n",
        "    test_df.columns = ['sentence_1', 'sentence_2', 'label']\n",
        "    \n",
        "    # Convert the DataFrames into the HuggingFace Dataset format\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    valid_dataset = Dataset.from_pandas(valid_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "    \n",
        "    # If noise is required, apply the noise to the training set\n",
        "    if noise_ratio > 0.0:\n",
        "        n_train = len(train_dataset)\n",
        "        noise_index = np.random.choice(n_train, size=int(noise_ratio * n_train), replace=False)\n",
        "        def flip_label(example, ind, noise_index):\n",
        "            if ind in noise_index:\n",
        "                example[\"label\"] = 1 - example[\"label\"]\n",
        "            return example\n",
        "        train_dataset = train_dataset.map(flip_label, with_indices=True, fn_kwargs={'noise_index': noise_index})\n",
        "    else:\n",
        "        noise_index = []\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    if sentence2_key is None:\n",
        "        tokenized_train = train_dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"sentence_1\"]  # Only remove sentence_1\n",
        "        )\n",
        "        tokenized_valid = valid_dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"sentence_1\"]\n",
        "        )\n",
        "        tokenized_test = test_dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"sentence_1\"]\n",
        "        )\n",
        "    else:\n",
        "        tokenized_train = train_dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"sentence_1\", \"sentence_2\"]\n",
        "        )\n",
        "        tokenized_valid = valid_dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"sentence_1\", \"sentence_2\"]\n",
        "        )\n",
        "        tokenized_test = test_dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"sentence_1\", \"sentence_2\"]\n",
        "        )\n",
        "\n",
        "    # Rename the 'label' column to 'labels' for compatibility with models\n",
        "    tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
        "    tokenized_valid = tokenized_valid.rename_column(\"label\", \"labels\")\n",
        "    tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n",
        "    \n",
        "    # Define the collate function for padding\n",
        "    def collate_fn(examples):\n",
        "        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
        "\n",
        "    # Create the DataLoader for train, validation, and test datasets\n",
        "    tokenized_dataset = {}\n",
        "    tokenized_dataset['train'] = tokenized_train\n",
        "    tokenized_dataset['validation'] = tokenized_valid\n",
        "    tokenized_dataset['test'] = tokenized_test\n",
        "    train_dataloader = DataLoader(tokenized_dataset['train'],\n",
        "                                  shuffle=True, \n",
        "                                  collate_fn=collate_fn,\n",
        "                                  batch_size=batch_size)\n",
        "    eval_dataloader = DataLoader(tokenized_dataset['validation'], \n",
        "                                 shuffle=False, \n",
        "                                 collate_fn=collate_fn, \n",
        "                                 batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(tokenized_dataset['test'], \n",
        "                                 shuffle=False, \n",
        "                                 collate_fn=collate_fn, \n",
        "                                 batch_size=batch_size)\n",
        "    \n",
        "    return train_dataloader, eval_dataloader, test_dataloader, noise_index, tokenized_dataset, collate_fn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MGmvTNh3s4Eg"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import pickle, os\n",
        "import torch\n",
        "import numpy as np\n",
        "### Influence Function Computation by different methods\n",
        "class IFEngine(object):\n",
        "    def __init__(self):\n",
        "        self.time_dict=defaultdict(list)\n",
        "        self.hvp_dict=defaultdict(list)\n",
        "        self.IF_dict=defaultdict(list)\n",
        "\n",
        "    def preprocess_gradients(self, tr_grad_dict, val_grad_dict, noise_index=None):\n",
        "        self.tr_grad_dict = tr_grad_dict\n",
        "        self.val_grad_dict = val_grad_dict\n",
        "        self.noise_index = noise_index\n",
        "\n",
        "        self.n_train = len(self.tr_grad_dict.keys())\n",
        "        self.n_val = len(self.val_grad_dict.keys())\n",
        "        self.compute_val_grad_avg()\n",
        "\n",
        "    def compute_val_grad_avg(self):\n",
        "        # Compute the avg gradient on the validation dataset\n",
        "        self.val_grad_avg_dict={}\n",
        "        for weight_name in self.val_grad_dict[0]:\n",
        "            self.val_grad_avg_dict[weight_name]=torch.zeros(self.val_grad_dict[0][weight_name].shape).to(self.val_grad_dict[0][weight_name].device)\n",
        "            for val_id in self.val_grad_dict:\n",
        "                self.val_grad_avg_dict[weight_name] += self.val_grad_dict[val_id][weight_name] / self.n_val\n",
        "\n",
        "    def compute_hvps(self, lambda_const_param=10, compute_accurate=True, compute_LiSSA=True):\n",
        "        '''\n",
        "        Compute the influence function score under each method\n",
        "        '''\n",
        "        self.compute_hvp_iterative(lambda_const_param=lambda_const_param) ## HyperInf\n",
        "        self.compute_hvp_identity() ## Baseline TracIn\n",
        "        self.compute_hvp_proposed(lambda_const_param=lambda_const_param) ## Datainf\n",
        "        if compute_LiSSA:\n",
        "            self.compute_hvp_LiSSA(lambda_const_param=lambda_const_param)\n",
        "        if compute_accurate:\n",
        "            self.compute_hvp_accurate(lambda_const_param=lambda_const_param)\n",
        "\n",
        "    def compute_hvp_identity(self):\n",
        "        '''\n",
        "        TracIN\n",
        "        '''\n",
        "        start_time = time()\n",
        "        self.hvp_dict['identity'] = self.val_grad_avg_dict.copy()\n",
        "        self.time_dict['identity'] = time()-start_time\n",
        "        print(\"Time taken for Hessian-free: \", self.time_dict['identity'])\n",
        "\n",
        "    def compute_hvp_iterative(self, lambda_const_param=10, n_iteration=30):\n",
        "        '''\n",
        "        Compute the influence funcion score by our method HyperINF\n",
        "        '''\n",
        "\n",
        "        def schulz_inverse_stable(A, damping_factor=0, max_iterations=20, tol=1e-6):\n",
        "            n = A.shape[0]\n",
        "            #I = np.eye(n)\n",
        "            I = torch.eye(n, device=A.device)\n",
        "            A_damped = A + damping_factor * I  # Apply damping\n",
        "\n",
        "            #X = np.eye(n) * 0.00005  # Initial estimate of inverse matrix\n",
        "            X = torch.eye(n, device=A.device) * 0.00005  # Initial estimate of inverse matrix\n",
        "\n",
        "            for _ in range(max_iterations):\n",
        "                #X = X.dot(2 * I - A_damped.dot(X))\n",
        "                X = X @ (2 * I - A_damped @ X)\n",
        "\n",
        "                # # Check for convergence\n",
        "                # if np.linalg.norm(I - A.dot(X)) < tol:\n",
        "                #     break\n",
        "\n",
        "            return X\n",
        "\n",
        "        start_time = time()\n",
        "        hvp_iterative_dict={}\n",
        "\n",
        "        for _, weight_name in enumerate(tqdm(self.val_grad_avg_dict)):\n",
        "            # lambda_const computation = 0.1 x (n * d_l)^(-1) \\sum_{i=1}^{n} ||grad_i^l||_2^2\n",
        "            S=torch.zeros(len(self.tr_grad_dict.keys())).to(self.val_grad_avg_dict[weight_name].device)\n",
        "            for tr_id in self.tr_grad_dict:\n",
        "                tmp_grad = self.tr_grad_dict[tr_id][weight_name]\n",
        "                S[tr_id]=torch.mean(tmp_grad**2)\n",
        "            lambda_const = torch.mean(S) / lambda_const_param # layer-wise lambda\n",
        "\n",
        "            # iterative hvp computation\n",
        "            # G_l: same shape of self.tr_grad_dict[0][weight_name].T @ self.tr_grad_dict[0][weight_name]\n",
        "            G_l = torch.zeros((self.tr_grad_dict[0][weight_name].T @ self.tr_grad_dict[0][weight_name]).shape).to(self.val_grad_avg_dict[weight_name].device)\n",
        "            for tr_id in self.tr_grad_dict:\n",
        "                tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device) # (grad_i^l)^T\n",
        "                G_l += tmp_grad.T @ tmp_grad / self.n_train\n",
        "\n",
        "            G_l = G_l + lambda_const * torch.eye(G_l.shape[0], device=G_l.device)\n",
        "           # G_l = G_l.cpu().detach().numpy()\n",
        "            G_l_inv = schulz_inverse_stable(G_l, damping_factor=0.001, max_iterations=n_iteration, tol=1e-6)\n",
        "\n",
        "            hvp_iterative_dict[weight_name] = torch.tensor(self.val_grad_avg_dict[weight_name] @ G_l_inv)\n",
        "            #print(hvp_iterative_dict[weight_name])\n",
        "        self.hvp_dict['iterative'] = hvp_iterative_dict\n",
        "        self.time_dict['iterative'] = time()-start_time\n",
        "        print(\"Time taken for HyperINF: \", self.time_dict['iterative'])\n",
        "\n",
        "\n",
        "\n",
        "    def compute_hvp_proposed(self, lambda_const_param=10):\n",
        "        '''\n",
        "        DataInf method\n",
        "        '''\n",
        "        start_time = time()\n",
        "        hvp_proposed_dict={}\n",
        "\n",
        "        for _ , weight_name in enumerate(tqdm(self.val_grad_avg_dict)):\n",
        "            # lambda_const computation = 0.1 x (n * d_l)^(-1) \\sum_{i=1}^{n} ||grad_i^l||_2^2\n",
        "            S=torch.zeros(len(self.tr_grad_dict.keys())).to(self.val_grad_avg_dict[weight_name].device)\n",
        "            for tr_id in self.tr_grad_dict:\n",
        "                tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device)\n",
        "                S[tr_id]=torch.mean(tmp_grad**2)\n",
        "            lambda_const = torch.mean(S) / lambda_const_param # layer-wise lambda\n",
        "\n",
        "            # hvp computation\n",
        "            hvp=torch.zeros(self.val_grad_avg_dict[weight_name].shape).to(self.val_grad_avg_dict[weight_name].device)\n",
        "            for tr_id in self.tr_grad_dict: # i\n",
        "                tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device) # grad_i^l\n",
        "                # L_(l,i) / (lambda + ||grad_i^l||_2^2) in Eqn. (5)\n",
        "                C_tmp = torch.sum(self.val_grad_avg_dict[weight_name] * tmp_grad) / (lambda_const + torch.sum(tmp_grad**2)).to(self.val_grad_avg_dict[weight_name].device)\n",
        "                # (v_l^T - C_tmp * (grad_i^l)^T ) / (n * lambda) in Eqn. (5)\n",
        "                hvp += (self.val_grad_avg_dict[weight_name] - C_tmp*tmp_grad) / (self.n_train*lambda_const)\n",
        "            hvp_proposed_dict[weight_name] = hvp\n",
        "        self.hvp_dict['proposed'] = hvp_proposed_dict\n",
        "        self.time_dict['proposed'] = time()-start_time\n",
        "        print(\"Time taken for Datainf: \", self.time_dict['proposed'])\n",
        "\n",
        "    def compute_hvp_accurate(self, lambda_const_param=10):\n",
        "        start_time = time()\n",
        "        hvp_accurate_dict={}\n",
        "        for _ , weight_name in enumerate(tqdm(self.val_grad_avg_dict)):\n",
        "\n",
        "            # lambda_const computation\n",
        "            S=torch.zeros(len(self.tr_grad_dict.keys()))\n",
        "            for tr_id in self.tr_grad_dict:\n",
        "                tmp_grad = self.tr_grad_dict[tr_id][weight_name]\n",
        "                S[tr_id]=torch.mean(tmp_grad**2)\n",
        "            lambda_const = torch.mean(S) / lambda_const_param # layer-wise lambda\n",
        "\n",
        "            # hvp computation (eigenvalue decomposition)\n",
        "            AAt_matrix = torch.zeros(torch.outer(self.tr_grad_dict[0][weight_name].reshape(-1),\n",
        "                                                 self.tr_grad_dict[0][weight_name].reshape(-1)).shape)\n",
        "            for tr_id in self.tr_grad_dict:\n",
        "\n",
        "                tmp_mat = torch.outer(self.tr_grad_dict[tr_id][weight_name].reshape(-1),\n",
        "                                      self.tr_grad_dict[tr_id][weight_name].reshape(-1))\n",
        "                AAt_matrix += tmp_mat\n",
        "\n",
        "\n",
        "            L, V = torch.linalg.eig(AAt_matrix)\n",
        "            L, V = L.float(), V.float()\n",
        "            hvp = self.val_grad_avg_dict[weight_name].reshape(-1) @ V\n",
        "            hvp = (hvp / (lambda_const + L/ self.n_train)) @ V.T\n",
        "\n",
        "            hvp_accurate_dict[weight_name] = hvp.reshape(len(self.tr_grad_dict[0][weight_name]), -1)\n",
        "            del tmp_mat, AAt_matrix, V # to save memory\n",
        "        self.hvp_dict['accurate'] = hvp_accurate_dict\n",
        "        self.time_dict['accurate'] = time()-start_time\n",
        "        print(\"Time taken for Accurate: \", self.time_dict['accurate'])\n",
        "\n",
        "    def compute_hvp_LiSSA(self, lambda_const_param=10, n_iteration=10, alpha_const=1.):\n",
        "        '''\n",
        "        LiSSA method\n",
        "        '''\n",
        "        start_time = time()\n",
        "        hvp_LiSSA_dict={}\n",
        "\n",
        "        for _, weight_name in enumerate(tqdm(self.val_grad_avg_dict)):\n",
        "            # lambda_const computation\n",
        "            S=torch.zeros(len(self.tr_grad_dict.keys())).to(self.val_grad_avg_dict[weight_name].device)\n",
        "            for tr_id in self.tr_grad_dict:\n",
        "                tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device)\n",
        "                S[tr_id]=torch.mean(tmp_grad**2)\n",
        "            lambda_const = torch.mean(S) / lambda_const_param # layer-wise lambda\n",
        "\n",
        "            # hvp computation\n",
        "            running_hvp=self.val_grad_avg_dict[weight_name]\n",
        "            for _ in range(n_iteration):\n",
        "                hvp_tmp=torch.zeros(self.val_grad_avg_dict[weight_name].shape).to(self.val_grad_avg_dict[weight_name].device)\n",
        "                for tr_id in self.tr_grad_dict:\n",
        "                    tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device)\n",
        "                    hvp_tmp += (torch.sum(tmp_grad*running_hvp)*tmp_grad - lambda_const*running_hvp) / self.n_train\n",
        "\n",
        "                running_hvp = self.val_grad_avg_dict[weight_name] + running_hvp - alpha_const*hvp_tmp\n",
        "            hvp_LiSSA_dict[weight_name] = running_hvp\n",
        "\n",
        "        self.hvp_dict['LiSSA'] = hvp_LiSSA_dict\n",
        "        self.time_dict['LiSSA'] = time()-start_time\n",
        "        print(\"Time taken for LiSSA: \", self.time_dict['LiSSA'])\n",
        "\n",
        "    def compute_IF(self):\n",
        "        for method_name in self.hvp_dict:\n",
        "            if_tmp_dict = {}\n",
        "            for tr_id in self.tr_grad_dict:\n",
        "                if_tmp_value = 0\n",
        "                for weight_name in self.val_grad_avg_dict:\n",
        "                    if_tmp_value += torch.sum(self.hvp_dict[method_name][weight_name]*self.tr_grad_dict[tr_id][weight_name])\n",
        "                if_tmp_dict[tr_id]= -if_tmp_value.cpu()\n",
        "               # print(-if_tmp_value)\n",
        "\n",
        "            self.IF_dict[method_name] = pd.Series(if_tmp_dict, dtype=float).to_numpy()\n",
        "\n",
        "    def save_result(self, noise_index, run_id=0):\n",
        "        results={}\n",
        "        results['runtime']=self.time_dict\n",
        "        results['noise_index']=noise_index\n",
        "        results['influence']=self.IF_dict\n",
        "\n",
        "        with open(f\"./results_{run_id}.pkl\",'wb') as file:\n",
        "            pickle.dump(results, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "57K8aADbs4Eh"
      },
      "outputs": [],
      "source": [
        "## LoRA model\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    # BitsAndBytesConfig,\n",
        "    # LlamaForCausalLM,\n",
        "    # LlamaTokenizer\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model\n",
        ")\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "\n",
        "class LORAEngineMultiClass(object):\n",
        "    def __init__(self,\n",
        "                model_name_or_path=\"roberta-large\",\n",
        "                target_modules=[\"value\"],\n",
        "                train_dataloader=None,\n",
        "                eval_dataloader=None,\n",
        "                device=\"cuda\",\n",
        "                num_epochs=10,\n",
        "                lr=3e-4,\n",
        "                lora=True,\n",
        "                low_rank=2,\n",
        "                task=\"mrpc\"):\n",
        "        self.model_name_or_path=model_name_or_path\n",
        "        self.target_modules=target_modules\n",
        "        self.train_dataloader=train_dataloader\n",
        "        self.eval_dataloader=eval_dataloader\n",
        "        self.device=device\n",
        "        self.num_epochs=num_epochs\n",
        "        self.lr=lr\n",
        "        self.task=task\n",
        "        self.lora=lora\n",
        "        self.low_rank=low_rank\n",
        "\n",
        "    def build_LORA_model(self):\n",
        "        '''\n",
        "        This function fine-tunes a model for classification tasks.\n",
        "        For text generation tasks, please see `notebooks/Influential_Data_Identification-Llama2-Math.ipynb`.\n",
        "        '''\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name_or_path,\n",
        "                                                                        return_dict=True)\n",
        "        self.model.config.use_cache = False\n",
        "        self.model.config.pad_token_id = self.model.config.eos_token_id\n",
        "\n",
        "        if self.lora:\n",
        "            peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
        "                                    inference_mode=False,\n",
        "                                    target_modules=self.target_modules,\n",
        "                                    r=self.low_rank,\n",
        "                                    lora_alpha=self.low_rank * 2,\n",
        "                                    use_rslora=True,\n",
        "                                    #lora_dropout=0.05\n",
        "                                    )\n",
        "            self.model = get_peft_model(self.model, peft_config)\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "    def train_LORA_model(self):\n",
        "        '''\n",
        "        This function fine-tunes a model for GLUE classification tasks.\n",
        "        For text generation tasks, please see `notebooks/Influential_Data_Identification-Llama2-Math.ipynb`.\n",
        "        '''\n",
        "        # metric = evaluate.load(\"glue\", self.task)\n",
        "        metric = evaluate.load(\"/home/yanmy/evaluate-main/metrics/f1\", \"f1\")\n",
        "        optimizer = AdamW(params=self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        # Instantiate scheduler\n",
        "        lr_scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=0.06*(len(self.train_dataloader)*self.num_epochs),\n",
        "            num_training_steps=(len(self.train_dataloader)*self.num_epochs),\n",
        "        )\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()\n",
        "            for step, batch in enumerate(tqdm(self.train_dataloader)):\n",
        "                batch.to(self.device)\n",
        "                outputs = self.model(**batch)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            self.model.eval()\n",
        "            for step, batch in enumerate(tqdm(self.eval_dataloader)):\n",
        "                batch.to(self.device)\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(**batch)\n",
        "                predictions = outputs.logits.argmax(dim=-1)\n",
        "                predictions, references = predictions, batch[\"labels\"]\n",
        "                metric.add_batch(\n",
        "                    predictions=predictions,\n",
        "                    references=references,\n",
        "                )\n",
        "\n",
        "            eval_metric = metric.compute()\n",
        "            print(f\"Epoch {(epoch+1)}:\", eval_metric)\n",
        "\n",
        "\n",
        "    def compute_gradient(self, tokenized_datasets, collate_fn):\n",
        "        train_dataloader_stochastic = DataLoader(tokenized_datasets[\"train\"],\n",
        "                                                  shuffle=False,\n",
        "                                                  collate_fn=collate_fn,\n",
        "                                                  batch_size=1)\n",
        "\n",
        "        val_dataloader_stochastic = DataLoader(tokenized_datasets[\"test\"],\n",
        "                                                  shuffle=False,\n",
        "                                                  collate_fn=collate_fn,\n",
        "                                                  batch_size=1)\n",
        "        # Compute the gradient\n",
        "        self.model.eval()\n",
        "        tr_grad_dict = {}\n",
        "        for step, batch in enumerate(tqdm(train_dataloader_stochastic)):\n",
        "            self.model.zero_grad() # zeroing out gradient\n",
        "            batch.to(self.device)\n",
        "            outputs = self.model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            grad_dict={}\n",
        "\n",
        "            if self.lora:\n",
        "                for k, v in self.model.named_parameters():\n",
        "                    if 'lora_A' in k:\n",
        "                        grad_dict[k]=v.grad.cpu()\n",
        "                    elif 'lora_B' in k:\n",
        "                        # first index of shape indicates low-rank\n",
        "                        grad_dict[k]=v.grad.cpu().T\n",
        "                    elif 'modules_to_save.default.out_proj.weight' in k:\n",
        "                        grad_dict[k]=v.grad.cpu()\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "\n",
        "            tr_grad_dict[step]=grad_dict\n",
        "            del grad_dict\n",
        "\n",
        "        val_grad_dict = {}\n",
        "        for step, batch in enumerate(tqdm(val_dataloader_stochastic)):\n",
        "            self.model.zero_grad() # zeroing out gradient\n",
        "            batch.to(self.device)\n",
        "            outputs = self.model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            grad_dict={}\n",
        "\n",
        "            if self.lora:\n",
        "                for k, v in self.model.named_parameters():\n",
        "                    if 'lora_A' in k:\n",
        "                        grad_dict[k]=v.grad.cpu()\n",
        "                    elif 'lora_B' in k:\n",
        "                        # first index of shape indicates low-rank\n",
        "                        grad_dict[k]=v.grad.cpu().T\n",
        "                    elif 'modules_to_save.default.out_proj.weight' in k:\n",
        "                        grad_dict[k]=v.grad.cpu()\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "            val_grad_dict[step]=grad_dict\n",
        "            del grad_dict\n",
        "\n",
        "        return tr_grad_dict, val_grad_dict\n",
        "\n",
        "\n",
        "# class LORAEngine(object):\n",
        "#     def __init__(self,\n",
        "#                 model_name_or_path=\"roberta-large\",\n",
        "#                 target_modules=[\"value\"],\n",
        "#                 train_dataloader=None,\n",
        "#                 eval_dataloader=None,\n",
        "#                 device=\"cuda\",\n",
        "#                 num_epochs=10,\n",
        "#                 lr=3e-4,\n",
        "#                 lora=False,\n",
        "#                 low_rank=2,\n",
        "#                 task=\"mrpc\"):\n",
        "#         self.model_name_or_path=model_name_or_path\n",
        "#         self.target_modules=target_modules\n",
        "#         self.train_dataloader=train_dataloader\n",
        "#         self.eval_dataloader=eval_dataloader\n",
        "#         self.device=device\n",
        "#         self.num_epochs=num_epochs\n",
        "#         self.lr=lr\n",
        "#         self.task=task\n",
        "#         self.lora=lora\n",
        "#         self.low_rank=low_rank\n",
        "\n",
        "#     def build_LORA_model(self):\n",
        "#         '''\n",
        "#         This function fine-tunes a model for classification tasks.\n",
        "#         For text generation tasks, please see `notebooks/Influential_Data_Identification-Llama2-Math.ipynb`.\n",
        "#         '''\n",
        "#         self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name_or_path,\n",
        "#                                                                         return_dict=True)\n",
        "#         self.model.config.use_cache = False\n",
        "#         self.model.config.pad_token_id = self.model.config.eos_token_id\n",
        "\n",
        "#         if self.lora:\n",
        "#             peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
        "#                                     inference_mode=False,\n",
        "#                                     target_modules=self.target_modules,\n",
        "#                                     r=self.low_rank,\n",
        "#                                     lora_alpha=self.low_rank * 2,\n",
        "#                                     use_rslora=True,\n",
        "#                                     #lora_dropout=0.05\n",
        "#                                     )\n",
        "#             self.model = get_peft_model(self.model, peft_config)\n",
        "#             self.model.print_trainable_parameters()\n",
        "\n",
        "#     def train_LORA_model(self):\n",
        "#         '''\n",
        "#         This function fine-tunes a model for GLUE classification tasks.\n",
        "#         For text generation tasks, please see `notebooks/Influential_Data_Identification-Llama2-Math.ipynb`.\n",
        "#         '''\n",
        "#         metric = evaluate.load(\"glue\", self.task)\n",
        "#         optimizer = AdamW(params=self.model.parameters(), lr=self.lr)\n",
        "\n",
        "#         # Instantiate scheduler\n",
        "#         lr_scheduler = get_linear_schedule_with_warmup(\n",
        "#             optimizer=optimizer,\n",
        "#             num_warmup_steps=0.06*(len(self.train_dataloader)*self.num_epochs),\n",
        "#             num_training_steps=(len(self.train_dataloader)*self.num_epochs),\n",
        "#         )\n",
        "\n",
        "#         self.model.to(self.device)\n",
        "#         for epoch in range(self.num_epochs):\n",
        "#             self.model.train()\n",
        "#             for step, batch in enumerate(tqdm(self.train_dataloader)):\n",
        "#                 batch.to(self.device)\n",
        "#                 outputs = self.model(**batch)\n",
        "#                 loss = outputs.loss\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#                 lr_scheduler.step()\n",
        "#                 optimizer.zero_grad()\n",
        "\n",
        "#             self.model.eval()\n",
        "#             for step, batch in enumerate(tqdm(self.eval_dataloader)):\n",
        "#                 batch.to(self.device)\n",
        "#                 with torch.no_grad():\n",
        "#                     outputs = self.model(**batch)\n",
        "#                 predictions = outputs.logits.argmax(dim=-1)\n",
        "#                 predictions, references = predictions, batch[\"labels\"]\n",
        "#                 metric.add_batch(\n",
        "#                     predictions=predictions,\n",
        "#                     references=references,\n",
        "#                 )\n",
        "\n",
        "#             eval_metric = metric.compute()\n",
        "#             print(f\"Epoch {(epoch+1)}:\", eval_metric)\n",
        "\n",
        "\n",
        "#     def compute_gradient(self, tokenized_datasets, collate_fn):\n",
        "#         train_dataloader_stochastic = DataLoader(tokenized_datasets[\"train\"],\n",
        "#                                                   shuffle=False,\n",
        "#                                                   collate_fn=collate_fn,\n",
        "#                                                   batch_size=1)\n",
        "\n",
        "#         val_dataloader_stochastic = DataLoader(tokenized_datasets[\"validation\"],\n",
        "#                                                   shuffle=False,\n",
        "#                                                   collate_fn=collate_fn,\n",
        "#                                                   batch_size=1)\n",
        "#         # Compute the gradient\n",
        "#         self.model.eval()\n",
        "#         tr_grad_dict = {}\n",
        "#         for step, batch in enumerate(tqdm(train_dataloader_stochastic)):\n",
        "#             self.model.zero_grad() # zeroing out gradient\n",
        "#             batch.to(self.device)\n",
        "#             outputs = self.model(**batch)\n",
        "#             loss = outputs.loss\n",
        "#             loss.backward()\n",
        "\n",
        "#             grad_dict={}\n",
        "\n",
        "#             if self.lora:\n",
        "#                 for k, v in self.model.named_parameters():\n",
        "#                     if 'lora_A' in k:\n",
        "#                         grad_dict[k]=v.grad.cpu()\n",
        "#                     elif 'lora_B' in k:\n",
        "#                         # first index of shape indicates low-rank\n",
        "#                         grad_dict[k]=v.grad.cpu().T\n",
        "#                     elif 'modules_to_save.default.out_proj.weight' in k:\n",
        "#                         grad_dict[k]=v.grad.cpu()\n",
        "#                     else:\n",
        "#                         pass\n",
        "\n",
        "\n",
        "#             tr_grad_dict[step]=grad_dict\n",
        "#             del grad_dict\n",
        "\n",
        "#         val_grad_dict = {}\n",
        "#         for step, batch in enumerate(tqdm(val_dataloader_stochastic)):\n",
        "#             self.model.zero_grad() # zeroing out gradient\n",
        "#             batch.to(self.device)\n",
        "#             outputs = self.model(**batch)\n",
        "#             loss = outputs.loss\n",
        "#             loss.backward()\n",
        "\n",
        "#             grad_dict={}\n",
        "\n",
        "#             if self.lora:\n",
        "#                 for k, v in self.model.named_parameters():\n",
        "#                     if 'lora_A' in k:\n",
        "#                         grad_dict[k]=v.grad.cpu()\n",
        "#                     elif 'lora_B' in k:\n",
        "#                         # first index of shape indicates low-rank\n",
        "#                         grad_dict[k]=v.grad.cpu().T\n",
        "#                     elif 'modules_to_save.default.out_proj.weight' in k:\n",
        "#                         grad_dict[k]=v.grad.cpu()\n",
        "#                     else:\n",
        "#                         pass\n",
        "\n",
        "#             val_grad_dict[step]=grad_dict\n",
        "#             del grad_dict\n",
        "\n",
        "#         return tr_grad_dict, val_grad_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xksw53hWs4Eh"
      },
      "source": [
        "## 2. Mislabeled Data Detection on MRPC (Roberta-large, LoRA rank=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PrRm8ywbs4Ei",
        "outputId": "61796e5e-5c26-4e51-9ecb-998be2902370"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "559b125c636946e18da56964fa382220",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5237 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b5c8c4243a140bf87df62b860ec689b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5237 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "774bfa7915634895804b32ba79a50747",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4179 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/yanmy/model/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,624,514 || all params: 357,986,308 || trainable%: 0.7331\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/164 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100%|██████████| 164/164 [01:21<00:00,  2.01it/s]\n",
            "100%|██████████| 131/131 [00:32<00:00,  3.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: {'f1': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 164/164 [01:21<00:00,  2.01it/s]\n",
            "100%|██████████| 131/131 [00:32<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: {'f1': 0.1966205837173579}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 164/164 [01:21<00:00,  2.02it/s]\n",
            "100%|██████████| 131/131 [00:32<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: {'f1': 0.7511150758251561}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 164/164 [01:21<00:00,  2.02it/s]\n",
            "100%|██████████| 131/131 [00:32<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: {'f1': 0.7617391304347826}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 164/164 [01:21<00:00,  2.02it/s]\n",
            "100%|██████████| 131/131 [00:32<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: {'f1': 0.757679180887372}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5237/5237 [06:18<00:00, 13.84it/s]\n",
            "  5%|▍         | 245/5237 [00:18<06:08, 13.55it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m lora_engine\u001b[38;5;241m.\u001b[39mtrain_LORA_model()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m## get the train_grad and val_grad from fine-tuned model\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m tr_grad_dict, val_grad_dict \u001b[38;5;241m=\u001b[39m \u001b[43mlora_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m## to cuda\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m tr_grad_dict:\n",
            "Cell \u001b[0;32mIn[9], line 155\u001b[0m, in \u001b[0;36mLORAEngineMultiClass.compute_gradient\u001b[0;34m(self, tokenized_datasets, collate_fn)\u001b[0m\n\u001b[1;32m    153\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    154\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m--> 155\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m grad_dict\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora:\n",
            "File \u001b[0;32m/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/yanmy/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "## set seed for production\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "## model's settings\n",
        "model_name_or_path=\"/home/yanmy/model/roberta-large\"\n",
        "task = \"ER\"\n",
        "noise_ratio=0\n",
        "batch_size=32\n",
        "rank = 16\n",
        "target_modules=[\"value\",\"query\"]\n",
        "device=\"cuda\"\n",
        "num_epochs=5\n",
        "lr= 3e-5\n",
        "\n",
        "## load data\n",
        "# dataloader_outputs = create_dataloaders_ER(model_name_or_path=model_name_or_path,\n",
        "#                                             task=task,\n",
        "#                                             noise_ratio=noise_ratio,\n",
        "#                                             batch_size=batch_size)\n",
        "\n",
        "dataloader_outputs = create_dataloaders_ER(model_name_or_path=model_name_or_path,\n",
        "                                           task=task,\n",
        "                                           noise_ratio=noise_ratio,\n",
        "                                           batch_size=batch_size,\n",
        "                                           train_file = '/home/yanmy/DataInf/ER/semi-text-c-FUSER/train.json',\n",
        "                                           valid_file = '/home/yanmy/DataInf/ER/semi-text-c-FUSER/valid.json',\n",
        "                                           test_file = '/home/yanmy/DataInf/ER/semi-text-c-FUSER/test.json')\n",
        "\n",
        "train_dataloader, eval_dataloader,test_dataloader, noise_index, tokenized_datasets, collate_fn = dataloader_outputs\n",
        "\n",
        "## LoRA Model\n",
        "# lora_engine = LORAEngine(model_name_or_path=model_name_or_path,\n",
        "\n",
        "lora_engine = LORAEngineMultiClass(model_name_or_path=model_name_or_path,\n",
        "                            target_modules=target_modules,\n",
        "                            train_dataloader=train_dataloader,\n",
        "                            eval_dataloader=test_dataloader,\n",
        "                            device=device,\n",
        "                            num_epochs=num_epochs,\n",
        "                            lr=lr,\n",
        "                            lora=True,\n",
        "                            low_rank=rank,\n",
        "                            task=task)\n",
        "\n",
        "\n",
        "lora_engine.build_LORA_model()\n",
        "lora_engine.train_LORA_model()\n",
        "\n",
        "## get the train_grad and val_grad from fine-tuned model\n",
        "tr_grad_dict, val_grad_dict = lora_engine.compute_gradient(tokenized_datasets, collate_fn)\n",
        "## to cuda\n",
        "for key in tr_grad_dict:\n",
        "    for kk in tr_grad_dict[key]:\n",
        "        tr_grad_dict[key][kk] = tr_grad_dict[key][kk].to(device)\n",
        "\n",
        "for key in val_grad_dict:\n",
        "    for kk in val_grad_dict[key]:\n",
        "        val_grad_dict[key][kk] = val_grad_dict[key][kk].to(device)\n",
        "\n",
        "\n",
        "## compute influence function\n",
        "influence_engine = IFEngine()\n",
        "influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict, noise_index)\n",
        "\n",
        "influence_engine.compute_hvps(compute_accurate=False)\n",
        "influence_engine.compute_IF()\n",
        "\n",
        "n_train=influence_engine.n_train\n",
        "true_label=np.zeros(n_train)\n",
        "true_label[noise_index]=1\n",
        "\n",
        "method_dict={\n",
        "    'identity': 'TracIN',\n",
        "            'proposed': 'DataInf',\n",
        "            'iterative': 'HyperINF',\n",
        "            'LiSSA': 'LiSSA'\n",
        "            }\n",
        "\n",
        "## plot the detection-ratio\n",
        "# plt.figure(figsize=(5,4))\n",
        "\n",
        "# for method in influence_engine.IF_dict:\n",
        "#     detection_rate_list=[]\n",
        "#     low_quality_to_high_quality=np.argsort(influence_engine.IF_dict[method])[::-1]\n",
        "#     for ind in range(1, len(low_quality_to_high_quality)+1):\n",
        "#         # detected_samples: the samples that are detected as noise\n",
        "#         detected_samples = set(low_quality_to_high_quality[:ind]).intersection(noise_index)\n",
        "#         detection_rate = 100*len(detected_samples)/len(noise_index)\n",
        "#         detection_rate_list.append(detection_rate)\n",
        "\n",
        "\n",
        "#     plt.plot(100*np.arange(len(low_quality_to_high_quality))/n_train,\n",
        "#             detection_rate_list,\n",
        "#             #marker='s',\n",
        "#             label=method_dict[method])\n",
        "\n",
        "\n",
        "# ## plot random detection rate from (0,0) to (100,100)\n",
        "# plt.plot([0, 100], [0, 100], linestyle='--', color='black', label='Random')\n",
        "# ## plot perfect detection rate from (0,0) to (20,100), (20,100) to (100,100)\n",
        "# plt.plot([0, 20], [0, 100], linestyle='--', color='red', label='Perfect')\n",
        "# plt.plot([20, 100], [100, 100], linestyle='--', color='red')\n",
        "# plt.legend(fontsize=20)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5237/5237 [06:37<00:00, 13.16it/s]\n",
            "100%|██████████| 5237/5237 [06:35<00:00, 13.24it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 31.75 GiB total capacity; 30.72 GiB already allocated; 1.50 MiB free; 30.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m tr_grad_dict:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kk \u001b[38;5;129;01min\u001b[39;00m tr_grad_dict[key]:\n\u001b[0;32m----> 5\u001b[0m         tr_grad_dict[key][kk] \u001b[38;5;241m=\u001b[39m \u001b[43mtr_grad_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m val_grad_dict:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kk \u001b[38;5;129;01min\u001b[39;00m val_grad_dict[key]:\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 31.75 GiB total capacity; 30.72 GiB already allocated; 1.50 MiB free; 30.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "tr_grad_dict, val_grad_dict = lora_engine.compute_gradient(tokenized_datasets, collate_fn)\n",
        "## to cuda\n",
        "for key in tr_grad_dict:\n",
        "    for kk in tr_grad_dict[key]:\n",
        "        tr_grad_dict[key][kk] = tr_grad_dict[key][kk].to(device)\n",
        "\n",
        "for key in val_grad_dict:\n",
        "    for kk in val_grad_dict[key]:\n",
        "        val_grad_dict[key][kk] = val_grad_dict[key][kk].to(device)\n",
        "\n",
        "\n",
        "## compute influence function\n",
        "influence_engine = IFEngine()\n",
        "influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict, noise_index)\n",
        "\n",
        "influence_engine.compute_hvps(compute_accurate=False)\n",
        "influence_engine.compute_IF()\n",
        "\n",
        "n_train=influence_engine.n_train\n",
        "true_label=np.zeros(n_train)\n",
        "true_label[noise_index]=1\n",
        "\n",
        "method_dict={\n",
        "    'identity': 'TracIN',\n",
        "            'proposed': 'DataInf',\n",
        "            'iterative': 'HyperINF',\n",
        "            'LiSSA': 'LiSSA'\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda:1'\n",
        "# tr_grad_dict_cut = {k: v for k, v in list(tr_grad_dict.items())[:1000]}\n",
        "# val_grad_dict_cut = {k: v for k, v in list(val_grad_dict.items())[:1000]}\n",
        "def check_key(kk):\n",
        "    # 提取数字部分\n",
        "    numbers = [int(num) for num in kk.split('.') if num.isdigit()]\n",
        "    # 判断是否存在大于12的数字\n",
        "    return all(num <= 12 for num in numbers)\n",
        "for key in tr_grad_dict:\n",
        "    for kk in tr_grad_dict[key]:\n",
        "        if check_key(kk):\n",
        "            tr_grad_dict[key][kk] = tr_grad_dict[key][kk].half().to(device)  \n",
        "\n",
        "for key in val_grad_dict:\n",
        "    for kk in val_grad_dict[key]:\n",
        "        if check_key(kk):\n",
        "            val_grad_dict[key][kk] = val_grad_dict[key][kk].half().to(device) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97/97 [00:33<00:00,  2.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for HyperINF:  33.9565486907959\n",
            "Time taken for Hessian-free:  9.059906005859375e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97/97 [00:59<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for Datainf:  59.52232599258423\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97/97 [04:57<00:00,  3.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for LiSSA:  297.79140067100525\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:2!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m influence_engine\u001b[38;5;241m.\u001b[39mpreprocess_gradients(tr_grad_dict, val_grad_dict, noise_index)\n\u001b[1;32m      4\u001b[0m influence_engine\u001b[38;5;241m.\u001b[39mcompute_hvps(compute_accurate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, compute_LiSSA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43minfluence_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_IF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m n_train\u001b[38;5;241m=\u001b[39minfluence_engine\u001b[38;5;241m.\u001b[39mn_train\n\u001b[1;32m      8\u001b[0m true_label\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(n_train)\n",
            "Cell \u001b[0;32mIn[39], line 205\u001b[0m, in \u001b[0;36mIFEngine.compute_IF\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m  if_tmp_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    204\u001b[0m  \u001b[38;5;28;01mfor\u001b[39;00m weight_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_grad_avg_dict:\n\u001b[0;32m--> 205\u001b[0m      if_tmp_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhvp_dict[method_name][weight_name]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_grad_dict[tr_id][weight_name])\n\u001b[1;32m    206\u001b[0m  if_tmp_dict[tr_id]\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mif_tmp_value\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# print(-if_tmp_value)\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:2!"
          ]
        }
      ],
      "source": [
        "influence_engine = IFEngine()\n",
        "influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict, noise_index)\n",
        "\n",
        "influence_engine.compute_hvps(compute_accurate=False, compute_LiSSA=True)\n",
        "influence_engine.compute_IF()\n",
        "\n",
        "n_train=influence_engine.n_train\n",
        "true_label=np.zeros(n_train)\n",
        "true_label[noise_index]=1\n",
        "\n",
        "method_dict={\n",
        "    'identity': 'TracIN',\n",
        "            'proposed': 'DataInf',\n",
        "            'iterative': 'HyperINF',\n",
        "            'LiSSA': 'LiSSA'\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([], dtype=int64)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argsort(influence_engine.IF_dict['iterative'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZBmDpBUs4Ei"
      },
      "source": [
        "## 3. Running Time Comparison\n",
        "- Gaussian Elimination\n",
        "- Fast Faussian Elimination (torch.inverse)\n",
        "- GMRES approximation with Scipy and torch adaptation\n",
        "- the Schulz's method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UVcuPlyI2dlV"
      },
      "outputs": [],
      "source": [
        "### Updated torch linalg\n",
        "### adapted from https://github.com/devzhk/Pytorch-linalg\n",
        "import torch\n",
        "from functools import partial\n",
        "import warnings\n",
        "\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "\n",
        "def _check_nan(vec, msg):\n",
        "    if torch.isnan(vec).any():\n",
        "        raise ValueError(msg)\n",
        "\n",
        "\n",
        "def _safe_normalize(x, threshold=None):\n",
        "    norm = torch.norm(x)\n",
        "    if threshold is None:\n",
        "        threshold = torch.finfo(norm.dtype).eps\n",
        "    normalized_x = x / norm if norm > threshold else torch.zeros_like(x)\n",
        "    return normalized_x, norm\n",
        "\n",
        "\n",
        "def Mvp(A, vec):\n",
        "    return A @ vec\n",
        "\n",
        "\n",
        "def arnoldi(vec,    # Matrix vector product\n",
        "            V,      # List of existing basis\n",
        "            H,      # H matrix\n",
        "            j):     # number of basis\n",
        "    '''\n",
        "    Arnoldi iteration to find the j th l2-orthonormal vector\n",
        "    compute the j-1 th column of Hessenberg matrix\n",
        "    '''\n",
        "    _check_nan(vec, 'Matrix vector product is Nan')\n",
        "\n",
        "    for i in range(j):\n",
        "        H[i, j - 1] = torch.dot(vec, V[i])\n",
        "        vec = vec - H[i, j-1] * V[i]\n",
        "    new_v, vnorm = _safe_normalize(vec)\n",
        "    H[j, j - 1] = vnorm\n",
        "    return new_v\n",
        "\n",
        "\n",
        "def cal_rotation(a, b):\n",
        "    '''\n",
        "    Args:\n",
        "        a: element h in position j\n",
        "        b: element h in position j+1\n",
        "    Returns:\n",
        "        cosine = a / \\sqrt{a^2 + b^2}\n",
        "        sine = - b / \\sqrt{a^2 + b^2}\n",
        "    '''\n",
        "    c = torch.sqrt(a * a + b * b)\n",
        "    return a / c, - b / c\n",
        "\n",
        "\n",
        "def apply_given_rotation(H, cs, ss, j):\n",
        "    '''\n",
        "    Apply givens rotation to H columns\n",
        "    :param H:\n",
        "    :param cs:\n",
        "    :param ss:\n",
        "    :param j:\n",
        "    :return:\n",
        "    '''\n",
        "    # apply previous rotation to the 0->j-1 columns\n",
        "    for i in range(j):\n",
        "        tmp = cs[i] * H[i, j] - ss[i] * H[i + 1, j]\n",
        "        H[i + 1, j] = cs[i] * H[i+1, j] + ss[i] * H[i, j]\n",
        "        H[i, j] = tmp\n",
        "    cs[j], ss[j] = cal_rotation(H[j, j], H[j + 1, j])\n",
        "    H[j, j] = cs[j] * H[j, j] - ss[j] * H[j + 1, j]\n",
        "    H[j + 1, j] = 0\n",
        "    return H, cs, ss\n",
        "\n",
        "\n",
        "'''\n",
        "    GMRES solver for solving Ax=b.\n",
        "    Reference: https://web.stanford.edu/class/cme324/saad-schultz.pdf\n",
        "'''\n",
        "\n",
        "def GMRES(A,                # Linear operator, matrix or function\n",
        "          b,                # RHS of the linear system in which the first half has the same shape as grad_gx, the second half has the same shape as grad_fy\n",
        "          x0=None,          # initial guess, tuple has the same shape as b\n",
        "          max_iter=None,    # maximum number of GMRES iterations\n",
        "          tol=1e-6,         # relative tolerance\n",
        "          atol=1e-6,        # absolute tolerance\n",
        "          track=False):     # If True, track the residual error of each iteration\n",
        "    '''\n",
        "    Return:\n",
        "        sol: solution\n",
        "        (j, err_history):\n",
        "            j is the number of iterations used to achieve the target accuracy;\n",
        "            err_history is a list of relative residual error at each iteration if track=True, empty list otherwise.\n",
        "    '''\n",
        "    if isinstance(A, torch.Tensor):\n",
        "        Avp = partial(Mvp, A)\n",
        "    elif hasattr(A, '__call__'):\n",
        "        Avp = A\n",
        "    else:\n",
        "        raise ValueError('A must be a function or matrix')\n",
        "\n",
        "    bnorm = torch.norm(b)\n",
        "\n",
        "    if max_iter == 0 or bnorm < 1e-8:\n",
        "        return b\n",
        "\n",
        "    if max_iter is None:\n",
        "        max_iter = b.shape[0]\n",
        "\n",
        "    if x0 is None:\n",
        "        x0 = torch.zeros_like(b)\n",
        "        r0 = b\n",
        "    else:\n",
        "        r0 = b - Avp(x0)\n",
        "\n",
        "    new_v, rnorm = _safe_normalize(r0)\n",
        "    # initial guess residual\n",
        "    beta = torch.zeros(max_iter + 1, device=b.device)\n",
        "    beta[0] = rnorm\n",
        "    err_history = []\n",
        "    if track:\n",
        "        err_history.append((rnorm / bnorm).item())\n",
        "\n",
        "    V = []\n",
        "    V.append(new_v)\n",
        "    H = torch.zeros((max_iter + 1, max_iter + 1), device=b.device)\n",
        "    cs = torch.zeros(max_iter, device=b.device)  # cosine values at each step\n",
        "    ss = torch.zeros(max_iter, device=b.device)  # sine values at each step\n",
        "\n",
        "    for j in range(max_iter):\n",
        "        p = Avp(V[j])\n",
        "        new_v = arnoldi(p, V, H, j + 1)  # Arnoldi iteration to get the j+1 th basis\n",
        "        V.append(new_v)\n",
        "\n",
        "        H, cs, ss = apply_given_rotation(H, cs, ss, j)\n",
        "        beta[j + 1] = ss[j] * beta[j]\n",
        "        beta[j] = cs[j] * beta[j]\n",
        "        residual = torch.abs(beta[j + 1])\n",
        "        if track:\n",
        "            err_history.append((residual / bnorm).item())\n",
        "        if residual < tol * bnorm or residual < atol:\n",
        "            break\n",
        "    y, _ = torch.triangular_solve(beta[0:j + 1].unsqueeze(-1), H[0:j + 1, 0:j + 1])  # j x j\n",
        "    V = torch.stack(V[:-1], dim=0)\n",
        "    sol = x0 + V.T @ y.squeeze(-1)\n",
        "    return sol, (j, err_history)\n",
        "\n",
        "\n",
        "'''\n",
        "  Conjugate Gradient algorithm for solving Ax=b.\n",
        "  Reference: https://en.wikipedia.org/wiki/Conjugate_gradient_method\n",
        "'''\n",
        "\n",
        "def CG(A,                   # linear operator\n",
        "       b,                   # RHS of the linear system\n",
        "       x0=None,             # initial guess\n",
        "       max_iter=None,       # maximum number of iterations\n",
        "       tol=1e-5,            # relative tolerance\n",
        "       atol=1e-6,           # absolute tolerance\n",
        "       track=False,         # if True, track the residual error of each iteration\n",
        "       ):\n",
        "    '''\n",
        "    Return:\n",
        "        sol: solution\n",
        "        (j, err_history):\n",
        "            j is the number of iterations used to achieve the target accuracy;\n",
        "            err_history is a list of relative residual error at each iteration if track=True, empty list otherwise.\n",
        "    '''\n",
        "    if isinstance(A, torch.Tensor):\n",
        "        Avp = partial(Mvp, A)\n",
        "    elif hasattr(A, '__call__'):\n",
        "        Avp = A\n",
        "    else:\n",
        "        raise ValueError('A must be a function or squared matrix')\n",
        "\n",
        "    if max_iter is None:\n",
        "        max_iter = b.shape[0]\n",
        "    if x0 is None:\n",
        "        x = torch.zeros_like(b)\n",
        "        r = b.detach().clone()\n",
        "    else:\n",
        "        Av = Avp(x0)\n",
        "        r = b.detach().clone() - Av\n",
        "        x = x0\n",
        "\n",
        "    p = r.clone()\n",
        "    rdotr = torch.dot(r, r)\n",
        "    err_history = []\n",
        "    if track:\n",
        "        err_history.append(rdotr.item())\n",
        "\n",
        "    residual_tol = max(tol * tol * torch.dot(b, b), atol * atol)\n",
        "    if rdotr < residual_tol:\n",
        "        return x, 0\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        Ap = Avp(p)\n",
        "\n",
        "        alpha = rdotr / torch.dot(p, Ap)\n",
        "        x.add_(alpha * p)\n",
        "        r.add_(-alpha * Ap)\n",
        "        new_rdotr = torch.dot(r, r)\n",
        "        beta = new_rdotr / rdotr\n",
        "        p = r + beta * p\n",
        "        rdotr = new_rdotr\n",
        "        if track:\n",
        "            err_history.append(rdotr.item())\n",
        "        if rdotr < residual_tol:\n",
        "            break\n",
        "    return x, (i + 1, err_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Xf27cF6XoZ",
        "outputId": "3e0a3843-05a3-48bf-9b08-f75db5fb01fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inverse of A:\n",
            "torch.Size([3, 3])\n",
            "Product of A and A_inv (should be close to identity):\n",
            "tensor([[ 1.0000e+00,  0.0000e+00, -7.4506e-09],\n",
            "        [-2.2352e-08,  1.0000e+00, -2.9802e-08],\n",
            "        [-7.4506e-09,  0.0000e+00,  1.0000e+00]], device='cuda:0')\n",
            "tensor([[1.0000e+00, 0.0000e+00, 7.4506e-09],\n",
            "        [1.1176e-08, 1.0000e+00, 0.0000e+00],\n",
            "        [7.4506e-09, 0.0000e+00, 1.0000e+00]], device='cuda:0')\n",
            "tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 9.3132e-09,  1.0000e+00, -2.9802e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse.linalg import gmres\n",
        "\n",
        "def scipy_gmres_inverse(A, tol=1e-6, maxiter=None):\n",
        "    \"\"\"\n",
        "    Compute the inverse of a matrix using the GMRES method.\n",
        "\n",
        "    Parameters:\n",
        "    A : numpy.ndarray\n",
        "        A square matrix to invert.\n",
        "    tol : float\n",
        "        Tolerance for convergence (default 1e-8).\n",
        "    maxiter : int or None\n",
        "        Maximum number of iterations (default None, i.e., no limit).\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray\n",
        "        The inverse of the matrix A.\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    A_inv = np.zeros_like(A, dtype=np.float64)\n",
        "    identity = np.eye(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        b = identity[:, i]\n",
        "        x, _ = gmres(A, b, atol=tol, maxiter=maxiter)\n",
        "        A_inv[:, i] = x\n",
        "    return A_inv\n",
        "\n",
        "def pytorch_gmres_inverse(A, tol=1e-6, maxiter=None):\n",
        "    \"\"\"\n",
        "    Compute the inverse of a matrix using the GMRES method (pytorch).\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    A_inv = torch.zeros_like(A).to(A.device)\n",
        "    identity = torch.eye(n).to(A.device)\n",
        "\n",
        "    for i in range(n):\n",
        "        b = identity[:, i]\n",
        "        x, _ = GMRES(A, b, atol=tol, max_iter=maxiter)\n",
        "        A_inv[:, i] = x\n",
        "\n",
        "    return A_inv\n",
        "\n",
        "def conjugate_gradient_inverse(A, tol=1e-6, maxiter=None):\n",
        "    \"\"\"\n",
        "    Compute the inverse of a matrix using the gradient conjugate method.\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    A_inv = torch.zeros_like(A).to(A.device)\n",
        "    identity = torch.eye(n).to(A.device)\n",
        "\n",
        "    for i in range(n):\n",
        "        b = identity[:, i]\n",
        "        x, _ = CG(A, b, atol=tol, max_iter=maxiter)\n",
        "        A_inv[:, i] = x\n",
        "\n",
        "    return A_inv\n",
        "\n",
        "\n",
        "def gaussian_elimination_inverse(A):\n",
        "    \"\"\"\n",
        "    Compute the inverse of a matrix using Gaussian elimination.\n",
        "\n",
        "    Parameters:\n",
        "    A : torch.Tensor\n",
        "        A square matrix to invert.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor\n",
        "        The inverse of the matrix A, if it exists. Otherwise, raises an error.\n",
        "    \"\"\"\n",
        "    # Ensure A is a float tensor for precision and avoid integer division issues\n",
        "    A = A.float()\n",
        "\n",
        "    n = A.shape[0]\n",
        "    assert A.shape[1] == n, \"Matrix must be square\"\n",
        "\n",
        "    # Create an augmented matrix with the identity matrix on the right\n",
        "    augmented = torch.cat((A, torch.eye(n).to(A.device)), dim=1).to(A.device)\n",
        "\n",
        "    # Gaussian elimination\n",
        "    for i in range(n):\n",
        "        # Find the pivot element and swap rows if necessary\n",
        "        max_row = torch.argmax(torch.abs(augmented[i:, i])) + i\n",
        "        if augmented[max_row, i] == 0:\n",
        "            raise ValueError(\"Matrix is singular and cannot be inverted.\")\n",
        "\n",
        "        if max_row != i:\n",
        "            # Swap rows\n",
        "            augmented[[i, max_row]] = augmented[[max_row, i]]\n",
        "\n",
        "        # Normalize the pivot row\n",
        "        pivot = augmented[i, i]\n",
        "        augmented[i] = augmented[i] / pivot\n",
        "\n",
        "        # Eliminate all other entries in the current column\n",
        "        for j in range(n):\n",
        "            if j != i:\n",
        "                factor = augmented[j, i]\n",
        "                augmented[j] -= factor * augmented[i]\n",
        "\n",
        "    # The right half of the augmented matrix is now the inverse of the original matrix\n",
        "    A_inv = augmented[:, n:]\n",
        "    return A_inv\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # A = np.array([[4, 1, 0], [1, 4, 1], [0, 1, 4]])\n",
        "    A = torch.tensor([[4, 1, 0], [1, 4, 1], [0, 1, 4]], dtype=torch.float32).to('cuda')\n",
        "    A_inv_gmres = pytorch_gmres_inverse(A)\n",
        "    A_inv_cg = conjugate_gradient_inverse(A)\n",
        "    A_inv_gaussian = gaussian_elimination_inverse(A)\n",
        "    print(\"Inverse of A:\")\n",
        "    print(A_inv_gmres.shape)\n",
        "    print(\"Product of A and A_inv (should be close to identity):\")\n",
        "    print(torch.matmul(A, A_inv_gmres))\n",
        "    print(torch.matmul(A, A_inv_cg))\n",
        "    print(torch.matmul(A, A_inv_gaussian))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LEmQW63s4Ej",
        "outputId": "915da62e-7ef8-40b2-9839-fddc1a625949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE:  cuda\n",
            "Dimension:  16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gaussian Elimination Time:  0.08016562461853027\n",
            "Conjugate Gradient Inverse Time:  0.1154015064239502\n",
            "GMRES(scipy) Time:  0.019170761108398438\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.08551239967346191\n",
            "GMRES(torch) Time:  0.41469550132751465\n",
            "Schulz Time:  0.0008151531219482422\n",
            "(torch.inverse) Error:  2.6009487896772045e-11\n",
            "Conjugate Gradient Inverse Error:  1.8356794271312536e-10\n",
            "GMRES (scipy) Error:  1.8275714380897133e-10\n",
            "Schulz Error:  4.8822772669154804e-11\n",
            "GMRES (torch) Error:  1.8345793932894593e-10\n",
            "Gaussian Elimination Time:  0.07729601860046387\n",
            "Conjugate Gradient Inverse Time:  0.07050967216491699\n",
            "GMRES(scipy) Time:  0.0063211917877197266\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.0007026195526123047\n",
            "GMRES(torch) Time:  0.5319318771362305\n",
            "Schulz Time:  0.0007874965667724609\n",
            "(torch.inverse) Error:  3.155670356136397e-11\n",
            "Conjugate Gradient Inverse Error:  4.424395228852518e-10\n",
            "GMRES (scipy) Error:  4.4291992578860617e-10\n",
            "Schulz Error:  3.629238563007675e-11\n",
            "GMRES (torch) Error:  1.0623186881275614e-10\n",
            "Gaussian Elimination Time:  0.0903024673461914\n",
            "Conjugate Gradient Inverse Time:  0.0854032039642334\n",
            "GMRES(scipy) Time:  0.0067102909088134766\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.002401113510131836\n",
            "GMRES(torch) Time:  0.4895005226135254\n",
            "Schulz Time:  0.000820159912109375\n",
            "(torch.inverse) Error:  3.320723180877394e-11\n",
            "Conjugate Gradient Inverse Error:  4.256779448041925e-10\n",
            "GMRES (scipy) Error:  4.259356320717859e-10\n",
            "Schulz Error:  4.384397698231624e-11\n",
            "GMRES (torch) Error:  8.656223258185491e-11\n",
            "====== Summary on Dim 16 ======\n",
            "-->> Time Cost (s)\n",
            "Gaussian Elimination:  0.08258803685506184 ± 0.005579306290605359\n",
            "Conjugate Gradient:  0.0904381275177002 ± 0.018669619477948086\n",
            "GMRES (scipy):  0.010734081268310547 ± 0.005967748016050837\n",
            "(torch.inverse):  0.029538710912068684 ± 0.03958544846783072\n",
            "GMRES (torch):  0.47870930035909015 ± 0.04846600053301335\n",
            "Schulz Time:  0.0008076032002766927 ± 1.4363716327017086e-05\n",
            "\n",
            "-->> Speedup Ratio\n",
            "ConjugateGradient/ge = 109.50511837012773%\n",
            "GMRES(scipy)/ge = 12.99713817771012%\n",
            "GMRES(torch)/ge = 579.6351609606638%\n",
            "Schulz/ge = 0.9778694724200779%\n",
            "\n",
            "ConjugateGradient/ge = 306.1681594261777%\n",
            "GMRES(scipy)/ge = 36.33903084079713%\n",
            "GMRES(torch)/fge = 1620.6167621333232%\n",
            "Schulz/fge = 2.734050252500115%\n",
            "\n",
            "-->> Error\n",
            "ConjugateGradient:  3.505618034675232e-10 ± 1.18280597936112e-10\n",
            "FGE (torch.inverse):  3.025780775563666e-11 ± 3.078660649404988e-12\n",
            "GMRES (scipy):  3.5053756722312107e-10 ± 1.188411248096616e-10\n",
            "GMRES (torch):  1.2541734690785233e-10 ± 4.181910587126118e-11\n",
            "Schulz:  4.29863784271826e-11 ± 5.151327097528642e-12\n",
            "=============================\n",
            "Dimension:  64\n",
            "Gaussian Elimination Time:  0.3422870635986328\n",
            "Conjugate Gradient Inverse Time:  0.5171163082122803\n",
            "GMRES(scipy) Time:  0.026914119720458984\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.017986536026000977\n",
            "GMRES(torch) Time:  2.422952175140381\n",
            "Schulz Time:  0.0009036064147949219\n",
            "(torch.inverse) Error:  9.586093483449076e-11\n",
            "Conjugate Gradient Inverse Error:  1.030901421472663e-09\n",
            "GMRES (scipy) Error:  1.0376824152652193e-09\n",
            "Schulz Error:  1.490022782490996e-10\n",
            "GMRES (torch) Error:  1.5593610669384362e-10\n",
            "Gaussian Elimination Time:  0.34894680976867676\n",
            "Conjugate Gradient Inverse Time:  0.5146758556365967\n",
            "GMRES(scipy) Time:  0.020240306854248047\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.0021817684173583984\n",
            "GMRES(torch) Time:  2.3552393913269043\n",
            "Schulz Time:  0.0008451938629150391\n",
            "(torch.inverse) Error:  8.9342296405448e-11\n",
            "Conjugate Gradient Inverse Error:  9.061767741513904e-10\n",
            "GMRES (scipy) Error:  9.125003075163098e-10\n",
            "Schulz Error:  1.3954575024399674e-10\n",
            "GMRES (torch) Error:  1.8337834717385703e-10\n",
            "Gaussian Elimination Time:  0.2985687255859375\n",
            "Conjugate Gradient Inverse Time:  0.3967475891113281\n",
            "GMRES(scipy) Time:  0.018607616424560547\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.002094268798828125\n",
            "GMRES(torch) Time:  2.308192253112793\n",
            "Schulz Time:  0.0008294582366943359\n",
            "(torch.inverse) Error:  7.529787922067043e-11\n",
            "Conjugate Gradient Inverse Error:  9.859232704911847e-10\n",
            "GMRES (scipy) Error:  9.915842432561515e-10\n",
            "Schulz Error:  1.4227189240045845e-10\n",
            "GMRES (torch) Error:  1.4198769804352195e-10\n",
            "====== Summary on Dim 64 ======\n",
            "-->> Time Cost (s)\n",
            "Gaussian Elimination:  0.32993419965108234 ± 0.022344764894128403\n",
            "Conjugate Gradient:  0.4761799176534017 ± 0.0561759738999125\n",
            "GMRES (scipy):  0.02192068099975586 ± 0.0035932568516469135\n",
            "(torch.inverse):  0.007420857747395833 ± 0.007471148156343493\n",
            "GMRES (torch):  2.362127939860026 ± 0.04710307139040506\n",
            "Schulz Time:  0.0008594195048014323 ± 3.1898429890502785e-05\n",
            "\n",
            "-->> Speedup Ratio\n",
            "ConjugateGradient/ge = 144.3257225704336%\n",
            "GMRES(scipy)/ge = 6.643955377447319%\n",
            "GMRES(torch)/ge = 715.9391000866427%\n",
            "Schulz/ge = 0.2604820917959703%\n",
            "\n",
            "ConjugateGradient/ge = 6416.777330363262%\n",
            "GMRES(scipy)/ge = 295.3928204249486%\n",
            "GMRES(torch)/fge = 31830.928718300205%\n",
            "Schulz/fge = 11.581134338588074%\n",
            "\n",
            "-->> Error\n",
            "ConjugateGradient:  9.743338220384126e-10 ± 5.157386869939438e-11\n",
            "FGE (torch.inverse):  8.683370348686973e-11 ± 8.580193970908531e-12\n",
            "GMRES (scipy):  9.80588988679227e-10 ± 5.169340217425195e-11\n",
            "GMRES (torch):  1.604340506370742e-10 ± 1.7194380548015498e-11\n",
            "Schulz:  1.436066402978516e-10 ± 3.974304504356313e-12\n",
            "=============================\n",
            "Dimension:  256\n",
            "Gaussian Elimination Time:  2.4971277713775635\n",
            "Conjugate Gradient Inverse Time:  2.2207071781158447\n",
            "GMRES(scipy) Time:  0.13718843460083008\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.0017428398132324219\n",
            "GMRES(torch) Time:  12.365940809249878\n",
            "Schulz Time:  0.002067089080810547\n",
            "(torch.inverse) Error:  3.99267901229905e-10\n",
            "Conjugate Gradient Inverse Error:  9.889081411529332e-09\n",
            "GMRES (scipy) Error:  1.0174483114584815e-08\n",
            "Schulz Error:  5.665725893777563e-10\n",
            "GMRES (torch) Error:  9.59816861723084e-10\n",
            "Gaussian Elimination Time:  2.47564435005188\n",
            "Conjugate Gradient Inverse Time:  2.3544979095458984\n",
            "GMRES(scipy) Time:  0.1347675323486328\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.0020520687103271484\n",
            "GMRES(torch) Time:  12.422774076461792\n",
            "Schulz Time:  0.00099945068359375\n",
            "(torch.inverse) Error:  3.792506277022767e-10\n",
            "Conjugate Gradient Inverse Error:  9.52103000599891e-09\n",
            "GMRES (scipy) Error:  9.787688823242395e-09\n",
            "Schulz Error:  5.347169462766033e-10\n",
            "GMRES (torch) Error:  1.0042230314866174e-09\n",
            "Gaussian Elimination Time:  2.5053348541259766\n",
            "Conjugate Gradient Inverse Time:  2.36108136177063\n",
            "GMRES(scipy) Time:  0.13835644721984863\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.0008928775787353516\n",
            "GMRES(torch) Time:  13.21488618850708\n",
            "Schulz Time:  0.0014805793762207031\n",
            "(torch.inverse) Error:  4.0520094444218556e-10\n",
            "Conjugate Gradient Inverse Error:  1.0399504390079528e-08\n",
            "GMRES (scipy) Error:  1.0742903315607968e-08\n",
            "Schulz Error:  5.415009127318627e-10\n",
            "GMRES (torch) Error:  7.149609700718429e-10\n",
            "====== Summary on Dim 256 ======\n",
            "-->> Time Cost (s)\n",
            "Gaussian Elimination:  2.49270232518514 ± 0.012518517977913483\n",
            "Conjugate Gradient:  2.3120954831441245 ± 0.06467715816656984\n",
            "GMRES (scipy):  0.13677080472310385 ± 0.0014946322901828264\n",
            "(torch.inverse):  0.0015625953674316406 ± 0.0004901000340554914\n",
            "GMRES (torch):  12.667867024739584 ± 0.3874962186586804\n",
            "Schulz Time:  0.0015157063802083333 ± 0.0004365687167062195\n",
            "\n",
            "-->> Speedup Ratio\n",
            "ConjugateGradient/ge = 92.75457642028711%\n",
            "GMRES(scipy)/ge = 5.4868486839055475%\n",
            "GMRES(torch)/ge = 508.1981469166683%\n",
            "Schulz/ge = 0.060805751448711684%\n",
            "\n",
            "ConjugateGradient/ge = 147965.0798494558%\n",
            "GMRES(scipy)/ge = 8752.797273929407%\n",
            "GMRES(torch)/fge = 810694.0087478386%\n",
            "Schulz/fge = 96.99928796663615%\n",
            "\n",
            "-->> Error\n",
            "ConjugateGradient:  9.936538602535922e-09 ± 3.6020221136201857e-10\n",
            "FGE (torch.inverse):  3.945731577914557e-10 ± 1.1102107808581373e-11\n",
            "GMRES (scipy):  1.0235025084478393e-08 ± 3.9230742978371004e-10\n",
            "GMRES (torch):  8.930002877605148e-10 ± 1.2719139373545892e-10\n",
            "Schulz:  5.475968161287408e-10 ± 1.3700742753954651e-11\n",
            "=============================\n",
            "Dimension:  1024\n",
            "Gaussian Elimination Time:  24.15657639503479\n",
            "Conjugate Gradient Inverse Time:  14.99242377281189\n",
            "GMRES(scipy) Time:  4.077633380889893\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.007900476455688477\n",
            "GMRES(torch) Time:  76.23052287101746\n",
            "Schulz Time:  0.0014183521270751953\n",
            "(torch.inverse) Error:  2.1159507014090196e-09\n",
            "Conjugate Gradient Inverse Error:  1.2148200039518997e-08\n",
            "GMRES (scipy) Error:  1.731261257100866e-08\n",
            "Schulz Error:  2.554114144004416e-09\n",
            "GMRES (torch) Error:  3.6422723496798426e-09\n",
            "Gaussian Elimination Time:  24.145045042037964\n",
            "Conjugate Gradient Inverse Time:  14.644216537475586\n",
            "GMRES(scipy) Time:  4.653319358825684\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.006780147552490234\n",
            "GMRES(torch) Time:  76.08224844932556\n",
            "Schulz Time:  0.0014379024505615234\n",
            "(torch.inverse) Error:  2.125440551026259e-09\n",
            "Conjugate Gradient Inverse Error:  1.0865159129025415e-08\n",
            "GMRES (scipy) Error:  1.3191511385933006e-08\n",
            "Schulz Error:  2.523297916923184e-09\n",
            "GMRES (torch) Error:  3.7199995858827606e-09\n",
            "Gaussian Elimination Time:  23.8923499584198\n",
            "Conjugate Gradient Inverse Time:  14.725414752960205\n",
            "GMRES(scipy) Time:  4.149023056030273\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.006737470626831055\n",
            "GMRES(torch) Time:  78.18334579467773\n",
            "Schulz Time:  0.0014452934265136719\n",
            "(torch.inverse) Error:  2.1576470317086206e-09\n",
            "Conjugate Gradient Inverse Error:  1.1642897879937663e-08\n",
            "GMRES (scipy) Error:  1.6205221924920392e-08\n",
            "Schulz Error:  2.599393337732181e-09\n",
            "GMRES (torch) Error:  3.723623012774624e-09\n",
            "====== Summary on Dim 1024 ======\n",
            "-->> Time Cost (s)\n",
            "Gaussian Elimination:  24.064657131830852 ± 0.12193048438123522\n",
            "Conjugate Gradient:  14.787351687749227 ± 0.14874857445464287\n",
            "GMRES (scipy):  4.293325265248616 ± 0.2562172671606956\n",
            "(torch.inverse):  0.007139364878336589 ± 0.0005384690983892755\n",
            "GMRES (torch):  76.83203903834026 ± 0.9574336489889749\n",
            "Schulz Time:  0.0014338493347167969 ± 1.1366008237914034e-05\n",
            "\n",
            "-->> Speedup Ratio\n",
            "ConjugateGradient/ge = 61.44842042311782%\n",
            "GMRES(scipy)/ge = 17.840791338638024%\n",
            "GMRES(torch)/ge = 319.2733585084527%\n",
            "Schulz/ge = 0.005958320232288756%\n",
            "\n",
            "ConjugateGradient/ge = 207124.19017298575%\n",
            "GMRES(scipy)/ge = 60135.95520626933%\n",
            "GMRES(torch)/fge = 1076174.706681212%\n",
            "Schulz/fge = 20.083709953914997%\n",
            "\n",
            "-->> Error\n",
            "ConjugateGradient:  1.1552085682827357e-08 ± 5.277206558133137e-10\n",
            "FGE (torch.inverse):  2.1330127613813e-09 ± 1.7844696081452294e-11\n",
            "GMRES (scipy):  1.5569781960620687e-08 ± 1.7413991894603791e-09\n",
            "GMRES (torch):  3.6952983161124093e-09 ± 3.7524189022468466e-11\n",
            "Schulz:  2.558935132886594e-09 ± 3.125230356710953e-11\n",
            "=============================\n",
            "Dimension:  4096\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.06648635864257812\n",
            "GMRES(torch) Time:  617.8460476398468\n",
            "Schulz Time:  0.0625617504119873\n",
            "Schulz Error:  1.7323778592981398e-08\n",
            "GMRES (torch) Error:  1.5161626506596805e-07\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.05222821235656738\n",
            "GMRES(torch) Time:  620.5823481082916\n",
            "Schulz Time:  0.0608372688293457\n",
            "Schulz Error:  1.746252382872626e-08\n",
            "GMRES (torch) Error:  1.5290292212739586e-07\n",
            "Faster Gaussian Elimination Time (torch.inverse):  0.05190896987915039\n",
            "GMRES(torch) Time:  631.206226348877\n",
            "Schulz Time:  0.0012934207916259766\n",
            "Schulz Error:  1.7457941430620848e-08\n",
            "GMRES (torch) Error:  1.509829075075686e-07\n",
            "====== Summary on Dim 4096 ======\n",
            "-->> Time Cost (s)\n",
            "(torch.inverse):  0.05687451362609863 ± 0.006797850268306056\n",
            "GMRES (torch):  623.2115406990051 ± 5.762411766127253\n",
            "Schulz Time:  0.04156414667765299 ± 0.02848440487615241\n",
            "\n",
            "-->> Speedup Ratio\n",
            "GMRES(torch)/fge = 1095765.9256588793%\n",
            "Schulz/fge = 73.08044329117567%\n",
            "\n",
            "-->> Error\n",
            "GMRES (torch):  1.5183403156697748e-07 ± 7.988244370360895e-10\n",
            "Schulz:  1.7414747950776172e-08 ± 6.435224749400197e-11\n",
            "=============================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "from scipy.sparse.linalg import gmres\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"DEVICE: \", device)\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def schulz_inverse_stable(A, damping_factor=0.001, max_iterations=20, tol=1e-6):\n",
        "    n = A.shape[0]\n",
        "    I = torch.eye(n, device=A.device, dtype=A.dtype)\n",
        "    A = A + damping_factor * I\n",
        "    X = torch.eye(n, device=A.device) * 0.00005\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        X = X @ (2 * I - A @ X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "d = [16,64,256,1024,4096]\n",
        "# d = [4096]\n",
        "Ns = 12800\n",
        "damping_factor = 0.1\n",
        "\n",
        "result_dict = {\n",
        "    \"dim\": d,\n",
        "    \"ge_time\": [],\n",
        "    \"cg_time\": [],\n",
        "    \"fge_time\": [],\n",
        "    \"schulz_time\": [],\n",
        "    \"gmres_scipy_time\": [],\n",
        "    \"gmres_torch_time\": [],\n",
        "}\n",
        "\n",
        "for dim in d:\n",
        "    ge_time = []\n",
        "    cg_time = []\n",
        "    fge_time = []\n",
        "    schulz_time = []\n",
        "    gmres_time = []\n",
        "    gmres_torch_time = []\n",
        "\n",
        "    schulz_errors = []\n",
        "    gmres_errors = []\n",
        "    gmres_torch_errors = []\n",
        "    fge_errors = []\n",
        "    cg_errors = []\n",
        "    print(\"Dimension: \", dim)\n",
        "\n",
        "\n",
        "    for seed in range(3):\n",
        "        set_seed(seed=seed)\n",
        "        time_schulz = 0\n",
        "        time_inv = 0\n",
        "\n",
        "        A = torch.zeros((dim, dim)).to(device)\n",
        "        for _ in range(Ns):\n",
        "            tmp = torch.randn((dim, 1)).to(device)\n",
        "            A += tmp @ tmp.T\n",
        "\n",
        "        I = torch.eye(dim, device=A.device)\n",
        "        A = A + damping_factor * I\n",
        "\n",
        "        if dim <= 1024:\n",
        "            st_time = time.time()\n",
        "            ## Exact Inverse by Gaussian Elimination\n",
        "            A_inv_ge = gaussian_elimination_inverse(A)\n",
        "            time_ge = time.time() - st_time\n",
        "            print(\"Gaussian Elimination Time: \", time_ge)\n",
        "            ge_time.append(time_ge)\n",
        "\n",
        "\n",
        "            st_time = time.time()\n",
        "            ## Conjugate Gradient\n",
        "            A_inv_cg = conjugate_gradient_inverse(A, tol=1e-6, maxiter=20)\n",
        "            time_cg = time.time() - st_time\n",
        "            print(\"Conjugate Gradient Inverse Time: \", time_cg)\n",
        "            cg_time.append(time_cg)\n",
        "\n",
        "            st_time = time.time()\n",
        "            ## Compute inverse by GMRES (scipy.linalg.gmres)\n",
        "            A_inv_gmres_scipy = scipy_gmres_inverse(A.cpu().numpy(), tol=1e-6, maxiter=20)\n",
        "            time_gmres = time.time() - st_time\n",
        "            print(\"GMRES(scipy) Time: \", time_gmres)\n",
        "            gmres_time.append(time_gmres)\n",
        "\n",
        "        st_time = time.time()\n",
        "        ## Faster Gaussian Elimination by (torch.inverse)\n",
        "        A_inv = torch.inverse(A)\n",
        "        time_inv = time.time() - st_time\n",
        "        print(\"Faster Gaussian Elimination Time (torch.inverse): \", time_inv)\n",
        "        fge_time.append(time_inv)\n",
        "\n",
        "        st_time = time.time()\n",
        "        ## Compute inverse by GMRES (rewrite in pytorch, enable GPU acceleration)\n",
        "        A_inv_gmres_torch = pytorch_gmres_inverse(A, tol=1e-6, maxiter=20)\n",
        "        time_gmres_torch = time.time() - st_time\n",
        "        print(\"GMRES(torch) Time: \", time_gmres_torch)\n",
        "        gmres_torch_time.append(time_gmres_torch)\n",
        "\n",
        "        st_time = time.time()\n",
        "        ## Compute inverse by Schulz method\n",
        "        A_inv_schulz = schulz_inverse_stable(A, damping_factor=0, max_iterations=20)\n",
        "        time_schulz = time.time() - st_time\n",
        "        print(\"Schulz Time: \", time_schulz)\n",
        "        schulz_time.append(time_schulz)\n",
        "\n",
        "        ## compute the approximation errors by Frobenius distance\n",
        "        if dim <= 1024:\n",
        "            fge_error = torch.norm(A_inv_ge*1e4 - A_inv*1e4, p='fro').item() /1e4 # multiply by 1e4 to prevent underflow\n",
        "            fge_errors.append(fge_error)\n",
        "            print(\"(torch.inverse) Error: \", fge_error)\n",
        "\n",
        "            cg_error = torch.norm(A_inv_ge*1e4 - A_inv_cg*1e4, p='fro').item() /1e4\n",
        "            cg_errors.append(cg_error)\n",
        "            print(\"Conjugate Gradient Inverse Error: \", cg_error)\n",
        "\n",
        "            gmres_scipy_error = np.linalg.norm(A_inv_ge.cpu().numpy()*1e4 - A_inv_gmres_scipy*1e4, ord='fro').item() /1e4\n",
        "            gmres_errors.append(gmres_scipy_error)\n",
        "            print(\"GMRES (scipy) Error: \", gmres_scipy_error)\n",
        "\n",
        "            sc_error = torch.norm(A_inv_ge*1e4 - A_inv_schulz*1e4, p='fro').item() /1e4\n",
        "            schulz_errors.append(sc_error)\n",
        "            print(\"Schulz Error: \", sc_error)\n",
        "            gmres_torch_error = torch.norm(A_inv_ge*1e4 - A_inv_gmres_torch*1e4, p='fro').item() /1e4\n",
        "            gmres_torch_errors.append(gmres_torch_error)\n",
        "            print(\"GMRES (torch) Error: \", gmres_torch_error)\n",
        "        else:\n",
        "            sc_error = torch.norm(A_inv*1e4 - A_inv_schulz*1e4, p='fro').item() /1e4\n",
        "            schulz_errors.append(sc_error)\n",
        "            print(\"Schulz Error: \", sc_error)\n",
        "            gmres_torch_error = torch.norm(A_inv*1e4 - A_inv_gmres_torch*1e4, p='fro').item() /1e4\n",
        "            gmres_torch_errors.append(gmres_torch_error)\n",
        "            print(\"GMRES (torch) Error: \", gmres_torch_error)\n",
        "\n",
        "\n",
        "    ## take the average of the times\n",
        "    print(f\"====== Summary on Dim {dim} ======\")\n",
        "    print(\"-->> Time Cost (s)\")\n",
        "    if dim <= 1024:\n",
        "      result_dict[\"ge_time\"].append(np.mean(ge_time))\n",
        "      result_dict[\"cg_time\"].append(np.mean(cg_time))\n",
        "      result_dict[\"gmres_scipy_time\"].append(np.mean(gmres_time))\n",
        "      print(\"Gaussian Elimination: \", np.mean(ge_time), \"±\", np.std(ge_time))\n",
        "      print(\"Conjugate Gradient: \", np.mean(cg_time), \"±\", np.std(cg_time))\n",
        "      print(\"GMRES (scipy): \", np.mean(gmres_time), \"±\", np.std(gmres_time))\n",
        "\n",
        "    result_dict[\"fge_time\"].append(np.mean(fge_time))\n",
        "    result_dict[\"schulz_time\"].append(np.mean(schulz_time))\n",
        "    result_dict[\"gmres_torch_time\"].append(np.mean(gmres_torch_time))\n",
        "    print(\"(torch.inverse): \", np.mean(fge_time), \"±\", np.std(fge_time))\n",
        "    print(\"GMRES (torch): \", np.mean(gmres_torch_time), \"±\", np.std(gmres_torch_time))\n",
        "    print(\"Schulz Time: \", np.mean(schulz_time), \"±\", np.std(schulz_time))\n",
        "    print()\n",
        "\n",
        "    print(\"-->> Speedup Ratio\")\n",
        "    if dim <= 1024:\n",
        "        print(f\"ConjugateGradient/ge = {100*np.mean(cg_time)/np.mean(ge_time)}%\")\n",
        "        print(f\"GMRES(scipy)/ge = {100*np.mean(gmres_time)/np.mean(ge_time)}%\")\n",
        "        print(f\"GMRES(torch)/ge = {100*np.mean(gmres_torch_time)/np.mean(ge_time)}%\")\n",
        "        print(f\"Schulz/ge = {100*np.mean(schulz_time)/np.mean(ge_time)}%\")\n",
        "        print()\n",
        "        print(f\"ConjugateGradient/ge = {100*np.mean(cg_time)/np.mean(fge_time)}%\")\n",
        "        print(f\"GMRES(scipy)/ge = {100*np.mean(gmres_time)/np.mean(fge_time)}%\")\n",
        "    print(f\"GMRES(torch)/fge = {100*np.mean(gmres_torch_time)/np.mean(fge_time)}%\")\n",
        "    print(f\"Schulz/fge = {100*np.mean(schulz_time)/np.mean(fge_time)}%\")\n",
        "    print()\n",
        "\n",
        "    print(\"-->> Error\")\n",
        "    if dim <= 1024:\n",
        "        print(\"ConjugateGradient: \", np.mean(cg_errors), \"±\", np.std(cg_errors))\n",
        "        print(\"FGE (torch.inverse): \", np.mean(fge_errors), \"±\", np.std(fge_errors))\n",
        "        print(\"GMRES (scipy): \", np.mean(gmres_errors), \"±\", np.std(gmres_errors))\n",
        "    print(\"GMRES (torch): \", np.mean(gmres_torch_errors), \"±\", np.std(gmres_torch_errors))\n",
        "    print(\"Schulz: \", np.mean(schulz_errors), \"±\", np.std(schulz_errors))\n",
        "    print(\"=============================\")\n",
        "\n",
        "\n",
        "## take the average of the times\n",
        "#print(\"Exact Time: \", np.mean(exact_time), \"±\", np.std(exact_time))\n",
        "#print(\"Schulz Time: \", np.mean(schulz_time), \"±\", np.std(schulz_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "LPUASzuYxLYy",
        "outputId": "4ed8e161-4a00-46cc-b367-97aad068eaac"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colormaps\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "i = 0\n",
        "#colors = mpl.colormaps['PuBuGn'](np.linspace(0, 1, 6))\n",
        "colors = [\"#1F77B4\", \"#AEC7E8\", \"#D62728\", \"#E37770\", \"#8C564B\", \"#D8B365\"]\n",
        "\n",
        "shape = ['o-', 's-', 'D-', 'v-', 'H-', '^-']\n",
        "methods = [\"ge_time\", \"cg_time\", \"fge_time\", \"schulz_time\", \"gmres_torch_time\", \"gmres_scipy_time\"]\n",
        "method2label = {\n",
        "    \"ge_time\": \"Gaussian Elimination\",\n",
        "    \"cg_time\": \"Conjugate Gradient\",\n",
        "    \"fge_time\": \"FGE (torch.inverse)\",\n",
        "    \"schulz_time\": \"Schulz\",\n",
        "    \"gmres_scipy_time\": \"GMRES (scipy)\",\n",
        "    \"gmres_torch_time\": \"GMRES (torch)\",\n",
        "}\n",
        "legends = []\n",
        "for k in methods:\n",
        "  if k==\"dim\":\n",
        "    continue\n",
        "  v = result_dict[k]\n",
        "  plt.plot(range(len(v)), v, shape[i], linewidth=2, c=colors[i])\n",
        "  legends.append(method2label[k])\n",
        "  i+=1\n",
        "\n",
        "plt.legend(legends)\n",
        "plt.xticks(ticks=np.arange(len(result_dict[\"dim\"])), labels=result_dict[\"dim\"])\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Matrix Dimension\")\n",
        "plt.ylabel(\"Time (s)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
