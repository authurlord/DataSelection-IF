{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list = {}\n",
    "file_path_list['AVE'] = {}\n",
    "file_path_list['CTA'] = {}\n",
    "file_path_list['DC'] = {}\n",
    "file_path_list['DI'] = {}\n",
    "file_path_list['ER'] = {}\n",
    "file_path_list['RE'] = {}\n",
    "file_path_list['SM'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list['AVE']['oa_mine'] = '/data/home/wangys/MELD/dataset/AVE/oa_mine_train_small.json'\n",
    "file_path_list['CTA']['SimTab'] = '/data/home/wangys/MELD/dataset/CTA/SimTab_Train_few.json'\n",
    "# file_path_list['CTA']['WebTable'] = '/data/home/wangys/MELD/dataset/CTA/webtable_train_few.json' ## selected-file\n",
    "file_path_list['CTA']['WebTable'] = '/data/home/wangys/DataSelection-IF/train/CTA/WebTable/WebTable-train.json'\n",
    "\n",
    "file_path_list['DC']['hospital'] = '/data/home/wangys/MELD/dataset/DC/hospital-train-MoE.json'\n",
    "file_path_list['DC']['rayyan'] = '/data/home/wangys/MELD/dataset/DC/rayyan-train-MoE.json'\n",
    "file_path_list['DC']['beer'] = '/data/home/wangys/MELD/dataset/DC/beer-train-MoE.json'\n",
    "# file_path_list['RE']['RE'] = '/data/home/wangys/MELD/dataset/RE/RE-train.json' ## selected-file\n",
    "file_path_list['RE']['RE'] = '/data/home/wangys/DataSelection-IF/train/RE/RE-train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list['DI']['amazon'] = '/data/home/wangys/MELD/dataset/DI/amazon_train_output_wide.json'\n",
    "file_path_list['DI']['walmart'] = '/data/home/wangys/MELD/dataset/DI/walmart_train_output_wide.json'\n",
    "file_path_list['DI']['restaurant'] = '/data/home/wangys/MELD/dataset/DI/restaurant_train_output_wide.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list['ER']['semi-text-w'] = '/data/home/wangys/MELD/dataset/ER/semi-text-w-train-MoE.json'\n",
    "file_path_list['ER']['semi-text-c'] = '/data/home/wangys/MELD/dataset/ER/semi-text-c-train-MoE.json'\n",
    "file_path_list['ER']['amazon-google'] = '/data/home/wangys/MELD/dataset/ER/amazon-google-train.json'\n",
    "file_path_list['ER']['walmart-amazon'] = '/data/home/wangys/MELD/dataset/ER/walmart_amazon_train_output.json'\n",
    "file_path_list['ER']['wdc'] = '/data/home/wangys/MELD/dataset/ER/wdc_all_train_output.json'\n",
    "file_path_list['ER']['abt-buy'] = '/data/home/wangys/MELD/dataset/ER/ant_buy_train_output.json'\n",
    "file_path_list['SM']['CMS'] = '/data/home/wangys/MELD/dataset/SM/CMS_train_few_output.json'\n",
    "file_path_list['SM']['synthea'] = '/data/home/wangys/MELD/dataset/SM/synthea_train_few_output.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list_test = {}\n",
    "file_path_list_test['AVE'] = {}\n",
    "file_path_list_test['CTA'] = {}\n",
    "file_path_list_test['DC'] = {}\n",
    "file_path_list_test['DI'] = {}\n",
    "file_path_list_test['ER'] = {}\n",
    "file_path_list_test['RE'] = {}\n",
    "file_path_list_test['SM'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list_test['AVE']['oa_mine'] = '/data/home/wangys/MELD/dataset/AVE/oa_mine_test_small.json'\n",
    "file_path_list_test['CTA']['SimTab'] = '/data/home/wangys/MELD/dataset/CTA/SimTab_test_few.json'\n",
    "# file_path_list['CTA']['WebTable'] = '/data/home/wangys/MELD/dataset/CTA/webtable_train_few.json' ## selected-file\n",
    "file_path_list_test['CTA']['WebTable'] = '/data/home/wangys/MELD/dataset/CTA/WebTable_Test_few.json'\n",
    "\n",
    "file_path_list_test['DC']['hospital'] = '/data/home/wangys/MELD/dataset/DC/hospital-test.json'\n",
    "file_path_list_test['DC']['rayyan'] = '/data/home/wangys/MELD/dataset/DC/rayyan-test-20.json'\n",
    "file_path_list_test['DC']['beer'] = '/data/home/wangys/MELD/dataset/DC/beer_test.json'\n",
    "# file_path_list['RE']['RE'] = '/data/home/wangys/MELD/dataset/RE/RE-train.json' ## selected-file\n",
    "file_path_list_test['RE']['RE'] = '/data/home/wangys/DataSelection-IF/train/RE/RE-train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list_test['DI']['amazon'] = '/data/home/wangys/MELD/dataset/DI/amazon_test_output_wide.json'\n",
    "file_path_list_test['DI']['walmart'] = '/data/home/wangys/MELD/dataset/DI/walmart_test_output_wide.json'\n",
    "file_path_list_test['DI']['restaurant'] = '/data/home/wangys/MELD/dataset/DI/restaurant_test_output_wide.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list_test['ER']['semi-text-w'] = '/data/home/wangys/MELD/dataset/ER/semi-text-w-test-MoE.json'\n",
    "file_path_list_test['ER']['semi-text-c'] = '/data/home/wangys/MELD/dataset/ER/semi-text-c-test-MoE.json'\n",
    "file_path_list_test['ER']['amazon-google'] = '/data/home/wangys/MELD/dataset/ER/amazon-google-test.json'\n",
    "file_path_list_test['ER']['walmart-amazon'] = '/data/home/wangys/MELD/dataset/ER/walmart_amazon_test_output.json'\n",
    "file_path_list_test['ER']['wdc'] = '/data/home/wangys/MELD/dataset/ER/wdc_all_test_output.json'\n",
    "file_path_list_test['ER']['abt-buy'] = '/data/home/wangys/MELD/dataset/ER/ant_buy_test_output.json'\n",
    "file_path_list_test['SM']['CMS'] = '/data/home/wangys/MELD/dataset/SM/CMS_test_few_output.json'\n",
    "file_path_list_test['SM']['synthea'] = '/data/home/wangys/MELD/dataset/SM/synthea_test_few_output.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for task in tqdm(file_path_list):\n",
    "    for dataset in file_path_list[task].keys():\n",
    "        raw_file = pd.read_json(file_path_list[task][dataset])\n",
    "        # raw_file\n",
    "        ppl_array = np.zeros(len(raw_file))\n",
    "        for process_num in range(1,9,1): ## maximum of k process\n",
    "            if os.path.exists('../ppl/{}/{}/ppl-init-{}.csv'.format(task,dataset,process_num)): ## i-th gradient \n",
    "                ppl_df = pd.read_csv('../ppl/{}/{}/ppl-init-{}.csv'.format(task,dataset,process_num),index_col=0)\n",
    "                for index,row in ppl_df.iterrows():\n",
    "                    ppl_array[index] = row[0]\n",
    "        if np.sum(ppl_array)>10: ## 存在Ppl结果\n",
    "            top_k = np.argsort(ppl_array)[::-1][:200]\n",
    "            df_select = raw_file.iloc[top_k].reset_index(drop=True)\n",
    "        else:\n",
    "            print('ppl do not exist for {}/{}'.format(task,dataset))\n",
    "            df_select = raw_file.sample(n=200,random_state=42).reset_index(drop=True)\n",
    "        df_select = df_select.iloc[:,:3]\n",
    "        df_select.columns = ['instruction','input','output']\n",
    "        df_select['task'] = task\n",
    "        df_select['dataset'] = dataset\n",
    "        df_all = pd.concat([df_all,df_select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6e4be01bbb4cd499dfaa7bc0746ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## For Test\n",
    "df_all = pd.DataFrame()\n",
    "for task in tqdm(file_path_list_test):\n",
    "    for dataset in file_path_list_test[task].keys():\n",
    "        raw_file = pd.read_json(file_path_list_test[task][dataset])\n",
    "        # print(raw_file.columns)\n",
    "        if 'instruction' not in raw_file.columns:\n",
    "            raw_file.columns = ['instruction', 'input', 'output'] ## for [0,1,2]\n",
    "        if 'index' not in raw_file.columns:\n",
    "            raw_file['index'] = raw_file.index\n",
    "        # raw_file\n",
    "            # print('ppl do not exist for {}/{}'.format(task,dataset))\n",
    "            # df_select = raw_file.sample(n=200,random_state=42).reset_index(drop=True)\n",
    "            \n",
    "        df_select = raw_file\n",
    "        # df_select.columns = ['instruction','input','output']\n",
    "        df_select['task'] = task\n",
    "        df_select['dataset'] = dataset\n",
    "        df_all = pd.concat([df_all,df_select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>index</th>\n",
       "      <th>task</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a world-class expert for extracting in...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Brand\": \"Shout\"}</td>\n",
       "      <td>0</td>\n",
       "      <td>AVE</td>\n",
       "      <td>oa_mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a world-class expert for extracting in...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Specific uses\": \"Stain Remover\"}</td>\n",
       "      <td>1</td>\n",
       "      <td>AVE</td>\n",
       "      <td>oa_mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a world-class expert for extracting in...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Item form\": \"Liquid Refill\"}</td>\n",
       "      <td>2</td>\n",
       "      <td>AVE</td>\n",
       "      <td>oa_mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are a world-class expert for extracting in...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Net content\": \"60 oz\"}</td>\n",
       "      <td>3</td>\n",
       "      <td>AVE</td>\n",
       "      <td>oa_mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a world-class expert for extracting in...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Pack size\": \"2 pk\"}</td>\n",
       "      <td>4</td>\n",
       "      <td>AVE</td>\n",
       "      <td>oa_mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>5923</td>\n",
       "      <td>SM</td>\n",
       "      <td>synthea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>5924</td>\n",
       "      <td>SM</td>\n",
       "      <td>synthea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>5925</td>\n",
       "      <td>SM</td>\n",
       "      <td>synthea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>5926</td>\n",
       "      <td>SM</td>\n",
       "      <td>synthea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>You are an expert in detecting if two columns ...</td>\n",
       "      <td></td>\n",
       "      <td>{'Output': 'dismatch'}</td>\n",
       "      <td>5927</td>\n",
       "      <td>SM</td>\n",
       "      <td>synthea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68097 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction input  \\\n",
       "0     You are a world-class expert for extracting in...         \n",
       "1     You are a world-class expert for extracting in...         \n",
       "2     You are a world-class expert for extracting in...         \n",
       "3     You are a world-class expert for extracting in...         \n",
       "4     You are a world-class expert for extracting in...         \n",
       "...                                                 ...   ...   \n",
       "5923  You are an expert in detecting if two columns ...         \n",
       "5924  You are an expert in detecting if two columns ...         \n",
       "5925  You are an expert in detecting if two columns ...         \n",
       "5926  You are an expert in detecting if two columns ...         \n",
       "5927  You are an expert in detecting if two columns ...         \n",
       "\n",
       "                                  output  index task  dataset  \n",
       "0                     {\"Brand\": \"Shout\"}      0  AVE  oa_mine  \n",
       "1     {\"Specific uses\": \"Stain Remover\"}      1  AVE  oa_mine  \n",
       "2         {\"Item form\": \"Liquid Refill\"}      2  AVE  oa_mine  \n",
       "3               {\"Net content\": \"60 oz\"}      3  AVE  oa_mine  \n",
       "4                  {\"Pack size\": \"2 pk\"}      4  AVE  oa_mine  \n",
       "...                                  ...    ...  ...      ...  \n",
       "5923              {'Output': 'dismatch'}   5923   SM  synthea  \n",
       "5924              {'Output': 'dismatch'}   5924   SM  synthea  \n",
       "5925              {'Output': 'dismatch'}   5925   SM  synthea  \n",
       "5926              {'Output': 'dismatch'}   5926   SM  synthea  \n",
       "5927              {'Output': 'dismatch'}   5927   SM  synthea  \n",
       "\n",
       "[68097 rows x 6 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(drop=True).to_csv('../train/router/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pseudo_Label_Router(row):\n",
    "    expert = '{}/{}'.format(row['task'],row['dataset'])\n",
    "    return [expert]\n",
    "df_all['labels'] = df_all.apply(Pseudo_Label_Router,axis=1)\n",
    "df_all['text'] = df_all['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['text','labels']].to_csv('../train/router/test_guided.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(drop=True).to_csv('../train/router/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021e3245e3974a6f84751ce7799f0799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68097 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Lengths: 307\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "# prompt_opt = pd.read_json('/home/yanmy/LLM_ER/data/dblp-acm/feature_align/dblp-acm-train-dpo.json') ## replace with your training file\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../../roberta-base/') ## replace with any LM\n",
    "\n",
    "\n",
    "token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in tqdm(df_all['output'].to_list())]\n",
    "\n",
    "print(\"Token Lengths:\", max(token_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert List for Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_path_list = {}\n",
    "expert_path_list['AVE'] = {}\n",
    "expert_path_list['CTA'] = {}\n",
    "expert_path_list['DC'] = {}\n",
    "expert_path_list['DI'] = {}\n",
    "expert_path_list['ER'] = {}\n",
    "expert_path_list['RE'] = {}\n",
    "expert_path_list['SM'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_path_list['AVE']['oa_mine'] = '/data/home/wangys/LLaMA-Factory-main/lora_weight/oa_mine/oa_mine_train_small'\n",
    "expert_path_list['AVE']['oa_mine'] = '/data/home/wangys/DataSelection-IF/lora/mistral-7B/AVE/oa_mine/select'\n",
    "\n",
    "expert_path_list['CTA']['SimTab'] = '/data/home/wangys/MELD/lora_weight/expert/CTA_SimTab_train_init'\n",
    "expert_path_list['CTA']['WebTable'] = '/data/home/wangys/MELD/lora_weight/expert/CTA_WebTable_train_init'\n",
    "\n",
    "expert_path_list['DC']['hospital'] = '/data/home/wangys/MELD/lora_weight/expert/hospital_train-MoE-Add'\n",
    "expert_path_list['DC']['rayyan'] = '/data/home/wangys/MELD/lora_weight/expert/rayyan_train-MoE-Add'\n",
    "expert_path_list['DC']['beer'] = '/data/home/wangys/MELD/lora_weight/expert/beer_train-MoE-Add'\n",
    "expert_path_list['RE']['RE'] = '/data/home/wangys/MELD/lora_weight/expert/RE-MoE-Add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_path_list['DI']['amazon'] = '/data/home/wangys/MELD/lora_weight/expert/amazon_train-MoE-Add'\n",
    "expert_path_list['DI']['walmart'] = '/data/home/wangys/MELD/lora_weight/expert/walmart_train-MoE-Add'\n",
    "expert_path_list['DI']['restaurant'] = '/data/home/wangys/MELD/lora_weight/expert/restaurant_train-MoE-Add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_path_list['ER']['semi-text-w'] = '/data/home/wangys/MELD/lora_weight/expert/semi_text_w-MoE-Add'\n",
    "expert_path_list['ER']['semi-text-c'] = '/data/home/wangys/MELD/lora_weight/expert/semi_text_c-MoE-Add'\n",
    "expert_path_list['ER']['amazon-google'] = '/data/home/wangys/MELD/lora_weight/expert/amazon_google-MoE-Add'\n",
    "expert_path_list['ER']['walmart-amazon'] = '/data/home/wangys/MELD/lora_weight/expert/walmart_amazon-MoE-Add'\n",
    "expert_path_list['ER']['wdc'] = '/data/home/wangys/MELD/lora_weight/expert/wdc_all-MoE-Add'\n",
    "expert_path_list['ER']['abt-buy'] = '/data/home/wangys/MELD/lora_weight/expert/ant_buy-MoE-Add'\n",
    "expert_path_list['SM']['CMS'] = '/data/home/wangys/MELD/lora_weight/expert/CMS_train-MoE-Add'\n",
    "expert_path_list['SM']['synthea'] = '/data/home/wangys/MELD/lora_weight/expert/synthea_train-MoE-Add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_count = 1\n",
    "expert_index = {}\n",
    "for task in expert_path_list:\n",
    "    for dataset in expert_path_list[task]:\n",
    "        expert_name = '{}/{}'.format(task,dataset)\n",
    "        expert_index[expert_name] = {}\n",
    "        expert_index[expert_name]['index'] = lora_count\n",
    "        expert_index[expert_name]['lora_path'] = expert_path_list[task][dataset]\n",
    "        lora_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_index_path = '../train/router/expert_index.npy'\n",
    "expert_index = np.load(expert_index_path,allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../train/router/expert_index.npy',expert_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pd.read_csv('../train/router/train_output_guided.csv',index_col=0)\n",
    "result = pd.read_csv('../train/router/train_output.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_example = result.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVE/oa_mine\n",
      "CTA/SimTab\n",
      "CTA/WebTable\n",
      "DC/hospital\n",
      "DC/rayyan\n",
      "DC/beer\n",
      "DI/amazon\n",
      "DI/walmart\n",
      "DI/restaurant\n",
      "ER/semi-text-w\n",
      "ER/semi-text-c\n",
      "ER/amazon-google\n",
      "ER/walmart-amazon\n",
      "ER/wdc\n",
      "ER/abt-buy\n",
      "RE/RE\n",
      "SM/CMS\n",
      "SM/synthea\n"
     ]
    }
   ],
   "source": [
    "# eval(row_example[10].strip())\n",
    "for x,y in row_example[5:].items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\}'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:1: SyntaxWarning: invalid escape sequence '\\_'\n"
     ]
    }
   ],
   "source": [
    "def AST(row):\n",
    "    output = row['output'].strip()\n",
    "    \n",
    "    output_item = list(eval(output).values())[0]\n",
    "    \n",
    "    positive = []\n",
    "    # try:\n",
    "    #     predict = row['predict'].strip()\n",
    "    #     predict_item = list(eval(predict).values())[0]\n",
    "    # except:\n",
    "    #     predict_item = ''\n",
    "    # row['output_item'] = str(output_item).lower().strip()\n",
    "    # row['predict_item'] = str(predict_item).lower().strip()\n",
    "    # return row\n",
    "    ground_truth = []\n",
    "    for x,y in row[5:].items():\n",
    "        try:\n",
    "            predict = row[x].strip()\n",
    "            predict_item = list(eval(predict).values())[0]\n",
    "        except:\n",
    "            predict_item = ''\n",
    "        output_item_final = str(output_item).lower().strip()\n",
    "        predict_item_final = str(predict_item).lower().strip()\n",
    "        if(output_item_final==predict_item_final):\n",
    "            ground_truth.append(x)\n",
    "        # else:\n",
    "        #     print(output_item_final,predict_item_final,x)\n",
    "    return ground_truth\n",
    "# AST(result.iloc[422])\n",
    "result['labels'] = result.apply(AST,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DC/hospital', 'DC/rayyan', 'DC/beer']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[['instruction','labels']].iloc[663,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_output = result[['instruction','labels']]\n",
    "result_output.columns = ['text', 'labels']\n",
    "result_output = result_output.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_output.iloc[:3300].to_csv('../train/router/train_guided.csv')\n",
    "result_output.iloc[3300:].to_csv('../train/router/valid_guided.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2094896/900560314.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  logits = torch.load('../train/router/logits.pkl')\n",
      "/tmp/ipykernel_2094896/900560314.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  id2label = torch.load('../train/router/id2label.pkl')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "logits = torch.load('../train/router/logits.pkl')\n",
    "id2label = torch.load('../train/router/id2label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (torch.sigmoid(torch.tensor(logits)) > 0.5).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_dispatch_result_all = np.argsort(-logits)\n",
    "for index in range(labels.shape[0]):\n",
    "    router_dispatch_result = router_dispatch_result_all[index]\n",
    "    router_dispatch_result = np.sort(router_dispatch_result[:2])\n",
    "    top_k_expert = [id2label[router_dispatch_result[0]],id2label[router_dispatch_result[1]]]\n",
    "    df_all.iloc[index,6] = str(top_k_expert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(drop=True).to_csv('../train/router/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CPU OffLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fc04da435a4152aaf7373dc355ddda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 加载基础模型\n",
    "from peft import PeftModel\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"/data/home/wangys/model/Mistral-7B-Instruct-v0.2\",device_map='cpu') ## 专家A,domain expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_path_list = {}\n",
    "expert_path_list['AVE'] = {}\n",
    "expert_path_list['CTA'] = {}\n",
    "expert_path_list['DC'] = {}\n",
    "expert_path_list['DI'] = {}\n",
    "expert_path_list['ER'] = {}\n",
    "expert_path_list['RE'] = {}\n",
    "expert_path_list['SM'] = {}\n",
    "# expert_path_list['AVE']['oa_mine'] = '/data/home/wangys/LLaMA-Factory-main/lora_weight/oa_mine/oa_mine_train_small'\n",
    "expert_path_list['AVE']['oa_mine'] = '/data/home/wangys/DataSelection-IF/lora/mistral-7B/AVE/oa_mine/select'\n",
    "\n",
    "expert_path_list['CTA']['SimTab'] = '/data/home/wangys/MELD/lora_weight/expert/CTA_SimTab_train_init'\n",
    "expert_path_list['CTA']['WebTable'] = '/data/home/wangys/MELD/lora_weight/expert/CTA_WebTable_train_init'\n",
    "\n",
    "expert_path_list['DC']['hospital'] = '/data/home/wangys/MELD/lora_weight/expert/hospital_train-MoE-Add'\n",
    "expert_path_list['DC']['rayyan'] = '/data/home/wangys/MELD/lora_weight/expert/rayyan_train-MoE-Add'\n",
    "expert_path_list['DC']['beer'] = '/data/home/wangys/MELD/lora_weight/expert/beer_train-MoE-Add'\n",
    "expert_path_list['RE']['RE'] = '/data/home/wangys/MELD/lora_weight/expert/RE-MoE-Add'\n",
    "expert_path_list['DI']['amazon'] = '/data/home/wangys/MELD/lora_weight/expert/amazon_train-MoE-Add'\n",
    "expert_path_list['DI']['walmart'] = '/data/home/wangys/MELD/lora_weight/expert/walmart_train-MoE-Add'\n",
    "expert_path_list['DI']['restaurant'] = '/data/home/wangys/MELD/lora_weight/expert/restaurant_train-MoE-Add'\n",
    "expert_path_list['ER']['semi-text-w'] = '/data/home/wangys/MELD/lora_weight/expert/semi_text_w-MoE-Add'\n",
    "expert_path_list['ER']['semi-text-c'] = '/data/home/wangys/MELD/lora_weight/expert/semi_text_c-MoE-Add'\n",
    "expert_path_list['ER']['amazon-google'] = '/data/home/wangys/MELD/lora_weight/expert/amazon_google-MoE-Add'\n",
    "expert_path_list['ER']['walmart-amazon'] = '/data/home/wangys/MELD/lora_weight/expert/walmart_amazon-MoE-Add'\n",
    "expert_path_list['ER']['wdc'] = '/data/home/wangys/MELD/lora_weight/expert/wdc_all-MoE-Add'\n",
    "expert_path_list['ER']['abt-buy'] = '/data/home/wangys/MELD/lora_weight/expert/ant_buy-MoE-Add'\n",
    "expert_path_list['SM']['CMS'] = '/data/home/wangys/MELD/lora_weight/expert/CMS_train-MoE-Add'\n",
    "expert_path_list['SM']['synthea'] = '/data/home/wangys/MELD/lora_weight/expert/synthea_train-MoE-Add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"/data/home/wangys/LLaMA-Factory-main/lora_weight/MoE/add/Mistral/amazon_google-MoE-Add\"\n",
    "model = PeftModel.from_pretrained(base_model, model_id=peft_model_id,adapter_name='amazon-google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in expert_path_list:\n",
    "    for dataset in expert_path_list[task]:\n",
    "        model.load_adapter(expert_path_list[task][dataset],adapter_name='{}--{}'.format(task,dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('../train/router/test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5189416d18eb4018b0f0f8d2612263e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for expert_combination in tqdm(list(df_all['labels'].unique())):\n",
    "    expert_0,expert_1 = eval(expert_combination.replace('/','--'))\n",
    "    # print(expert_0,expert_1)\n",
    "    model.add_weighted_adapter(\n",
    "        adapters=[expert_0,expert_1],\n",
    "        weights=[1,1],\n",
    "        adapter_name='{}|{}'.format(expert_0,expert_1),\n",
    "        combination_type=\"cat\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b0e49ae18849e38eaedfcf876d8c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapter_list = []\n",
    "for expert_combination in tqdm(list(df_all['labels'].unique())):\n",
    "    expert_0,expert_1 = eval(expert_combination.replace('/','--'))\n",
    "    adapter_list.append('{}|{}'.format(expert_0,expert_1))\n",
    "\n",
    "model.save_pretrained(save_directory='/data/home/wangys/Expert_Combination',selected_adapters=adapter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.iloc[:,:-1].to_csv('../train/router/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# 假设三个文件分别为file1.txt, file2.txt, file3.txt\n",
    "files_to_copy = ['/data/home/wangys/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/restaurant-MoE-CT/tokenizer.model', '/data/home/wangys/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/restaurant-MoE-CT/tokenizer_config.json', '/data/home/wangys/LLaMA-Factory-main/lora_weight/MoE_CT/add/Mistral/restaurant-MoE-CT/special_tokens_map.json']\n",
    "folder_A = '/data/home/wangys/Expert_Combination'\n",
    "\n",
    "for root, dirs, files in os.walk(folder_A):\n",
    "    if root != folder_A:  # 排除folder A本身\n",
    "        for file in files_to_copy:\n",
    "            shutil.copy(file, root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
