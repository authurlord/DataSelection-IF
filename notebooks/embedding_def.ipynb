{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "def is_folder_empty(folder_path):\n",
    "    \"\"\"\n",
    "    检查指定的文件夹是否为空文件夹。\n",
    "\n",
    "    参数:\n",
    "    folder_path (str): 要检查的文件夹的路径\n",
    "\n",
    "    返回:\n",
    "    bool: 如果文件夹为空，返回True；否则返回False。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise ValueError(f\"指定的路径 {folder_path} 不存在，请检查输入的文件夹路径是否正确。\")\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"{folder_path} 不是一个文件夹，请确保输入的是文件夹路径。\")\n",
    "\n",
    "    # 获取文件夹下的所有文件和子文件夹列表\n",
    "    contents = os.listdir(folder_path)\n",
    "    return len(contents) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test embed on Relation Extraction Task(RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_train = pd.read_json('../train/RE/RE-train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RE_Embed_Text(df): ## 返回3列分别是text_1,text_2,label\n",
    "    SimTab = df\n",
    "    context_list = []\n",
    "    target_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(SimTab)):\n",
    "        text = SimTab.iloc[i,0]\n",
    "        label = list(eval(SimTab.iloc[i,2]).values())[0]\n",
    "        label_list.append(label) ## add label\n",
    "        context = text.split('\\n\\nTable1:')[1].split('\\n\\nRelation Option: \\n\\n')[0].replace('<table_title>','<table_title> ').replace('<header>',' <header> ').replace('|',' | ')\n",
    "        context_list.append(str(context)) ## add context\n",
    "        target = text.split('<header>')[1].split('\\n\\nRelation Option: \\n\\n')[0]\n",
    "        target_list.append(str(target.replace('|',' | ')))\n",
    "    return context_list,target_list,label_list\n",
    "def CTA_Embed_Text(df): ## 返回3列分别是text_1,text_2,label\n",
    "    SimTab = df\n",
    "    context_list = []\n",
    "    target_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(SimTab)):\n",
    "        text = SimTab.iloc[i,0]\n",
    "        label = list(eval(SimTab.iloc[i,2]).values())[0]\n",
    "        label_list.append(label) ## add label\n",
    "        context = text.split('Table 1:\\n\\n')[1].split('\\n\\nReference tables:')[0].split('\\n')\n",
    "        context_list.append(str(context)) ## add context\n",
    "        \n",
    "        target = text.split('\\n\\nColumn: ')[1].split('\\n\\nOptions:')[0]\n",
    "        related_context = []\n",
    "        for candidate in context:\n",
    "            try:\n",
    "                dict_output = eval(candidate)\n",
    "                related_context.append(dict_output[target])\n",
    "            except:\n",
    "                print('fail')\n",
    "                print(dict_output.keys(),target)\n",
    "        target_list.append(str({target:related_context}))\n",
    "    return context_list,target_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,t,l = RE_Embed_Text(RE_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTA_WebTable = pd.read_json('../train/CTA/WebTable/WebTable-train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c,t,l = RE_Embed_Text(RE_train)\n",
    "c,t,l = CTA_Embed_Text(CTA_WebTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'init_select': 143.5319640636444, 'init-train': 143.53495621681213, 'batch-division': 165.7140564918518, 'gradient-calculation': 263.05509662628174, 'IF-Score': 303.7691614627838, 'Final Selection': 304.0284495353699} CTA-WebTable 14K data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ER_train = pd.read_json('../../MELD/dataset/ER/wdc_all_train_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ER_Embed_Text(df): ## 返回3列分别是text_1,text_2,label\n",
    "    SimTab = df\n",
    "    context_list = []\n",
    "    target_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(SimTab)):\n",
    "        text = SimTab.iloc[i,0]\n",
    "        label = list(eval(SimTab.iloc[i,2]).values())[0]\n",
    "        label_list.append(label) ## add label\n",
    "        context = text.split('Output format example:{\"Output\": \"\"}\\n\\nEntity 1:')[1].split('\\n\\nEntity 2:')[0].replace('\\\\n', '').replace('\\n', '').replace('\\\\', '')\n",
    "        context_list.append(context) ## add context\n",
    "        \n",
    "        target = text.split('\\n\\nEntity 2:')[1].split('\\n\\nTake these examples as reference:')[0].replace('\\\\n', '').replace('\\n', '').replace('\\\\', '')\n",
    "        target_list.append(target)\n",
    "    return context_list,target_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(ER_train.iloc[0,0])\n",
    "# train = pd.read_json('../../MELD/dataset/ER/semi-text-w-train-MoE.json')\n",
    "# # print(ER_train.iloc[0,0])\n",
    "# test = pd.read_json('../../MELD/dataset/ER/semi-text-w-test-MoE.json')\n",
    "\n",
    "# print(ER_train.iloc[0,0])\n",
    "train = pd.read_json('../../MELD/dataset/ER/walmart_amazon_train_output.json')\n",
    "# print(ER_train.iloc[0,0])\n",
    "test = pd.read_json('../../MELD/dataset/ER/walmart_amazon_test_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.columns = train.columns\n",
    "train.columns = ['instruction','input','output']\n",
    "test.columns = ['instruction','input','output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['instruction','input','output']:\n",
    "    train[column] = train[column].str.replace('dismatch','mismatch')\n",
    "    test[column] = test[column].str.replace('dismatch','mismatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['instruction'] = train['instruction'].str.split('\\n\\nTake these examples as reference:')[0]\n",
    "# test['instruction'] = test['instruction'].str.split('\\n\\nTake these examples as reference:')[0]\n",
    "def cut_length(row):\n",
    "    instruction = row['instruction']\n",
    "    instruction = instruction.split('\\n\\nTake these examples as reference:')[0]\n",
    "    return instruction\n",
    "train['instruction'] = train.apply(cut_length,axis=1)\n",
    "test['instruction'] = test.apply(cut_length,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7103"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(train.to_dict(orient='records'), open('../train/ER/walmart-amazon/walmart-amazon-train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(test.to_dict(orient='records'), open('../train/ER/walmart-amazon/walmart-amazon-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'init_select': 61.000027894973755, 'init-train': 171.5253722667694, 'batch-division': 182.59329438209534, 'gradient-calculation': 228.29285073280334, 'IF-Score': 261.42748260498047, 'Final Selection': 261.6604371070862, 'Selection fine-tune': 509.3796503543854, 'Selection Inference': 590.3308668136597} wdc-time-4K 0.8678544914625093\n",
    "semi-text-w 0.7230\n",
    "semi-text-c F1:0.7885597548518897\n",
    "{'init_select': 56.48771262168884, 'init-train': 165.2728819847107, 'batch-division': 171.84898138046265, 'gradient-calculation': 210.02755188941956, 'IF-Score': 234.4348783493042, 'Final Selection': 234.57274293899536, 'Selection fine-tune': 491.0318133831024, 'Selection Inference': 552.2762784957886} Abt-Buy 0.8957871396895787\n",
    "\n",
    "{'init_select': 36.64403533935547, 'init-train': 117.11347103118896, 'batch-division': 122.72263956069946, 'gradient-calculation': 150.83489441871643, 'IF-Score': 172.2080376148224, 'Final Selection': 172.37958312034607, 'Selection fine-tune': 347.78654956817627, 'Selection Inference': 408.50533270835876} amazon-google F1:0.782435129740519\n",
    "\n",
    "{'init_select': 42.48365902900696, 'init-train': 129.5051372051239, 'batch-division': 135.72513437271118, 'gradient-calculation': 169.5727024078369, 'IF-Score': 193.17675495147705, 'Final Selection': 193.3632688522339, 'Selection fine-tune': 507.74013566970825, 'Selection Inference': 569.8979163169861} F1:0.8017817371937639 walmart-amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,t,l = ER_Embed_Text(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation Task training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('../../MELD/dataset/DC/hospital-train-MoE.json')\n",
    "# print(ER_train.iloc[0,0])\n",
    "test = pd.read_json('../../MELD/dataset/DC/hospital-test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DC_Embed_Text(df): ## 返回3列分别是text_1,text_2,label\n",
    "    SimTab = df\n",
    "    context_list = []\n",
    "    target_list = []\n",
    "    label_list = []\n",
    "    for i in tqdm(range(len(SimTab))):\n",
    "        text = SimTab.iloc[i,0]\n",
    "        label = SimTab.iloc[i,2]\n",
    "        label_list.append(label) ## add label\n",
    "        if text.__contains__('You are an expert in Cleaning Hospital Dataset.'):\n",
    "            context = text.split('Entity 1:\\n\\n')[1].split('\\n\\nTake these rows as reference:')[0]\n",
    "            context_list.append(context) ## add context\n",
    "            \n",
    "            context_dict = eval(context)\n",
    "            \n",
    "            target = text.split('Output Format Example:\\n\\n')[1].split('\\n\\nEntity 1:')[0]\n",
    "            \n",
    "            target_attr = list(eval(target).keys())[0]\n",
    "            \n",
    "            related_context = context_dict[target_attr]\n",
    "            target_list.append(str({target_attr:related_context}))\n",
    "        elif text.__contains__('You are an expert in Cleaning Beers Dataset.'):\n",
    "            context = text.split('Entity 1:\\n\\n')[1].split('\\n\\nThe input')[0]\n",
    "            context_list.append(context) ## add context\n",
    "            \n",
    "            context_dict = eval(context)\n",
    "            \n",
    "            target = text.split('Output Format Example:\\n\\n')[1].split('\\n\\nEntity 1:')[0]\n",
    "            \n",
    "            target_attr = list(eval(target).keys())[0]\n",
    "            \n",
    "            related_context = context_dict[target_attr]\n",
    "            target_list.append(str({target_attr:related_context}))\n",
    "        elif text.__contains__('You are an expert in Cleaning Rayyan Dataset.'):\n",
    "            context = text.split('Entity 1:\\n\\n')[1].split('\\n\\nThe input')[0]\n",
    "            context_list.append(context) ## add context\n",
    "            \n",
    "            context_dict = eval(context)\n",
    "            \n",
    "            target = text.split('Output Format Example:\\n\\n')[1].split('\\n\\nEntity 1:')[0]\n",
    "            \n",
    "            target_attr = list(eval(target).keys())[0]\n",
    "            \n",
    "            related_context = context_dict[target_attr]\n",
    "            target_list.append(str({target_attr:related_context}))\n",
    "    return context_list,target_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a134aace28db45c680d0ce1235e1770a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c,t,l = DC_Embed_Text(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"journal_title\": \"\"}'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
