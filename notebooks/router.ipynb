{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "# from dataloader import create_dataloaders\n",
    "from dataloader_ER import create_dataloaders,create_dataloaders_multi_label\n",
    "# from lora_model import LORAEngine,LORAEngineDeberta,LORAEngineDebertaMultiClass\n",
    "from lora_model_vmap import LORAEngineDebertaMultiClass,LORAEngineDebertaMultiLabel\n",
    "\n",
    "# from influence import IFEngine\n",
    "# from influence_hyperinf import IFEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path=\"roberta-large\"\n",
    "# model_name_or_path=\"/home/yanmy/roberta-base\"\n",
    "# model_name_or_path=\"../../model/deberta-v3-base\"\n",
    "model_name_or_path=\"../../model/ModernBERT-base/\"\n",
    "# model_name_or_path=\"../../model/Qwen2.5-0.5B-Instruct/\"\n",
    "# model_name_or_path=\"../../model/roberta-large\"\n",
    "# task=\"mrpc\"\n",
    "task = \"Router\"\n",
    "noise_ratio=0\n",
    "batch_size=16\n",
    "# target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "# target_modules = \n",
    "target_modules = ['Wqkv', 'Wo', 'Wi']\n",
    "# target_modules=[\"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "# target_modules=[\"q_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "\n",
    "device=\"cuda:7\"\n",
    "num_epochs=1\n",
    "lr=3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 22592\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b6ff956e9b4f81a28a52710e524d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75c4ee941b1451bb971b33301ff0005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc816946bbb47a59cb61e15fc535dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataloader_outputs = create_dataloaders_multi_label(model_name_or_path=model_name_or_path,\n",
    "                                        #    task=task,\n",
    "                                        #    noise_ratio=noise_ratio,\n",
    "                                           batch_size=batch_size,\n",
    "                                           train_file = '/data/home/wangys/MoE-Example/Router/train.csv',\n",
    "                                           valid_file = '/data/home/wangys/MoE-Example/Router/valid.csv',\n",
    "                                           test_file = '/data/home/wangys/MoE-Example/Router/test.csv',\n",
    "                                           max_length=2048)\n",
    "# train_dataloader, eval_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "train_dataloader, eval_dataloader,test_dataloader, tokenized_datasets, collate_fn, label2id, id2label =dataloader_outputs\n",
    "\n",
    "num_labels = len(label2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 15306\n",
      "Test dataset size: 4921\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset id from huggingface.co/dataset\n",
    "dataset_id = \"../datasets/LLM_Router\"\n",
    "\n",
    "# Load raw dataset\n",
    "raw_dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(raw_dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(raw_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 15306\n",
      "Test dataset size: 4921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertForSequenceClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at ../../model/ModernBERT-base/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset id from huggingface.co/dataset\n",
    "dataset_id = \"../datasets/LLM_Router\"\n",
    "\n",
    "# Load raw dataset\n",
    "raw_dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(raw_dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(raw_dataset['test'])}\")\n",
    " \n",
    "# Model id to load the tokenizer\n",
    "model_id = \"../../model/ModernBERT-base/\"\n",
    "# model_id = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 1024 # set model_max_length to 512 as prompts are not longer than 1024 tokens\n",
    "\n",
    "# Tokenize helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['prompt'], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize dataset\n",
    "raw_dataset =  raw_dataset.rename_column(\"label\", \"labels\") # to match Trainer\n",
    "tokenized_dataset = raw_dataset.map(tokenize, batched=True,remove_columns=[\"prompt\"])\n",
    "# labels = tokenized_dataset[\"train\"].features[\"labels\"].names\n",
    "# num_labels = len(labels)\n",
    "# label2id, id2label = dict(), dict()\n",
    "# for i, label in enumerate(labels):\n",
    "#     label2id[label] = str(i)\n",
    "#     id2label[str(i)] = label\n",
    "# Prepare model labels - useful for inference\n",
    "# labels = tokenized_dataset[\"train\"].features[\"labels\"].names\n",
    "# num_labels = len(labels)\n",
    "# label2id, id2label = dict(), dict()\n",
    "# for i, label in enumerate(labels):\n",
    "#     label2id[label] = str(i)\n",
    "#     id2label[str(i)] = label\n",
    " \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# è®¾ç½®CUDAè®¾å¤‡\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Dataset folder containing train.csv, valid.csv, and test.csv\n",
    "dataset_folder = \"/data/home/wangys/MoE-Example/Router/\"\n",
    "\n",
    "# åŠ è½½ CSV æ–‡ä»¶å¹¶åˆ›å»º DatasetDict\n",
    "def load_csv_dataset(folder_path):\n",
    "    data_files = {\n",
    "        \"train\": os.path.join(folder_path, \"train.csv\"),\n",
    "        \"valid\": os.path.join(folder_path, \"valid.csv\"),\n",
    "        \"test\": os.path.join(folder_path, \"test.csv\"),\n",
    "    }\n",
    "\n",
    "    dataset = {}\n",
    "    for split, file_path in data_files.items():\n",
    "        df = pd.read_csv(file_path,index_col=0).sample(n=2000).reset_index(drop=True)\n",
    "        df.columns = ['text','labels']\n",
    "        # å°† `labels` åˆ—è½¬æ¢ä¸º Python åˆ—è¡¨å¯¹è±¡\n",
    "        df[\"labels\"] = df[\"labels\"].apply(eval)  # ä½¿ç”¨ eval() å°†å­—ç¬¦ä¸²è§£æžä¸ºåˆ—è¡¨\n",
    "        dataset[split] = Dataset.from_pandas(df)\n",
    "\n",
    "    return DatasetDict(dataset)\n",
    "\n",
    "raw_dataset = load_csv_dataset(dataset_folder)\n",
    "\n",
    "print(f\"Train dataset size: {len(raw_dataset['train'])}\")\n",
    "print(f\"Valid dataset size: {len(raw_dataset['valid'])}\")\n",
    "print(f\"Test dataset size: {len(raw_dataset['test'])}\")\n",
    "\n",
    "# Model id to load the tokenizer\n",
    "model_id = \"../../model/ModernBERT-base/\"\n",
    "# model_id = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 1024  # è®¾ç½®æœ€å¤§é•¿åº¦\n",
    "\n",
    "# èŽ·å–æ‰€æœ‰çš„æ ‡ç­¾åˆ—è¡¨\n",
    "def extract_all_labels(dataset):\n",
    "    all_labels = set()\n",
    "    for split in dataset:  # éåŽ† train/valid/test\n",
    "        for labels in dataset[split][\"labels\"]:\n",
    "            all_labels.update(labels)\n",
    "    return sorted(all_labels)\n",
    "\n",
    "all_labels = extract_all_labels(raw_dataset)\n",
    "num_labels = len(all_labels)\n",
    "label2id = {label: i for i, label in enumerate(all_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(\"Label2ID mapping:\", label2id)\n",
    "\n",
    "# Tokenize helper function\n",
    "def tokenize(batch):\n",
    "    # Tokenize the text column\n",
    "    encodings = tokenizer(\n",
    "        batch['text'],  # Handle a list of text inputs\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "    )\n",
    "\n",
    "    # Ensure labels are converted correctly\n",
    "    if isinstance(batch['labels'][0], str):  # If labels are strings, evaluate them to convert to lists\n",
    "        labels = [eval(label) for label in batch['labels']]\n",
    "    else:  # Otherwise, assume they are already lists\n",
    "        labels = [label2id[b] for b in batch['labels']]\n",
    "    print(batch['labels'])\n",
    "    # Convert labels to tensors\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    # Combine tokenized inputs and labels\n",
    "    encodings[\"labels\"] = labels_tensor\n",
    "    return encodings\n",
    "\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = raw_dataset.map(tokenize, batched=True, remove_columns=[\"text\", \"labels\"])\n",
    "\n",
    "# å‡†å¤‡æ¨¡åž‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "print(\"Model and dataset are ready for multi-label classification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    " \n",
    "# Metric helper method\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    score = f1_score(\n",
    "            labels, predictions, labels=labels, pos_label=1, average=\"weighted\"\n",
    "        )\n",
    "    return {\"f1\": float(score) if score == 1 else score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Trainer, TrainingArguments\n",
    " \n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"modernbert-llm-router\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "\tnum_train_epochs=5,\n",
    "    bf16=True, # bfloat16 training \n",
    "    optim=\"adamw_torch_fused\", # improved optimizer \n",
    "    # logging & evaluation strategies\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"none\",\n",
    ")\n",
    " \n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/bert24/lib/python3.11/site-packages/transformers/training_args.py:1588: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Metric helper method\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = (predictions > 0.5).astype(int)  # Convert logits to binary predictions\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"modernbert-llm-router\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=2,\n",
    "    bf16=True,  # bfloat16 training \n",
    "    optim=\"adamw_torch_fused\",  # improved optimizer \n",
    "    # logging & evaluation strategies\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lora_engine = LORAEngineDebertaMultiLabel(model_name_or_path=model_name_or_path,\n",
    "                            target_modules=target_modules,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            # eval_dataloader=eval_dataloader,\n",
    "                            eval_dataloader=eval_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            device=device,\n",
    "                            num_epochs=num_epochs,\n",
    "                            lr=lr,\n",
    "                            low_rank=16, \n",
    "                            task=task,\n",
    "                            save_path = '../models/ER',\n",
    "                            valid_each_epoch=True,\n",
    "                            cal_grad_per_sample = False,\n",
    "                            num_labels=num_labels,\n",
    "                            label2id=label2id,\n",
    "                            id2label=id2label,\n",
    "                            use_lora = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at ../../model/ModernBERT-base/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,383,814 || all params: 152,993,292 || trainable%: 2.2117\n",
      "Total Steps:1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1412/1412 [04:54<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 0.32238240604402324\n",
      "Epoch 1:Precision: 0.8750, Recall: 1.0000, F1-score: 0.9333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 177/177 [00:14<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9012, Recall: 1.0000, F1-score: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_engine.build_LORA_model()\n",
    "all_logits = lora_engine.train_LORA_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
