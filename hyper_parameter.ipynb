{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util_func import load_yaml,z_score_normalize,get_top_k_indices,run_command,round_down_to_power_of_two,load_yaml_args,normalize_to_neg1_pos1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vary Selection Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Method MELD-DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(sample_IF):\n",
    "    \"\"\"\n",
    "    对sample_IF字典中各个'method'对应的值（键为index、值为float的字典）进行Z-score归一化。\n",
    "\n",
    "    参数:\n",
    "    sample_IF (dict): 包含多个'method'键的字典，每个'method'键对应的值为需要进行归一化处理的字典数据。\n",
    "\n",
    "    返回:\n",
    "    dict: 归一化后的字典，结构与输入的sample_IF一致，其中每个'method'键对应的值都已经完成Z-score归一化。\n",
    "    \"\"\"\n",
    "    for method in sample_IF.keys():\n",
    "        # 获取当前method对应需要归一化的值列表，保持原有顺序\n",
    "        values_list = list(sample_IF[method].values())\n",
    "        # 将列表转换为torch.Tensor\n",
    "        tensor_value = torch.tensor(values_list).unsqueeze(1)  # 添加维度，变为二维张量\n",
    "\n",
    "        # 计算均值和标准差\n",
    "        mean_value = tensor_value.mean()\n",
    "        std_value = tensor_value.std()\n",
    "\n",
    "        # 进行Z-score归一化\n",
    "        normalized_tensor = (tensor_value - mean_value) / std_value\n",
    "\n",
    "        # 将归一化后的结果再转换回列表\n",
    "        normalized_list = normalized_tensor.squeeze(1).tolist()\n",
    "\n",
    "        # 更新原字典中当前method对应的值\n",
    "        index_list = list(sample_IF[method].keys())\n",
    "        normalized_dict = {}\n",
    "        for index, normalized_value in zip(index_list, normalized_list):\n",
    "            normalized_dict[index] = normalized_value\n",
    "\n",
    "        sample_IF[method] = normalized_dict\n",
    "\n",
    "    return sample_IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100015/3535340534.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ppl_array[index] = row[0]\n",
      "/tmp/ipykernel_100015/3535340534.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ppl_array[index] = row[0]\n",
      "/tmp/ipykernel_100015/3535340534.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ppl_array[index] = row[0]\n",
      "/tmp/ipykernel_100015/3535340534.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ppl_array[index] = row[0]\n",
      "/tmp/ipykernel_100015/3535340534.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ppl_array[index] = row[0]\n",
      "/tmp/ipykernel_100015/3535340534.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ppl_array[index] = row[0]\n",
      "/tmp/ipykernel_100015/3535340534.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ppl_array[index] = row[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09059286146400485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100015/3535340534.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ppl_array[index] = row[0]\n"
     ]
    }
   ],
   "source": [
    "## reverse total score\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from src.util_func import load_yaml\n",
    "task_list = [\n",
    "    # ['RE','RE'],\n",
    "    # ['ER','abt-buy'],\n",
    "    # ['ER','semi-text-w'],\n",
    "    # ['DC','hospital'],\n",
    "    # ['DC','beer'],\n",
    "    # ['DC','rayyan'],\n",
    "    # ['DI','amazon'],\n",
    "    # ['DI','walmart'],\n",
    "    # ['AVE','oa_mine'],\n",
    "    # ['CTA','SimTab'],\n",
    "    # ['CTA','WebTable'],\n",
    "    # ['ER','walmart-amazon'],\n",
    "    # ['ER','semi-text-c'],\n",
    "    ['ER','amazon-google'],\n",
    "    # ['ER','wdc'],\n",
    "    # ['SM','CMS']\n",
    "]\n",
    "for task,dataset in task_list:\n",
    "    yaml_path = f'script/config_{task}_{dataset}.yaml'\n",
    "    config = load_yaml(yaml_path)\n",
    "    cluster_num = config['cluster_num']\n",
    "    cluster_per_budget = config['sample_per_cluster']\n",
    "    cluster_per_budget = 3\n",
    "    train_file_path = config['train_file_path']\n",
    "    train_file = pd.read_json(train_file_path)\n",
    "\n",
    "    cluster_rank = torch.load(f'../DataSelection-IF/selection/{task}/{dataset}/Cluster-Rank.pkl',weights_only=False)\n",
    "    total_score = torch.load(f'../DataSelection-IF/selection/{task}/{dataset}/Total-Score.pkl',weights_only=False)\n",
    "    sample_IF = torch.load(f'Influence/{task}/{dataset}/score.pkl',weights_only=False)\n",
    "    sample_IF = z_score_normalize(sample_IF)\n",
    "\n",
    "    ppl_array = np.zeros(len(total_score))\n",
    "    for process_num in range(1,9,1): ## maximum of k process\n",
    "        if os.path.exists('ppl/{}/{}/ppl-init-{}.csv'.format(task,dataset,process_num)): ## i-th gradient \n",
    "            ppl_df = pd.read_csv('ppl/{}/{}/ppl-init-{}.csv'.format(task,dataset,process_num),index_col=0)\n",
    "            for index,row in ppl_df.iterrows():\n",
    "                ppl_array[index] = row[0]\n",
    "\n",
    "    # QuRating_Score = np.load(f'QuRating/data/select_index_main_writing/{task}-{dataset}-QuRating-score.npy',allow_pickle=True).item()\n",
    "    # for key in range(len(total_score)):\n",
    "    #     # total_score[i] = total_score[i] - ppl_array[i]\n",
    "    #     if sample_IF['iterative'].__contains__(key):\n",
    "    #         total_score[key] += sample_IF['iterative'][key]\n",
    "    #     # total_score[i] = total_score[i] + 6 * QuRating_Score[i]\n",
    "    for key in sample_IF['iterative'].keys():\n",
    "        total_score[key] += sample_IF['iterative'][key]\n",
    "\n",
    "\n",
    "    p2_choose_index = []\n",
    "    count = 0\n",
    "    for i in cluster_rank.keys():\n",
    "        index_all = cluster_rank[i]\n",
    "        sorted_score_all = total_score[index_all]\n",
    "        sorted_index_all = index_all[np.argsort(-sorted_score_all)]\n",
    "        # print(index_all,sorted_index_all)\n",
    "        # break\n",
    "        change = cluster_per_budget - len(set(index_all[:cluster_per_budget]).intersection(set(sorted_index_all[:cluster_per_budget])))\n",
    "        count += change\n",
    "        p2_choose_index.extend(sorted_index_all[:cluster_per_budget])\n",
    "    select_IF_df = train_file.iloc[p2_choose_index]\n",
    "    print(len(select_IF_df) / len(train_file))\n",
    "    json.dump(select_IF_df.to_dict(orient='records'), open('train/{}/{}/train-select-w-main-size-10.json'.format(task,dataset), 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2066997145861499"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python Ablation_Study.py --yaml_path script/config_RE_RE.yaml --device 0,1 --model mistral-7B --DO_TRAIN_MAIN_VAR --DO_EVAL_MAIN_VAR --main_var IF-single-size-10 IF-single-size-20 DSIR-size-10 DSIR-size-20 SuperFiltering-size-10 SuperFiltering-size-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python Ablation_Study.py --yaml_path script/config_ER_amazon-google.yaml --device 2,3 --model mistral-7B --DO_TRAIN_MAIN_VAR --DO_EVAL_MAIN_VAR --main_var IF-single-size-10 IF-single-size-20 DSIR-size-10 DSIR-size-20 SuperFiltering-size-10 SuperFiltering-size-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataInf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "IF-single-size-10 IF-single-size-20 DSIR-size-10 DSIR-size-20 SuperFiltering-size-10 SuperFiltering-size-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'ER'\n",
    "dataset = 'amazon-google'\n",
    "select_num_df = pd.read_json('train/{}/{}/train-select-w-main-size-10.json'.format(task,dataset))\n",
    "select_num = len(select_num_df)\n",
    "yaml_path = f'script/config_{task}_{dataset}.yaml'\n",
    "config = load_yaml(yaml_path)\n",
    "train_file_path = config['train_file_path']\n",
    "train_file = pd.read_json(train_file_path)\n",
    "if os.path.exists('Influence_single/{}/{}'.format(task,dataset)):\n",
    "    sample_IF_single = torch.load('Influence_single/{}/{}/score.pkl'.format(task,dataset),weights_only=False)\n",
    "    for IF_method in sample_IF_single.keys():\n",
    "        select_IF_single = get_top_k_indices(sample_IF_single[IF_method],k=select_num,IF=True)\n",
    "        select_IF_single_df = train_file.iloc[select_IF_single]\n",
    "        json.dump(select_IF_single_df.to_dict(orient='records'), open('train/{}/{}/train-select-w-IF-single-size-10.json'.format(task,dataset,IF_method), 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SuperFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUDA_VISIBLE_DEVICES=3 python code_ifd/select_data.py --json_data_path output/ER-amazon-google-gpt2.jsonl --json_save_path ../DataSelection-IF/train/ER/amazon-google/train-select-w-SuperFiltering-size-10.json --sample_num 599 '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = 'ER'\n",
    "dataset = 'amazon-google'\n",
    "percentage = 10\n",
    "select_num_df = pd.read_json(f'train/{task}/{dataset}/train-select-w-main-size-{percentage}.json')\n",
    "select_size = len(select_num_df)\n",
    "step_3_command = f'CUDA_VISIBLE_DEVICES=3 python code_ifd/select_data.py \\\n",
    "--json_data_path output/{task}-{dataset}-gpt2.jsonl \\\n",
    "--json_save_path ../DataSelection-IF/train/{task}/{dataset}/train-select-w-SuperFiltering-size-{percentage}.json \\\n",
    "--sample_num {select_size} '\n",
    "step_3_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IF-single-iterative': {'acc': 0.8711389961389961,\n",
       "  'micro_f1': 0.7471026490066226,\n",
       "  'macro_f1': 0.5905163780263318},\n",
       " 'IF-single-identity': {'acc': 0.8426640926640927,\n",
       "  'micro_f1': 0.722682119205298,\n",
       "  'macro_f1': 0.55191678272745},\n",
       " 'IF-single-proposed': {'acc': 0.8537644787644788,\n",
       "  'micro_f1': 0.732201986754967,\n",
       "  'macro_f1': 0.5632134148611553},\n",
       " 'main': {'acc': 0.9005791505791506,\n",
       "  'micro_f1': 0.7723509933774834,\n",
       "  'macro_f1': 0.5830344674616523},\n",
       " 'ppl': {'acc': 0.8513513513513513,\n",
       "  'micro_f1': 0.7301324503311257,\n",
       "  'macro_f1': 0.5310203623037854},\n",
       " 'FL': {'acc': 0.9073359073359073,\n",
       "  'micro_f1': 0.7781456953642383,\n",
       "  'macro_f1': 0.5600671023778724},\n",
       " 'IF': {'acc': 0.8388030888030888,\n",
       "  'micro_f1': 0.7193708609271523,\n",
       "  'macro_f1': 0.5556746386823592},\n",
       " 'QuRating': {'acc': 0.9145752895752896,\n",
       "  'micro_f1': 0.7843543046357615,\n",
       "  'macro_f1': 0.5714786595124909},\n",
       " 'main-QuRating': {'acc': 0.8943050193050193,\n",
       "  'micro_f1': 0.7669701986754967,\n",
       "  'macro_f1': 0.576607674834364},\n",
       " 'LESS': {'acc': 0.9247104247104247,\n",
       "  'micro_f1': 0.793046357615894,\n",
       "  'macro_f1': 0.597989768285966},\n",
       " 'SuperFiltering': {'acc': 0.8093629343629344,\n",
       "  'micro_f1': 0.6941225165562914,\n",
       "  'macro_f1': 0.46218595077079705},\n",
       " 'DSIR': {'acc': 0.902992277992278,\n",
       "  'micro_f1': 0.7744205298013245,\n",
       "  'macro_f1': 0.5621622023574433},\n",
       " 'main-DSIR': {'acc': 0.8991312741312741,\n",
       "  'micro_f1': 0.7711092715231789,\n",
       "  'macro_f1': 0.5931276174650362},\n",
       " 'main-IF': {'acc': 0.793918918918919,\n",
       "  'micro_f1': 0.6808774834437087,\n",
       "  'macro_f1': 0.46861737990672686},\n",
       " 'main-noIF': {'acc': 0.8885135135135135,\n",
       "  'micro_f1': 0.7620033112582781,\n",
       "  'macro_f1': 0.5568016333590095},\n",
       " 'main-size-10': {'acc': 0.874034749034749,\n",
       "  'micro_f1': 0.7495860927152318,\n",
       "  'macro_f1': 0.5524624924254952},\n",
       " 'main-size-20': {'acc': 0.8947876447876448,\n",
       "  'micro_f1': 0.7673841059602649,\n",
       "  'macro_f1': 0.577594502289667}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('eval_result/mistral-7B-RE-RE.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
