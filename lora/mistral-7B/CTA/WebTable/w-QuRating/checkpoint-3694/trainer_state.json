{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3694,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005414185165132647,
      "grad_norm": 125.84483337402344,
      "learning_rate": 2.432432432432433e-06,
      "loss": 4.7168,
      "step": 10
    },
    {
      "epoch": 0.010828370330265295,
      "grad_norm": 52.09004592895508,
      "learning_rate": 5.135135135135136e-06,
      "loss": 3.5279,
      "step": 20
    },
    {
      "epoch": 0.016242555495397944,
      "grad_norm": 26.09783172607422,
      "learning_rate": 7.837837837837838e-06,
      "loss": 2.6042,
      "step": 30
    },
    {
      "epoch": 0.02165674066053059,
      "grad_norm": 20.314117431640625,
      "learning_rate": 1.0540540540540541e-05,
      "loss": 0.9146,
      "step": 40
    },
    {
      "epoch": 0.02707092582566324,
      "grad_norm": 4.854868412017822,
      "learning_rate": 1.3243243243243244e-05,
      "loss": 0.3835,
      "step": 50
    },
    {
      "epoch": 0.03248511099079589,
      "grad_norm": 11.562946319580078,
      "learning_rate": 1.5945945945945947e-05,
      "loss": 0.4187,
      "step": 60
    },
    {
      "epoch": 0.03789929615592853,
      "grad_norm": 9.63088607788086,
      "learning_rate": 1.864864864864865e-05,
      "loss": 0.3676,
      "step": 70
    },
    {
      "epoch": 0.04331348132106118,
      "grad_norm": 4.453158378601074,
      "learning_rate": 2.1351351351351353e-05,
      "loss": 0.1218,
      "step": 80
    },
    {
      "epoch": 0.04872766648619383,
      "grad_norm": 7.841948986053467,
      "learning_rate": 2.4054054054054056e-05,
      "loss": 0.0658,
      "step": 90
    },
    {
      "epoch": 0.05414185165132648,
      "grad_norm": 1.6542608737945557,
      "learning_rate": 2.6756756756756756e-05,
      "loss": 0.119,
      "step": 100
    },
    {
      "epoch": 0.05955603681645912,
      "grad_norm": 1.1822863817214966,
      "learning_rate": 2.945945945945946e-05,
      "loss": 0.0788,
      "step": 110
    },
    {
      "epoch": 0.06497022198159177,
      "grad_norm": 1.4891260862350464,
      "learning_rate": 3.2162162162162165e-05,
      "loss": 0.0181,
      "step": 120
    },
    {
      "epoch": 0.07038440714672442,
      "grad_norm": 5.63910436630249,
      "learning_rate": 3.486486486486487e-05,
      "loss": 0.1264,
      "step": 130
    },
    {
      "epoch": 0.07579859231185707,
      "grad_norm": 10.248591423034668,
      "learning_rate": 3.756756756756757e-05,
      "loss": 0.0681,
      "step": 140
    },
    {
      "epoch": 0.08121277747698971,
      "grad_norm": 4.169712066650391,
      "learning_rate": 4.0270270270270274e-05,
      "loss": 0.0578,
      "step": 150
    },
    {
      "epoch": 0.08662696264212236,
      "grad_norm": 1.317439317703247,
      "learning_rate": 4.297297297297298e-05,
      "loss": 0.0296,
      "step": 160
    },
    {
      "epoch": 0.092041147807255,
      "grad_norm": 3.992046356201172,
      "learning_rate": 4.567567567567568e-05,
      "loss": 0.0898,
      "step": 170
    },
    {
      "epoch": 0.09745533297238766,
      "grad_norm": 0.3586070239543915,
      "learning_rate": 4.837837837837838e-05,
      "loss": 0.0408,
      "step": 180
    },
    {
      "epoch": 0.10286951813752031,
      "grad_norm": 4.471882343292236,
      "learning_rate": 5.1081081081081086e-05,
      "loss": 0.0397,
      "step": 190
    },
    {
      "epoch": 0.10828370330265295,
      "grad_norm": 0.764348030090332,
      "learning_rate": 5.378378378378378e-05,
      "loss": 0.0416,
      "step": 200
    },
    {
      "epoch": 0.1136978884677856,
      "grad_norm": 15.63594913482666,
      "learning_rate": 5.648648648648649e-05,
      "loss": 0.0698,
      "step": 210
    },
    {
      "epoch": 0.11911207363291824,
      "grad_norm": 0.08774740248918533,
      "learning_rate": 5.918918918918919e-05,
      "loss": 0.0291,
      "step": 220
    },
    {
      "epoch": 0.12452625879805089,
      "grad_norm": 5.901494979858398,
      "learning_rate": 6.189189189189189e-05,
      "loss": 0.0705,
      "step": 230
    },
    {
      "epoch": 0.12994044396318355,
      "grad_norm": 2.41074538230896,
      "learning_rate": 6.459459459459459e-05,
      "loss": 0.0835,
      "step": 240
    },
    {
      "epoch": 0.1353546291283162,
      "grad_norm": 3.990743637084961,
      "learning_rate": 6.72972972972973e-05,
      "loss": 0.0557,
      "step": 250
    },
    {
      "epoch": 0.14076881429344884,
      "grad_norm": 0.5979434847831726,
      "learning_rate": 7e-05,
      "loss": 0.0515,
      "step": 260
    },
    {
      "epoch": 0.1461829994585815,
      "grad_norm": 0.28575748205184937,
      "learning_rate": 7.27027027027027e-05,
      "loss": 0.061,
      "step": 270
    },
    {
      "epoch": 0.15159718462371413,
      "grad_norm": 1.7549219131469727,
      "learning_rate": 7.54054054054054e-05,
      "loss": 0.049,
      "step": 280
    },
    {
      "epoch": 0.15701136978884678,
      "grad_norm": 1.6557278633117676,
      "learning_rate": 7.810810810810811e-05,
      "loss": 0.0653,
      "step": 290
    },
    {
      "epoch": 0.16242555495397942,
      "grad_norm": 3.4390883445739746,
      "learning_rate": 8.08108108108108e-05,
      "loss": 0.0402,
      "step": 300
    },
    {
      "epoch": 0.16783974011911207,
      "grad_norm": 0.09036239236593246,
      "learning_rate": 8.351351351351352e-05,
      "loss": 0.0402,
      "step": 310
    },
    {
      "epoch": 0.17325392528424471,
      "grad_norm": 3.6212005615234375,
      "learning_rate": 8.621621621621621e-05,
      "loss": 0.0494,
      "step": 320
    },
    {
      "epoch": 0.17866811044937736,
      "grad_norm": 0.1447906792163849,
      "learning_rate": 8.891891891891892e-05,
      "loss": 0.0162,
      "step": 330
    },
    {
      "epoch": 0.18408229561451,
      "grad_norm": 0.673901379108429,
      "learning_rate": 9.162162162162162e-05,
      "loss": 0.0578,
      "step": 340
    },
    {
      "epoch": 0.18949648077964265,
      "grad_norm": 0.8859617114067078,
      "learning_rate": 9.432432432432433e-05,
      "loss": 0.0433,
      "step": 350
    },
    {
      "epoch": 0.19491066594477532,
      "grad_norm": 0.2760806083679199,
      "learning_rate": 9.702702702702702e-05,
      "loss": 0.051,
      "step": 360
    },
    {
      "epoch": 0.20032485110990797,
      "grad_norm": 1.4033362865447998,
      "learning_rate": 9.972972972972973e-05,
      "loss": 0.0404,
      "step": 370
    },
    {
      "epoch": 0.20573903627504062,
      "grad_norm": 2.7674193382263184,
      "learning_rate": 9.999819116012134e-05,
      "loss": 0.0378,
      "step": 380
    },
    {
      "epoch": 0.21115322144017326,
      "grad_norm": 0.04012174904346466,
      "learning_rate": 9.99919385483188e-05,
      "loss": 0.0203,
      "step": 390
    },
    {
      "epoch": 0.2165674066053059,
      "grad_norm": 0.021367007866501808,
      "learning_rate": 9.998122039161928e-05,
      "loss": 0.0209,
      "step": 400
    },
    {
      "epoch": 0.22198159177043855,
      "grad_norm": 3.3930253982543945,
      "learning_rate": 9.996603764742536e-05,
      "loss": 0.0659,
      "step": 410
    },
    {
      "epoch": 0.2273957769355712,
      "grad_norm": 3.0523598194122314,
      "learning_rate": 9.99463916719402e-05,
      "loss": 0.0337,
      "step": 420
    },
    {
      "epoch": 0.23280996210070384,
      "grad_norm": 6.736275672912598,
      "learning_rate": 9.992228422004645e-05,
      "loss": 0.0191,
      "step": 430
    },
    {
      "epoch": 0.2382241472658365,
      "grad_norm": 3.5886807441711426,
      "learning_rate": 9.989371744514949e-05,
      "loss": 0.0344,
      "step": 440
    },
    {
      "epoch": 0.24363833243096913,
      "grad_norm": 2.753753662109375,
      "learning_rate": 9.986069389898502e-05,
      "loss": 0.0487,
      "step": 450
    },
    {
      "epoch": 0.24905251759610178,
      "grad_norm": 5.27712869644165,
      "learning_rate": 9.982321653139126e-05,
      "loss": 0.0542,
      "step": 460
    },
    {
      "epoch": 0.25446670276123445,
      "grad_norm": 1.3543390035629272,
      "learning_rate": 9.978128869004524e-05,
      "loss": 0.0364,
      "step": 470
    },
    {
      "epoch": 0.2598808879263671,
      "grad_norm": 1.6606029272079468,
      "learning_rate": 9.973491412016401e-05,
      "loss": 0.0585,
      "step": 480
    },
    {
      "epoch": 0.26529507309149974,
      "grad_norm": 6.667534828186035,
      "learning_rate": 9.968409696416986e-05,
      "loss": 0.0441,
      "step": 490
    },
    {
      "epoch": 0.2707092582566324,
      "grad_norm": 1.2167683839797974,
      "learning_rate": 9.962884176132057e-05,
      "loss": 0.0467,
      "step": 500
    },
    {
      "epoch": 0.27612344342176504,
      "grad_norm": 2.236240863800049,
      "learning_rate": 9.956915344730363e-05,
      "loss": 0.0389,
      "step": 510
    },
    {
      "epoch": 0.2815376285868977,
      "grad_norm": 0.11750505119562149,
      "learning_rate": 9.950503735379561e-05,
      "loss": 0.0683,
      "step": 520
    },
    {
      "epoch": 0.2869518137520303,
      "grad_norm": 1.8754128217697144,
      "learning_rate": 9.943649920798578e-05,
      "loss": 0.0188,
      "step": 530
    },
    {
      "epoch": 0.292365998917163,
      "grad_norm": 0.08526628464460373,
      "learning_rate": 9.936354513206456e-05,
      "loss": 0.032,
      "step": 540
    },
    {
      "epoch": 0.2977801840822956,
      "grad_norm": 3.003363847732544,
      "learning_rate": 9.928618164267665e-05,
      "loss": 0.0384,
      "step": 550
    },
    {
      "epoch": 0.30319436924742826,
      "grad_norm": 0.3713514804840088,
      "learning_rate": 9.920441565033892e-05,
      "loss": 0.0442,
      "step": 560
    },
    {
      "epoch": 0.3086085544125609,
      "grad_norm": 7.691275119781494,
      "learning_rate": 9.911825445882316e-05,
      "loss": 0.0885,
      "step": 570
    },
    {
      "epoch": 0.31402273957769355,
      "grad_norm": 0.5232574939727783,
      "learning_rate": 9.902770576450364e-05,
      "loss": 0.0412,
      "step": 580
    },
    {
      "epoch": 0.3194369247428262,
      "grad_norm": 0.0693608894944191,
      "learning_rate": 9.893277765566959e-05,
      "loss": 0.0214,
      "step": 590
    },
    {
      "epoch": 0.32485110990795885,
      "grad_norm": 1.7488300800323486,
      "learning_rate": 9.883347861180283e-05,
      "loss": 0.0387,
      "step": 600
    },
    {
      "epoch": 0.3302652950730915,
      "grad_norm": 0.9067704081535339,
      "learning_rate": 9.87298175028202e-05,
      "loss": 0.0322,
      "step": 610
    },
    {
      "epoch": 0.33567948023822414,
      "grad_norm": 0.6581000685691833,
      "learning_rate": 9.862180358828133e-05,
      "loss": 0.0335,
      "step": 620
    },
    {
      "epoch": 0.3410936654033568,
      "grad_norm": 1.8716267347335815,
      "learning_rate": 9.850944651656152e-05,
      "loss": 0.0363,
      "step": 630
    },
    {
      "epoch": 0.34650785056848943,
      "grad_norm": 0.24174907803535461,
      "learning_rate": 9.839275632398989e-05,
      "loss": 0.0308,
      "step": 640
    },
    {
      "epoch": 0.3519220357336221,
      "grad_norm": 0.11814843863248825,
      "learning_rate": 9.827174343395286e-05,
      "loss": 0.0535,
      "step": 650
    },
    {
      "epoch": 0.3573362208987547,
      "grad_norm": 1.492432951927185,
      "learning_rate": 9.814641865596312e-05,
      "loss": 0.04,
      "step": 660
    },
    {
      "epoch": 0.36275040606388737,
      "grad_norm": 1.641128659248352,
      "learning_rate": 9.801679318469401e-05,
      "loss": 0.0296,
      "step": 670
    },
    {
      "epoch": 0.36816459122902,
      "grad_norm": 0.19025912880897522,
      "learning_rate": 9.788287859897964e-05,
      "loss": 0.033,
      "step": 680
    },
    {
      "epoch": 0.37357877639415266,
      "grad_norm": 0.017675884068012238,
      "learning_rate": 9.774468686078051e-05,
      "loss": 0.024,
      "step": 690
    },
    {
      "epoch": 0.3789929615592853,
      "grad_norm": 1.970911979675293,
      "learning_rate": 9.760223031411505e-05,
      "loss": 0.0291,
      "step": 700
    },
    {
      "epoch": 0.38440714672441795,
      "grad_norm": 2.3891661167144775,
      "learning_rate": 9.745552168395701e-05,
      "loss": 0.0482,
      "step": 710
    },
    {
      "epoch": 0.38982133188955065,
      "grad_norm": 0.34417304396629333,
      "learning_rate": 9.730457407509875e-05,
      "loss": 0.0351,
      "step": 720
    },
    {
      "epoch": 0.3952355170546833,
      "grad_norm": 2.2434170246124268,
      "learning_rate": 9.714940097098069e-05,
      "loss": 0.0183,
      "step": 730
    },
    {
      "epoch": 0.40064970221981594,
      "grad_norm": 1.1336456537246704,
      "learning_rate": 9.699001623248686e-05,
      "loss": 0.0168,
      "step": 740
    },
    {
      "epoch": 0.4060638873849486,
      "grad_norm": 0.26655063033103943,
      "learning_rate": 9.682643409670681e-05,
      "loss": 0.0142,
      "step": 750
    },
    {
      "epoch": 0.41147807255008123,
      "grad_norm": 0.018038704991340637,
      "learning_rate": 9.665866917566385e-05,
      "loss": 0.026,
      "step": 760
    },
    {
      "epoch": 0.4168922577152139,
      "grad_norm": 0.2737966775894165,
      "learning_rate": 9.648673645500985e-05,
      "loss": 0.0065,
      "step": 770
    },
    {
      "epoch": 0.4223064428803465,
      "grad_norm": 2.12507963180542,
      "learning_rate": 9.631065129268664e-05,
      "loss": 0.0419,
      "step": 780
    },
    {
      "epoch": 0.42772062804547917,
      "grad_norm": 0.1116512268781662,
      "learning_rate": 9.613042941755409e-05,
      "loss": 0.0547,
      "step": 790
    },
    {
      "epoch": 0.4331348132106118,
      "grad_norm": 0.9920666217803955,
      "learning_rate": 9.594608692798526e-05,
      "loss": 0.0379,
      "step": 800
    },
    {
      "epoch": 0.43854899837574446,
      "grad_norm": 0.0758490338921547,
      "learning_rate": 9.575764029042826e-05,
      "loss": 0.0378,
      "step": 810
    },
    {
      "epoch": 0.4439631835408771,
      "grad_norm": 0.005599972791969776,
      "learning_rate": 9.55651063379355e-05,
      "loss": 0.011,
      "step": 820
    },
    {
      "epoch": 0.44937736870600975,
      "grad_norm": 1.5851383209228516,
      "learning_rate": 9.536850226866e-05,
      "loss": 0.0244,
      "step": 830
    },
    {
      "epoch": 0.4547915538711424,
      "grad_norm": 0.09629037231206894,
      "learning_rate": 9.516784564431917e-05,
      "loss": 0.0299,
      "step": 840
    },
    {
      "epoch": 0.46020573903627504,
      "grad_norm": 0.47334450483322144,
      "learning_rate": 9.496315438862616e-05,
      "loss": 0.0288,
      "step": 850
    },
    {
      "epoch": 0.4656199242014077,
      "grad_norm": 0.5971724390983582,
      "learning_rate": 9.475444678568872e-05,
      "loss": 0.0236,
      "step": 860
    },
    {
      "epoch": 0.47103410936654033,
      "grad_norm": 0.01746099442243576,
      "learning_rate": 9.454174147837607e-05,
      "loss": 0.0623,
      "step": 870
    },
    {
      "epoch": 0.476448294531673,
      "grad_norm": 0.17525719106197357,
      "learning_rate": 9.432505746665351e-05,
      "loss": 0.0139,
      "step": 880
    },
    {
      "epoch": 0.4818624796968056,
      "grad_norm": 0.14830513298511505,
      "learning_rate": 9.410441410588537e-05,
      "loss": 0.0601,
      "step": 890
    },
    {
      "epoch": 0.48727666486193827,
      "grad_norm": 3.1174051761627197,
      "learning_rate": 9.3879831105106e-05,
      "loss": 0.0557,
      "step": 900
    },
    {
      "epoch": 0.4926908500270709,
      "grad_norm": 0.22923779487609863,
      "learning_rate": 9.365132852525924e-05,
      "loss": 0.0047,
      "step": 910
    },
    {
      "epoch": 0.49810503519220356,
      "grad_norm": 0.0387270525097847,
      "learning_rate": 9.341892677740657e-05,
      "loss": 0.0199,
      "step": 920
    },
    {
      "epoch": 0.5035192203573362,
      "grad_norm": 0.03415464237332344,
      "learning_rate": 9.318264662090376e-05,
      "loss": 0.036,
      "step": 930
    },
    {
      "epoch": 0.5089334055224689,
      "grad_norm": 3.0797295570373535,
      "learning_rate": 9.29425091615466e-05,
      "loss": 0.0243,
      "step": 940
    },
    {
      "epoch": 0.5143475906876015,
      "grad_norm": 0.14299878478050232,
      "learning_rate": 9.269853584968568e-05,
      "loss": 0.0362,
      "step": 950
    },
    {
      "epoch": 0.5197617758527342,
      "grad_norm": 0.035804953426122665,
      "learning_rate": 9.245074847831021e-05,
      "loss": 0.0147,
      "step": 960
    },
    {
      "epoch": 0.5251759610178668,
      "grad_norm": 0.9607359170913696,
      "learning_rate": 9.219916918110138e-05,
      "loss": 0.0094,
      "step": 970
    },
    {
      "epoch": 0.5305901461829995,
      "grad_norm": 0.14787735044956207,
      "learning_rate": 9.194382043045537e-05,
      "loss": 0.0294,
      "step": 980
    },
    {
      "epoch": 0.5360043313481321,
      "grad_norm": 1.3866773843765259,
      "learning_rate": 9.168472503547586e-05,
      "loss": 0.0193,
      "step": 990
    },
    {
      "epoch": 0.5414185165132648,
      "grad_norm": 0.9536610245704651,
      "learning_rate": 9.142190613993666e-05,
      "loss": 0.0224,
      "step": 1000
    },
    {
      "epoch": 0.5468327016783974,
      "grad_norm": 0.094327911734581,
      "learning_rate": 9.115538722021442e-05,
      "loss": 0.0238,
      "step": 1010
    },
    {
      "epoch": 0.5522468868435301,
      "grad_norm": 0.06930060684680939,
      "learning_rate": 9.088519208319154e-05,
      "loss": 0.0193,
      "step": 1020
    },
    {
      "epoch": 0.5576610720086627,
      "grad_norm": 0.27972403168678284,
      "learning_rate": 9.061134486412962e-05,
      "loss": 0.0104,
      "step": 1030
    },
    {
      "epoch": 0.5630752571737954,
      "grad_norm": 0.028068341314792633,
      "learning_rate": 9.03338700245136e-05,
      "loss": 0.0158,
      "step": 1040
    },
    {
      "epoch": 0.568489442338928,
      "grad_norm": 0.011992385610938072,
      "learning_rate": 9.005279234986672e-05,
      "loss": 0.0368,
      "step": 1050
    },
    {
      "epoch": 0.5739036275040607,
      "grad_norm": 1.7494678497314453,
      "learning_rate": 8.976813694753651e-05,
      "loss": 0.0202,
      "step": 1060
    },
    {
      "epoch": 0.5793178126691932,
      "grad_norm": 0.15968522429466248,
      "learning_rate": 8.947992924445213e-05,
      "loss": 0.0202,
      "step": 1070
    },
    {
      "epoch": 0.584731997834326,
      "grad_norm": 0.019011400640010834,
      "learning_rate": 8.918819498485303e-05,
      "loss": 0.0578,
      "step": 1080
    },
    {
      "epoch": 0.5901461829994585,
      "grad_norm": 1.394095778465271,
      "learning_rate": 8.889296022798941e-05,
      "loss": 0.0776,
      "step": 1090
    },
    {
      "epoch": 0.5955603681645912,
      "grad_norm": 1.5680569410324097,
      "learning_rate": 8.859425134579444e-05,
      "loss": 0.0592,
      "step": 1100
    },
    {
      "epoch": 0.6009745533297238,
      "grad_norm": 1.3112528324127197,
      "learning_rate": 8.829209502052857e-05,
      "loss": 0.0258,
      "step": 1110
    },
    {
      "epoch": 0.6063887384948565,
      "grad_norm": 2.210642099380493,
      "learning_rate": 8.798651824239612e-05,
      "loss": 0.0561,
      "step": 1120
    },
    {
      "epoch": 0.6118029236599891,
      "grad_norm": 1.292048692703247,
      "learning_rate": 8.767754830713439e-05,
      "loss": 0.0252,
      "step": 1130
    },
    {
      "epoch": 0.6172171088251218,
      "grad_norm": 0.140386700630188,
      "learning_rate": 8.73652128135755e-05,
      "loss": 0.037,
      "step": 1140
    },
    {
      "epoch": 0.6226312939902545,
      "grad_norm": 1.0726150274276733,
      "learning_rate": 8.704953966118103e-05,
      "loss": 0.0135,
      "step": 1150
    },
    {
      "epoch": 0.6280454791553871,
      "grad_norm": 0.24958038330078125,
      "learning_rate": 8.673055704754994e-05,
      "loss": 0.0066,
      "step": 1160
    },
    {
      "epoch": 0.6334596643205198,
      "grad_norm": 0.1304091513156891,
      "learning_rate": 8.640829346589979e-05,
      "loss": 0.0314,
      "step": 1170
    },
    {
      "epoch": 0.6388738494856524,
      "grad_norm": 0.013022479601204395,
      "learning_rate": 8.608277770252165e-05,
      "loss": 0.0253,
      "step": 1180
    },
    {
      "epoch": 0.6442880346507851,
      "grad_norm": 2.787271499633789,
      "learning_rate": 8.575403883420867e-05,
      "loss": 0.0144,
      "step": 1190
    },
    {
      "epoch": 0.6497022198159177,
      "grad_norm": 0.09507196396589279,
      "learning_rate": 8.542210622565879e-05,
      "loss": 0.0036,
      "step": 1200
    },
    {
      "epoch": 0.6551164049810504,
      "grad_norm": 0.00226972927339375,
      "learning_rate": 8.508700952685175e-05,
      "loss": 0.0227,
      "step": 1210
    },
    {
      "epoch": 0.660530590146183,
      "grad_norm": 1.0151863098144531,
      "learning_rate": 8.474877867040067e-05,
      "loss": 0.0178,
      "step": 1220
    },
    {
      "epoch": 0.6659447753113157,
      "grad_norm": 22.912487030029297,
      "learning_rate": 8.440744386887818e-05,
      "loss": 0.0485,
      "step": 1230
    },
    {
      "epoch": 0.6713589604764483,
      "grad_norm": 0.984298050403595,
      "learning_rate": 8.406303561211774e-05,
      "loss": 0.0161,
      "step": 1240
    },
    {
      "epoch": 0.676773145641581,
      "grad_norm": 0.3730125427246094,
      "learning_rate": 8.371558466449011e-05,
      "loss": 0.0174,
      "step": 1250
    },
    {
      "epoch": 0.6821873308067136,
      "grad_norm": 0.004657266195863485,
      "learning_rate": 8.336512206215539e-05,
      "loss": 0.0141,
      "step": 1260
    },
    {
      "epoch": 0.6876015159718463,
      "grad_norm": 0.0064207641407847404,
      "learning_rate": 8.301167911029055e-05,
      "loss": 0.0137,
      "step": 1270
    },
    {
      "epoch": 0.6930157011369789,
      "grad_norm": 0.05174608901143074,
      "learning_rate": 8.265528738029324e-05,
      "loss": 0.0223,
      "step": 1280
    },
    {
      "epoch": 0.6984298863021116,
      "grad_norm": 1.818171501159668,
      "learning_rate": 8.229597870696157e-05,
      "loss": 0.0323,
      "step": 1290
    },
    {
      "epoch": 0.7038440714672441,
      "grad_norm": 0.9815323352813721,
      "learning_rate": 8.19337851856505e-05,
      "loss": 0.012,
      "step": 1300
    },
    {
      "epoch": 0.7092582566323768,
      "grad_norm": 2.1632232666015625,
      "learning_rate": 8.156873916940489e-05,
      "loss": 0.0267,
      "step": 1310
    },
    {
      "epoch": 0.7146724417975094,
      "grad_norm": 0.007005088496953249,
      "learning_rate": 8.120087326606957e-05,
      "loss": 0.0126,
      "step": 1320
    },
    {
      "epoch": 0.7200866269626421,
      "grad_norm": 1.1907438039779663,
      "learning_rate": 8.083022033537664e-05,
      "loss": 0.0185,
      "step": 1330
    },
    {
      "epoch": 0.7255008121277747,
      "grad_norm": 1.1393976211547852,
      "learning_rate": 8.045681348601025e-05,
      "loss": 0.0195,
      "step": 1340
    },
    {
      "epoch": 0.7309149972929074,
      "grad_norm": 4.078137397766113,
      "learning_rate": 8.008068607264912e-05,
      "loss": 0.0185,
      "step": 1350
    },
    {
      "epoch": 0.73632918245804,
      "grad_norm": 1.4351844787597656,
      "learning_rate": 7.970187169298724e-05,
      "loss": 0.0047,
      "step": 1360
    },
    {
      "epoch": 0.7417433676231727,
      "grad_norm": 0.6353096961975098,
      "learning_rate": 7.932040418473263e-05,
      "loss": 0.0363,
      "step": 1370
    },
    {
      "epoch": 0.7471575527883053,
      "grad_norm": 0.21844147145748138,
      "learning_rate": 7.89363176225848e-05,
      "loss": 0.046,
      "step": 1380
    },
    {
      "epoch": 0.752571737953438,
      "grad_norm": 0.07143494486808777,
      "learning_rate": 7.854964631519105e-05,
      "loss": 0.0087,
      "step": 1390
    },
    {
      "epoch": 0.7579859231185706,
      "grad_norm": 0.5895802974700928,
      "learning_rate": 7.816042480208183e-05,
      "loss": 0.0115,
      "step": 1400
    },
    {
      "epoch": 0.7634001082837033,
      "grad_norm": 2.6574838161468506,
      "learning_rate": 7.77686878505855e-05,
      "loss": 0.0187,
      "step": 1410
    },
    {
      "epoch": 0.7688142934488359,
      "grad_norm": 0.012055299244821072,
      "learning_rate": 7.737447045272266e-05,
      "loss": 0.0158,
      "step": 1420
    },
    {
      "epoch": 0.7742284786139686,
      "grad_norm": 2.04085373878479,
      "learning_rate": 7.697780782208056e-05,
      "loss": 0.0212,
      "step": 1430
    },
    {
      "epoch": 0.7796426637791013,
      "grad_norm": 0.3383457064628601,
      "learning_rate": 7.657873539066756e-05,
      "loss": 0.0073,
      "step": 1440
    },
    {
      "epoch": 0.7850568489442339,
      "grad_norm": 3.335899829864502,
      "learning_rate": 7.617728880574826e-05,
      "loss": 0.031,
      "step": 1450
    },
    {
      "epoch": 0.7904710341093666,
      "grad_norm": 0.19468721747398376,
      "learning_rate": 7.577350392665918e-05,
      "loss": 0.0076,
      "step": 1460
    },
    {
      "epoch": 0.7958852192744992,
      "grad_norm": 0.0053457473404705524,
      "learning_rate": 7.536741682160571e-05,
      "loss": 0.0054,
      "step": 1470
    },
    {
      "epoch": 0.8012994044396319,
      "grad_norm": 0.6662625074386597,
      "learning_rate": 7.495906376444022e-05,
      "loss": 0.018,
      "step": 1480
    },
    {
      "epoch": 0.8067135896047645,
      "grad_norm": 0.7977598905563354,
      "learning_rate": 7.454848123142197e-05,
      "loss": 0.0079,
      "step": 1490
    },
    {
      "epoch": 0.8121277747698972,
      "grad_norm": 1.777335286140442,
      "learning_rate": 7.41357058979588e-05,
      "loss": 0.0271,
      "step": 1500
    },
    {
      "epoch": 0.8175419599350298,
      "grad_norm": 0.9003738164901733,
      "learning_rate": 7.372077463533109e-05,
      "loss": 0.0113,
      "step": 1510
    },
    {
      "epoch": 0.8229561451001625,
      "grad_norm": 0.0036126982886344194,
      "learning_rate": 7.330372450739829e-05,
      "loss": 0.0183,
      "step": 1520
    },
    {
      "epoch": 0.828370330265295,
      "grad_norm": 1.8423842191696167,
      "learning_rate": 7.288459276728807e-05,
      "loss": 0.0535,
      "step": 1530
    },
    {
      "epoch": 0.8337845154304278,
      "grad_norm": 4.116658687591553,
      "learning_rate": 7.246341685406879e-05,
      "loss": 0.0428,
      "step": 1540
    },
    {
      "epoch": 0.8391987005955603,
      "grad_norm": 0.08053385466337204,
      "learning_rate": 7.204023438940511e-05,
      "loss": 0.0089,
      "step": 1550
    },
    {
      "epoch": 0.844612885760693,
      "grad_norm": 0.004629564005881548,
      "learning_rate": 7.161508317419759e-05,
      "loss": 0.0049,
      "step": 1560
    },
    {
      "epoch": 0.8500270709258256,
      "grad_norm": 0.005140126682817936,
      "learning_rate": 7.118800118520593e-05,
      "loss": 0.0065,
      "step": 1570
    },
    {
      "epoch": 0.8554412560909583,
      "grad_norm": 0.35412144660949707,
      "learning_rate": 7.075902657165687e-05,
      "loss": 0.018,
      "step": 1580
    },
    {
      "epoch": 0.8608554412560909,
      "grad_norm": 0.043654851615428925,
      "learning_rate": 7.032819765183634e-05,
      "loss": 0.0089,
      "step": 1590
    },
    {
      "epoch": 0.8662696264212236,
      "grad_norm": 0.04077580198645592,
      "learning_rate": 6.989555290966673e-05,
      "loss": 0.0049,
      "step": 1600
    },
    {
      "epoch": 0.8716838115863562,
      "grad_norm": 0.008583649061620235,
      "learning_rate": 6.946113099126938e-05,
      "loss": 0.0129,
      "step": 1610
    },
    {
      "epoch": 0.8770979967514889,
      "grad_norm": 0.07225753366947174,
      "learning_rate": 6.902497070151234e-05,
      "loss": 0.0031,
      "step": 1620
    },
    {
      "epoch": 0.8825121819166215,
      "grad_norm": 0.0026265212800353765,
      "learning_rate": 6.858711100054425e-05,
      "loss": 0.0232,
      "step": 1630
    },
    {
      "epoch": 0.8879263670817542,
      "grad_norm": 0.004113343078643084,
      "learning_rate": 6.814759100031414e-05,
      "loss": 0.0006,
      "step": 1640
    },
    {
      "epoch": 0.8933405522468868,
      "grad_norm": 0.006093104835599661,
      "learning_rate": 6.770644996107782e-05,
      "loss": 0.0166,
      "step": 1650
    },
    {
      "epoch": 0.8987547374120195,
      "grad_norm": 0.14034339785575867,
      "learning_rate": 6.726372728789082e-05,
      "loss": 0.0211,
      "step": 1660
    },
    {
      "epoch": 0.9041689225771521,
      "grad_norm": 0.014673382975161076,
      "learning_rate": 6.681946252708869e-05,
      "loss": 0.0127,
      "step": 1670
    },
    {
      "epoch": 0.9095831077422848,
      "grad_norm": 1.5227645635604858,
      "learning_rate": 6.637369536275431e-05,
      "loss": 0.0144,
      "step": 1680
    },
    {
      "epoch": 0.9149972929074174,
      "grad_norm": 0.2898022532463074,
      "learning_rate": 6.592646561317331e-05,
      "loss": 0.0144,
      "step": 1690
    },
    {
      "epoch": 0.9204114780725501,
      "grad_norm": 0.16000008583068848,
      "learning_rate": 6.54778132272771e-05,
      "loss": 0.0217,
      "step": 1700
    },
    {
      "epoch": 0.9258256632376828,
      "grad_norm": 0.07952141016721725,
      "learning_rate": 6.502777828107457e-05,
      "loss": 0.0078,
      "step": 1710
    },
    {
      "epoch": 0.9312398484028154,
      "grad_norm": 0.9451358318328857,
      "learning_rate": 6.45764009740722e-05,
      "loss": 0.0187,
      "step": 1720
    },
    {
      "epoch": 0.9366540335679481,
      "grad_norm": 0.2728198766708374,
      "learning_rate": 6.412372162568322e-05,
      "loss": 0.0144,
      "step": 1730
    },
    {
      "epoch": 0.9420682187330807,
      "grad_norm": 1.0453542470932007,
      "learning_rate": 6.366978067162614e-05,
      "loss": 0.0067,
      "step": 1740
    },
    {
      "epoch": 0.9474824038982134,
      "grad_norm": 0.2527516186237335,
      "learning_rate": 6.321461866031278e-05,
      "loss": 0.0139,
      "step": 1750
    },
    {
      "epoch": 0.952896589063346,
      "grad_norm": 0.56496661901474,
      "learning_rate": 6.275827624922621e-05,
      "loss": 0.0193,
      "step": 1760
    },
    {
      "epoch": 0.9583107742284787,
      "grad_norm": 0.005957538727670908,
      "learning_rate": 6.230079420128911e-05,
      "loss": 0.017,
      "step": 1770
    },
    {
      "epoch": 0.9637249593936112,
      "grad_norm": 0.5009787082672119,
      "learning_rate": 6.184221338122257e-05,
      "loss": 0.0345,
      "step": 1780
    },
    {
      "epoch": 0.969139144558744,
      "grad_norm": 0.3646543622016907,
      "learning_rate": 6.138257475189578e-05,
      "loss": 0.0162,
      "step": 1790
    },
    {
      "epoch": 0.9745533297238765,
      "grad_norm": 1.1433452367782593,
      "learning_rate": 6.092191937066712e-05,
      "loss": 0.0249,
      "step": 1800
    },
    {
      "epoch": 0.9799675148890092,
      "grad_norm": 0.005269546061754227,
      "learning_rate": 6.04602883857166e-05,
      "loss": 0.016,
      "step": 1810
    },
    {
      "epoch": 0.9853817000541418,
      "grad_norm": 0.027783077210187912,
      "learning_rate": 5.9997723032370346e-05,
      "loss": 0.018,
      "step": 1820
    },
    {
      "epoch": 0.9907958852192745,
      "grad_norm": 0.015877699479460716,
      "learning_rate": 5.953426462941718e-05,
      "loss": 0.0064,
      "step": 1830
    },
    {
      "epoch": 0.9962100703844071,
      "grad_norm": 0.7508984208106995,
      "learning_rate": 5.906995457541788e-05,
      "loss": 0.0288,
      "step": 1840
    },
    {
      "epoch": 1.0016242555495398,
      "grad_norm": 0.26469868421554565,
      "learning_rate": 5.8604834345007244e-05,
      "loss": 0.0105,
      "step": 1850
    },
    {
      "epoch": 1.0070384407146724,
      "grad_norm": 0.0750700905919075,
      "learning_rate": 5.8138945485189244e-05,
      "loss": 0.0046,
      "step": 1860
    },
    {
      "epoch": 1.012452625879805,
      "grad_norm": 1.693765640258789,
      "learning_rate": 5.767232961162596e-05,
      "loss": 0.0277,
      "step": 1870
    },
    {
      "epoch": 1.0178668110449378,
      "grad_norm": 0.018022233620285988,
      "learning_rate": 5.720502840492018e-05,
      "loss": 0.0121,
      "step": 1880
    },
    {
      "epoch": 1.0232809962100704,
      "grad_norm": 0.025364132598042488,
      "learning_rate": 5.673708360689226e-05,
      "loss": 0.0026,
      "step": 1890
    },
    {
      "epoch": 1.028695181375203,
      "grad_norm": 0.0128355473279953,
      "learning_rate": 5.6268537016851585e-05,
      "loss": 0.0073,
      "step": 1900
    },
    {
      "epoch": 1.0341093665403356,
      "grad_norm": 1.0056512355804443,
      "learning_rate": 5.579943048786275e-05,
      "loss": 0.0115,
      "step": 1910
    },
    {
      "epoch": 1.0395235517054684,
      "grad_norm": 0.02687026560306549,
      "learning_rate": 5.5329805923007094e-05,
      "loss": 0.0013,
      "step": 1920
    },
    {
      "epoch": 1.044937736870601,
      "grad_norm": 0.8745640516281128,
      "learning_rate": 5.485970527163963e-05,
      "loss": 0.0037,
      "step": 1930
    },
    {
      "epoch": 1.0503519220357336,
      "grad_norm": 0.13542549312114716,
      "learning_rate": 5.438917052564198e-05,
      "loss": 0.0029,
      "step": 1940
    },
    {
      "epoch": 1.0557661072008662,
      "grad_norm": 0.010415283963084221,
      "learning_rate": 5.391824371567137e-05,
      "loss": 0.0106,
      "step": 1950
    },
    {
      "epoch": 1.061180292365999,
      "grad_norm": 0.007145554292947054,
      "learning_rate": 5.344696690740628e-05,
      "loss": 0.0021,
      "step": 1960
    },
    {
      "epoch": 1.0665944775311316,
      "grad_norm": 0.3527683913707733,
      "learning_rate": 5.297538219778889e-05,
      "loss": 0.0022,
      "step": 1970
    },
    {
      "epoch": 1.0720086626962642,
      "grad_norm": 0.0030679586343467236,
      "learning_rate": 5.2503531711264755e-05,
      "loss": 0.0124,
      "step": 1980
    },
    {
      "epoch": 1.077422847861397,
      "grad_norm": 0.07452504336833954,
      "learning_rate": 5.203145759602002e-05,
      "loss": 0.001,
      "step": 1990
    },
    {
      "epoch": 1.0828370330265296,
      "grad_norm": 1.045310378074646,
      "learning_rate": 5.155920202021655e-05,
      "loss": 0.0053,
      "step": 2000
    },
    {
      "epoch": 1.0882512181916622,
      "grad_norm": 0.037452057003974915,
      "learning_rate": 5.108680716822523e-05,
      "loss": 0.0006,
      "step": 2010
    },
    {
      "epoch": 1.0936654033567947,
      "grad_norm": 0.05544527620077133,
      "learning_rate": 5.0614315236857804e-05,
      "loss": 0.0092,
      "step": 2020
    },
    {
      "epoch": 1.0990795885219276,
      "grad_norm": 0.010088340379297733,
      "learning_rate": 5.014176843159768e-05,
      "loss": 0.0008,
      "step": 2030
    },
    {
      "epoch": 1.1044937736870601,
      "grad_norm": 0.011209269985556602,
      "learning_rate": 4.9669208962829924e-05,
      "loss": 0.0085,
      "step": 2040
    },
    {
      "epoch": 1.1099079588521927,
      "grad_norm": 0.0014909579185768962,
      "learning_rate": 4.9196679042070716e-05,
      "loss": 0.0036,
      "step": 2050
    },
    {
      "epoch": 1.1153221440173253,
      "grad_norm": 0.0007312348461709917,
      "learning_rate": 4.872422087819689e-05,
      "loss": 0.0009,
      "step": 2060
    },
    {
      "epoch": 1.1207363291824581,
      "grad_norm": 0.008905081078410149,
      "learning_rate": 4.8251876673675565e-05,
      "loss": 0.0006,
      "step": 2070
    },
    {
      "epoch": 1.1261505143475907,
      "grad_norm": 0.18999351561069489,
      "learning_rate": 4.777968862079439e-05,
      "loss": 0.003,
      "step": 2080
    },
    {
      "epoch": 1.1315646995127233,
      "grad_norm": 0.17193354666233063,
      "learning_rate": 4.730769889789276e-05,
      "loss": 0.0146,
      "step": 2090
    },
    {
      "epoch": 1.136978884677856,
      "grad_norm": 0.6419105529785156,
      "learning_rate": 4.6835949665594144e-05,
      "loss": 0.0008,
      "step": 2100
    },
    {
      "epoch": 1.1423930698429887,
      "grad_norm": 0.5692811012268066,
      "learning_rate": 4.6364483063040136e-05,
      "loss": 0.0013,
      "step": 2110
    },
    {
      "epoch": 1.1478072550081213,
      "grad_norm": 0.0003832950606010854,
      "learning_rate": 4.589334120412635e-05,
      "loss": 0.011,
      "step": 2120
    },
    {
      "epoch": 1.153221440173254,
      "grad_norm": 0.06042022258043289,
      "learning_rate": 4.542256617374056e-05,
      "loss": 0.013,
      "step": 2130
    },
    {
      "epoch": 1.1586356253383865,
      "grad_norm": 0.22592328488826752,
      "learning_rate": 4.4952200024003484e-05,
      "loss": 0.0082,
      "step": 2140
    },
    {
      "epoch": 1.1640498105035193,
      "grad_norm": 0.0038675295654684305,
      "learning_rate": 4.4482284770512474e-05,
      "loss": 0.0118,
      "step": 2150
    },
    {
      "epoch": 1.169463995668652,
      "grad_norm": 0.5443112850189209,
      "learning_rate": 4.401286238858837e-05,
      "loss": 0.003,
      "step": 2160
    },
    {
      "epoch": 1.1748781808337845,
      "grad_norm": 0.9986633062362671,
      "learning_rate": 4.354397480952617e-05,
      "loss": 0.0011,
      "step": 2170
    },
    {
      "epoch": 1.180292365998917,
      "grad_norm": 0.49970999360084534,
      "learning_rate": 4.307566391684944e-05,
      "loss": 0.0208,
      "step": 2180
    },
    {
      "epoch": 1.1857065511640499,
      "grad_norm": 0.0017699070740491152,
      "learning_rate": 4.260797154256905e-05,
      "loss": 0.0117,
      "step": 2190
    },
    {
      "epoch": 1.1911207363291825,
      "grad_norm": 0.01859360747039318,
      "learning_rate": 4.214093946344655e-05,
      "loss": 0.0143,
      "step": 2200
    },
    {
      "epoch": 1.196534921494315,
      "grad_norm": 1.7023776769638062,
      "learning_rate": 4.167460939726239e-05,
      "loss": 0.0385,
      "step": 2210
    },
    {
      "epoch": 1.2019491066594479,
      "grad_norm": 0.005938851274549961,
      "learning_rate": 4.120902299908953e-05,
      "loss": 0.004,
      "step": 2220
    },
    {
      "epoch": 1.2073632918245805,
      "grad_norm": 0.16613930463790894,
      "learning_rate": 4.0744221857572564e-05,
      "loss": 0.0045,
      "step": 2230
    },
    {
      "epoch": 1.212777476989713,
      "grad_norm": 0.0063484469428658485,
      "learning_rate": 4.028024749121279e-05,
      "loss": 0.0019,
      "step": 2240
    },
    {
      "epoch": 1.2181916621548456,
      "grad_norm": 0.030094869434833527,
      "learning_rate": 3.981714134465956e-05,
      "loss": 0.0021,
      "step": 2250
    },
    {
      "epoch": 1.2236058473199782,
      "grad_norm": 0.0037768217734992504,
      "learning_rate": 3.935494478500824e-05,
      "loss": 0.0026,
      "step": 2260
    },
    {
      "epoch": 1.229020032485111,
      "grad_norm": 0.0034660936798900366,
      "learning_rate": 3.8893699098105084e-05,
      "loss": 0.0035,
      "step": 2270
    },
    {
      "epoch": 1.2344342176502436,
      "grad_norm": 0.0008792284061200917,
      "learning_rate": 3.8433445484859345e-05,
      "loss": 0.0071,
      "step": 2280
    },
    {
      "epoch": 1.2398484028153762,
      "grad_norm": 0.03415992483496666,
      "learning_rate": 3.797422505756298e-05,
      "loss": 0.0031,
      "step": 2290
    },
    {
      "epoch": 1.245262587980509,
      "grad_norm": 0.02024753950536251,
      "learning_rate": 3.7516078836218333e-05,
      "loss": 0.026,
      "step": 2300
    },
    {
      "epoch": 1.2506767731456416,
      "grad_norm": 1.17003333568573,
      "learning_rate": 3.705904774487396e-05,
      "loss": 0.0084,
      "step": 2310
    },
    {
      "epoch": 1.2560909583107742,
      "grad_norm": 0.0014685009373351932,
      "learning_rate": 3.6603172607969105e-05,
      "loss": 0.0024,
      "step": 2320
    },
    {
      "epoch": 1.2615051434759068,
      "grad_norm": 0.00356222759000957,
      "learning_rate": 3.6148494146687016e-05,
      "loss": 0.0084,
      "step": 2330
    },
    {
      "epoch": 1.2669193286410394,
      "grad_norm": 0.16045640408992767,
      "learning_rate": 3.569505297531757e-05,
      "loss": 0.0023,
      "step": 2340
    },
    {
      "epoch": 1.2723335138061722,
      "grad_norm": 0.06978600472211838,
      "learning_rate": 3.5242889597629325e-05,
      "loss": 0.0068,
      "step": 2350
    },
    {
      "epoch": 1.2777476989713048,
      "grad_norm": 0.030386654660105705,
      "learning_rate": 3.479204440325151e-05,
      "loss": 0.0004,
      "step": 2360
    },
    {
      "epoch": 1.2831618841364374,
      "grad_norm": 0.027763698250055313,
      "learning_rate": 3.434255766406629e-05,
      "loss": 0.0009,
      "step": 2370
    },
    {
      "epoch": 1.2885760693015702,
      "grad_norm": 0.0032124249264597893,
      "learning_rate": 3.389446953061137e-05,
      "loss": 0.0176,
      "step": 2380
    },
    {
      "epoch": 1.2939902544667028,
      "grad_norm": 0.00499959709122777,
      "learning_rate": 3.3447820028493595e-05,
      "loss": 0.0074,
      "step": 2390
    },
    {
      "epoch": 1.2994044396318354,
      "grad_norm": 0.01623588055372238,
      "learning_rate": 3.300264905481365e-05,
      "loss": 0.0044,
      "step": 2400
    },
    {
      "epoch": 1.304818624796968,
      "grad_norm": 0.005087947007268667,
      "learning_rate": 3.255899637460221e-05,
      "loss": 0.0008,
      "step": 2410
    },
    {
      "epoch": 1.3102328099621008,
      "grad_norm": 0.027212021872401237,
      "learning_rate": 3.2116901617267915e-05,
      "loss": 0.0082,
      "step": 2420
    },
    {
      "epoch": 1.3156469951272334,
      "grad_norm": 0.0023790805134922266,
      "learning_rate": 3.167640427305748e-05,
      "loss": 0.0143,
      "step": 2430
    },
    {
      "epoch": 1.321061180292366,
      "grad_norm": 0.007598313502967358,
      "learning_rate": 3.123754368952825e-05,
      "loss": 0.0013,
      "step": 2440
    },
    {
      "epoch": 1.3264753654574988,
      "grad_norm": 0.0005349611747078598,
      "learning_rate": 3.08003590680334e-05,
      "loss": 0.012,
      "step": 2450
    },
    {
      "epoch": 1.3318895506226314,
      "grad_norm": 0.014023731462657452,
      "learning_rate": 3.0364889460220293e-05,
      "loss": 0.0004,
      "step": 2460
    },
    {
      "epoch": 1.337303735787764,
      "grad_norm": 0.026530081406235695,
      "learning_rate": 2.9931173764542186e-05,
      "loss": 0.0104,
      "step": 2470
    },
    {
      "epoch": 1.3427179209528965,
      "grad_norm": 0.43288663029670715,
      "learning_rate": 2.949925072278363e-05,
      "loss": 0.0194,
      "step": 2480
    },
    {
      "epoch": 1.3481321061180291,
      "grad_norm": 1.2451562881469727,
      "learning_rate": 2.906915891659979e-05,
      "loss": 0.0033,
      "step": 2490
    },
    {
      "epoch": 1.353546291283162,
      "grad_norm": 0.005054580979049206,
      "learning_rate": 2.864093676407018e-05,
      "loss": 0.0032,
      "step": 2500
    },
    {
      "epoch": 1.3589604764482945,
      "grad_norm": 0.15100334584712982,
      "learning_rate": 2.821462251626697e-05,
      "loss": 0.0011,
      "step": 2510
    },
    {
      "epoch": 1.3643746616134271,
      "grad_norm": 0.012098588049411774,
      "learning_rate": 2.77902542538381e-05,
      "loss": 0.0029,
      "step": 2520
    },
    {
      "epoch": 1.36978884677856,
      "grad_norm": 0.0038488046266138554,
      "learning_rate": 2.736786988360587e-05,
      "loss": 0.0041,
      "step": 2530
    },
    {
      "epoch": 1.3752030319436925,
      "grad_norm": 0.04221956431865692,
      "learning_rate": 2.6947507135180744e-05,
      "loss": 0.0079,
      "step": 2540
    },
    {
      "epoch": 1.3806172171088251,
      "grad_norm": 5.0392937660217285,
      "learning_rate": 2.6529203557591244e-05,
      "loss": 0.0078,
      "step": 2550
    },
    {
      "epoch": 1.3860314022739577,
      "grad_norm": 0.020177649334073067,
      "learning_rate": 2.611299651592981e-05,
      "loss": 0.0031,
      "step": 2560
    },
    {
      "epoch": 1.3914455874390903,
      "grad_norm": 0.002122863195836544,
      "learning_rate": 2.5698923188015244e-05,
      "loss": 0.0056,
      "step": 2570
    },
    {
      "epoch": 1.3968597726042231,
      "grad_norm": 8.019600868225098,
      "learning_rate": 2.528702056107165e-05,
      "loss": 0.0181,
      "step": 2580
    },
    {
      "epoch": 1.4022739577693557,
      "grad_norm": 0.647739052772522,
      "learning_rate": 2.4877325428424702e-05,
      "loss": 0.0038,
      "step": 2590
    },
    {
      "epoch": 1.4076881429344883,
      "grad_norm": 0.28581151366233826,
      "learning_rate": 2.4469874386214898e-05,
      "loss": 0.013,
      "step": 2600
    },
    {
      "epoch": 1.413102328099621,
      "grad_norm": 0.0012231934815645218,
      "learning_rate": 2.4064703830128753e-05,
      "loss": 0.0119,
      "step": 2610
    },
    {
      "epoch": 1.4185165132647537,
      "grad_norm": 0.001883997581899166,
      "learning_rate": 2.366184995214762e-05,
      "loss": 0.002,
      "step": 2620
    },
    {
      "epoch": 1.4239306984298863,
      "grad_norm": 0.0037328582257032394,
      "learning_rate": 2.3261348737314952e-05,
      "loss": 0.0057,
      "step": 2630
    },
    {
      "epoch": 1.4293448835950189,
      "grad_norm": 0.00040012079989537597,
      "learning_rate": 2.2863235960521793e-05,
      "loss": 0.0031,
      "step": 2640
    },
    {
      "epoch": 1.4347590687601515,
      "grad_norm": 1.9957863092422485,
      "learning_rate": 2.2467547183311272e-05,
      "loss": 0.0037,
      "step": 2650
    },
    {
      "epoch": 1.4401732539252843,
      "grad_norm": 0.21706080436706543,
      "learning_rate": 2.207431775070205e-05,
      "loss": 0.0179,
      "step": 2660
    },
    {
      "epoch": 1.4455874390904169,
      "grad_norm": 0.008977403864264488,
      "learning_rate": 2.1683582788031042e-05,
      "loss": 0.0058,
      "step": 2670
    },
    {
      "epoch": 1.4510016242555495,
      "grad_norm": 0.00040086504304781556,
      "learning_rate": 2.1295377197815963e-05,
      "loss": 0.0125,
      "step": 2680
    },
    {
      "epoch": 1.4564158094206823,
      "grad_norm": 0.09992031008005142,
      "learning_rate": 2.0909735656637484e-05,
      "loss": 0.0025,
      "step": 2690
    },
    {
      "epoch": 1.4618299945858149,
      "grad_norm": 0.172300785779953,
      "learning_rate": 2.052669261204192e-05,
      "loss": 0.0016,
      "step": 2700
    },
    {
      "epoch": 1.4672441797509475,
      "grad_norm": 0.004767312668263912,
      "learning_rate": 2.0146282279464018e-05,
      "loss": 0.0013,
      "step": 2710
    },
    {
      "epoch": 1.47265836491608,
      "grad_norm": 0.34872570633888245,
      "learning_rate": 1.9768538639170804e-05,
      "loss": 0.0008,
      "step": 2720
    },
    {
      "epoch": 1.4780725500812129,
      "grad_norm": 0.4164210259914398,
      "learning_rate": 1.939349543322616e-05,
      "loss": 0.0004,
      "step": 2730
    },
    {
      "epoch": 1.4834867352463454,
      "grad_norm": 0.7618734240531921,
      "learning_rate": 1.90211861624769e-05,
      "loss": 0.0034,
      "step": 2740
    },
    {
      "epoch": 1.488900920411478,
      "grad_norm": 0.0021833758801221848,
      "learning_rate": 1.865164408356021e-05,
      "loss": 0.0103,
      "step": 2750
    },
    {
      "epoch": 1.4943151055766108,
      "grad_norm": 0.44284117221832275,
      "learning_rate": 1.8284902205933114e-05,
      "loss": 0.0061,
      "step": 2760
    },
    {
      "epoch": 1.4997292907417434,
      "grad_norm": 0.9600495100021362,
      "learning_rate": 1.7920993288923722e-05,
      "loss": 0.0008,
      "step": 2770
    },
    {
      "epoch": 1.505143475906876,
      "grad_norm": 0.010129008442163467,
      "learning_rate": 1.755994983880518e-05,
      "loss": 0.0032,
      "step": 2780
    },
    {
      "epoch": 1.5105576610720086,
      "grad_norm": 0.29140201210975647,
      "learning_rate": 1.720180410589186e-05,
      "loss": 0.0091,
      "step": 2790
    },
    {
      "epoch": 1.5159718462371412,
      "grad_norm": 0.015046076849102974,
      "learning_rate": 1.6846588081658753e-05,
      "loss": 0.0003,
      "step": 2800
    },
    {
      "epoch": 1.5213860314022738,
      "grad_norm": 1.2843390703201294,
      "learning_rate": 1.649433349588369e-05,
      "loss": 0.0074,
      "step": 2810
    },
    {
      "epoch": 1.5268002165674066,
      "grad_norm": 0.05821951851248741,
      "learning_rate": 1.6145071813813124e-05,
      "loss": 0.0115,
      "step": 2820
    },
    {
      "epoch": 1.5322144017325392,
      "grad_norm": 1.5211820602416992,
      "learning_rate": 1.5798834233351534e-05,
      "loss": 0.0037,
      "step": 2830
    },
    {
      "epoch": 1.537628586897672,
      "grad_norm": 0.0018208596156910062,
      "learning_rate": 1.5455651682274564e-05,
      "loss": 0.0039,
      "step": 2840
    },
    {
      "epoch": 1.5430427720628046,
      "grad_norm": 0.4173990488052368,
      "learning_rate": 1.5115554815466488e-05,
      "loss": 0.0047,
      "step": 2850
    },
    {
      "epoch": 1.5484569572279372,
      "grad_norm": 0.00475574703887105,
      "learning_rate": 1.4778574012181857e-05,
      "loss": 0.0061,
      "step": 2860
    },
    {
      "epoch": 1.5538711423930698,
      "grad_norm": 0.08495434373617172,
      "learning_rate": 1.4444739373331973e-05,
      "loss": 0.0109,
      "step": 2870
    },
    {
      "epoch": 1.5592853275582024,
      "grad_norm": 0.07049257308244705,
      "learning_rate": 1.411408071879599e-05,
      "loss": 0.0021,
      "step": 2880
    },
    {
      "epoch": 1.5646995127233352,
      "grad_norm": 0.0006400761776603758,
      "learning_rate": 1.3786627584757383e-05,
      "loss": 0.0018,
      "step": 2890
    },
    {
      "epoch": 1.5701136978884678,
      "grad_norm": 0.7941042184829712,
      "learning_rate": 1.346240922106548e-05,
      "loss": 0.0082,
      "step": 2900
    },
    {
      "epoch": 1.5755278830536006,
      "grad_norm": 0.012076081708073616,
      "learning_rate": 1.3141454588622836e-05,
      "loss": 0.0017,
      "step": 2910
    },
    {
      "epoch": 1.5809420682187332,
      "grad_norm": 0.46522969007492065,
      "learning_rate": 1.2823792356798181e-05,
      "loss": 0.0021,
      "step": 2920
    },
    {
      "epoch": 1.5863562533838658,
      "grad_norm": 0.024888111278414726,
      "learning_rate": 1.2509450900865638e-05,
      "loss": 0.0016,
      "step": 2930
    },
    {
      "epoch": 1.5917704385489984,
      "grad_norm": 0.013893131166696548,
      "learning_rate": 1.2198458299469929e-05,
      "loss": 0.0004,
      "step": 2940
    },
    {
      "epoch": 1.597184623714131,
      "grad_norm": 0.004099879879504442,
      "learning_rate": 1.1890842332118452e-05,
      "loss": 0.0057,
      "step": 2950
    },
    {
      "epoch": 1.6025988088792635,
      "grad_norm": 0.02608601748943329,
      "learning_rate": 1.1586630476699667e-05,
      "loss": 0.0019,
      "step": 2960
    },
    {
      "epoch": 1.6080129940443963,
      "grad_norm": 0.017944563180208206,
      "learning_rate": 1.1285849907028794e-05,
      "loss": 0.0079,
      "step": 2970
    },
    {
      "epoch": 1.613427179209529,
      "grad_norm": 0.04556708037853241,
      "learning_rate": 1.0988527490420359e-05,
      "loss": 0.0153,
      "step": 2980
    },
    {
      "epoch": 1.6188413643746618,
      "grad_norm": 0.00997182447463274,
      "learning_rate": 1.0694689785288343e-05,
      "loss": 0.0012,
      "step": 2990
    },
    {
      "epoch": 1.6242555495397943,
      "grad_norm": 0.009032734669744968,
      "learning_rate": 1.0404363038773867e-05,
      "loss": 0.008,
      "step": 3000
    },
    {
      "epoch": 1.629669734704927,
      "grad_norm": 0.0009320198441855609,
      "learning_rate": 1.0117573184400574e-05,
      "loss": 0.0037,
      "step": 3010
    },
    {
      "epoch": 1.6350839198700595,
      "grad_norm": 0.0004226955061312765,
      "learning_rate": 9.834345839758196e-06,
      "loss": 0.0036,
      "step": 3020
    },
    {
      "epoch": 1.6404981050351921,
      "grad_norm": 0.01206124946475029,
      "learning_rate": 9.554706304214183e-06,
      "loss": 0.0036,
      "step": 3030
    },
    {
      "epoch": 1.6459122902003247,
      "grad_norm": 0.16217440366744995,
      "learning_rate": 9.278679556653885e-06,
      "loss": 0.01,
      "step": 3040
    },
    {
      "epoch": 1.6513264753654575,
      "grad_norm": 0.0007590691675432026,
      "learning_rate": 9.006290253249261e-06,
      "loss": 0.0014,
      "step": 3050
    },
    {
      "epoch": 1.65674066053059,
      "grad_norm": 0.10210487991571426,
      "learning_rate": 8.737562725256505e-06,
      "loss": 0.0101,
      "step": 3060
    },
    {
      "epoch": 1.662154845695723,
      "grad_norm": 0.0020508214365690947,
      "learning_rate": 8.472520976842597e-06,
      "loss": 0.0014,
      "step": 3070
    },
    {
      "epoch": 1.6675690308608555,
      "grad_norm": 0.010141194798052311,
      "learning_rate": 8.211188682941184e-06,
      "loss": 0.0062,
      "step": 3080
    },
    {
      "epoch": 1.672983216025988,
      "grad_norm": 0.002423088764771819,
      "learning_rate": 7.95358918713774e-06,
      "loss": 0.0004,
      "step": 3090
    },
    {
      "epoch": 1.6783974011911207,
      "grad_norm": 0.0029966579750180244,
      "learning_rate": 7.699745499584466e-06,
      "loss": 0.0044,
      "step": 3100
    },
    {
      "epoch": 1.6838115863562533,
      "grad_norm": 1.5740716457366943,
      "learning_rate": 7.449680294944844e-06,
      "loss": 0.0043,
      "step": 3110
    },
    {
      "epoch": 1.6892257715213859,
      "grad_norm": 0.005612372886389494,
      "learning_rate": 7.203415910368233e-06,
      "loss": 0.0032,
      "step": 3120
    },
    {
      "epoch": 1.6946399566865187,
      "grad_norm": 4.183136940002441,
      "learning_rate": 6.9609743434945975e-06,
      "loss": 0.0153,
      "step": 3130
    },
    {
      "epoch": 1.7000541418516515,
      "grad_norm": 0.011109275743365288,
      "learning_rate": 6.722377250489603e-06,
      "loss": 0.0022,
      "step": 3140
    },
    {
      "epoch": 1.705468327016784,
      "grad_norm": 0.0008483441779389977,
      "learning_rate": 6.487645944110093e-06,
      "loss": 0.0013,
      "step": 3150
    },
    {
      "epoch": 1.7108825121819167,
      "grad_norm": 0.35129183530807495,
      "learning_rate": 6.256801391800365e-06,
      "loss": 0.0047,
      "step": 3160
    },
    {
      "epoch": 1.7162966973470493,
      "grad_norm": 0.161061629652977,
      "learning_rate": 6.029864213819275e-06,
      "loss": 0.0004,
      "step": 3170
    },
    {
      "epoch": 1.7217108825121819,
      "grad_norm": 0.010923434980213642,
      "learning_rate": 5.80685468139826e-06,
      "loss": 0.0003,
      "step": 3180
    },
    {
      "epoch": 1.7271250676773144,
      "grad_norm": 1.1401299238204956,
      "learning_rate": 5.587792714930668e-06,
      "loss": 0.0041,
      "step": 3190
    },
    {
      "epoch": 1.7325392528424473,
      "grad_norm": 0.01541453879326582,
      "learning_rate": 5.372697882192301e-06,
      "loss": 0.0047,
      "step": 3200
    },
    {
      "epoch": 1.7379534380075798,
      "grad_norm": 0.031119931489229202,
      "learning_rate": 5.161589396593597e-06,
      "loss": 0.0031,
      "step": 3210
    },
    {
      "epoch": 1.7433676231727127,
      "grad_norm": 0.012519028969109058,
      "learning_rate": 4.954486115463303e-06,
      "loss": 0.0016,
      "step": 3220
    },
    {
      "epoch": 1.7487818083378452,
      "grad_norm": 0.0014455881901085377,
      "learning_rate": 4.751406538364095e-06,
      "loss": 0.0002,
      "step": 3230
    },
    {
      "epoch": 1.7541959935029778,
      "grad_norm": 1.0400147438049316,
      "learning_rate": 4.552368805440066e-06,
      "loss": 0.019,
      "step": 3240
    },
    {
      "epoch": 1.7596101786681104,
      "grad_norm": 0.023476798087358475,
      "learning_rate": 4.357390695796393e-06,
      "loss": 0.0018,
      "step": 3250
    },
    {
      "epoch": 1.765024363833243,
      "grad_norm": 0.025158202275633812,
      "learning_rate": 4.166489625911146e-06,
      "loss": 0.0014,
      "step": 3260
    },
    {
      "epoch": 1.7704385489983756,
      "grad_norm": 0.0046820444986224174,
      "learning_rate": 3.979682648079641e-06,
      "loss": 0.0045,
      "step": 3270
    },
    {
      "epoch": 1.7758527341635084,
      "grad_norm": 0.020878376439213753,
      "learning_rate": 3.7969864488911598e-06,
      "loss": 0.0003,
      "step": 3280
    },
    {
      "epoch": 1.781266919328641,
      "grad_norm": 0.016544997692108154,
      "learning_rate": 3.6184173477384563e-06,
      "loss": 0.0002,
      "step": 3290
    },
    {
      "epoch": 1.7866811044937738,
      "grad_norm": 0.5539665818214417,
      "learning_rate": 3.443991295360033e-06,
      "loss": 0.001,
      "step": 3300
    },
    {
      "epoch": 1.7920952896589064,
      "grad_norm": 0.0030527780763804913,
      "learning_rate": 3.2737238724153073e-06,
      "loss": 0.0046,
      "step": 3310
    },
    {
      "epoch": 1.797509474824039,
      "grad_norm": 0.6723085641860962,
      "learning_rate": 3.1076302880928664e-06,
      "loss": 0.0015,
      "step": 3320
    },
    {
      "epoch": 1.8029236599891716,
      "grad_norm": 0.002620289335027337,
      "learning_rate": 2.9457253787519124e-06,
      "loss": 0.0006,
      "step": 3330
    },
    {
      "epoch": 1.8083378451543042,
      "grad_norm": 1.7662423849105835,
      "learning_rate": 2.7880236065970287e-06,
      "loss": 0.0067,
      "step": 3340
    },
    {
      "epoch": 1.8137520303194368,
      "grad_norm": 0.005312061868607998,
      "learning_rate": 2.6345390583862707e-06,
      "loss": 0.0004,
      "step": 3350
    },
    {
      "epoch": 1.8191662154845696,
      "grad_norm": 0.30299705266952515,
      "learning_rate": 2.4852854441729246e-06,
      "loss": 0.0023,
      "step": 3360
    },
    {
      "epoch": 1.8245804006497022,
      "grad_norm": 0.8743293881416321,
      "learning_rate": 2.340276096080818e-06,
      "loss": 0.0008,
      "step": 3370
    },
    {
      "epoch": 1.829994585814835,
      "grad_norm": 1.6160459518432617,
      "learning_rate": 2.1995239671134483e-06,
      "loss": 0.017,
      "step": 3380
    },
    {
      "epoch": 1.8354087709799676,
      "grad_norm": 0.007568253669887781,
      "learning_rate": 2.063041629996909e-06,
      "loss": 0.0014,
      "step": 3390
    },
    {
      "epoch": 1.8408229561451002,
      "grad_norm": 1.250002384185791,
      "learning_rate": 1.9308412760568995e-06,
      "loss": 0.006,
      "step": 3400
    },
    {
      "epoch": 1.8462371413102328,
      "grad_norm": 0.005619577597826719,
      "learning_rate": 1.8029347141296505e-06,
      "loss": 0.0042,
      "step": 3410
    },
    {
      "epoch": 1.8516513264753653,
      "grad_norm": 0.00218252488411963,
      "learning_rate": 1.6793333695071578e-06,
      "loss": 0.0054,
      "step": 3420
    },
    {
      "epoch": 1.8570655116404982,
      "grad_norm": 0.3127131462097168,
      "learning_rate": 1.5600482829165764e-06,
      "loss": 0.0072,
      "step": 3430
    },
    {
      "epoch": 1.8624796968056307,
      "grad_norm": 0.3204992711544037,
      "learning_rate": 1.4450901095340252e-06,
      "loss": 0.0065,
      "step": 3440
    },
    {
      "epoch": 1.8678938819707636,
      "grad_norm": 0.006390782073140144,
      "learning_rate": 1.334469118032805e-06,
      "loss": 0.002,
      "step": 3450
    },
    {
      "epoch": 1.8733080671358961,
      "grad_norm": 0.023697618395090103,
      "learning_rate": 1.228195189666137e-06,
      "loss": 0.0004,
      "step": 3460
    },
    {
      "epoch": 1.8787222523010287,
      "grad_norm": 0.013372454792261124,
      "learning_rate": 1.1262778173845356e-06,
      "loss": 0.0004,
      "step": 3470
    },
    {
      "epoch": 1.8841364374661613,
      "grad_norm": 0.015291068702936172,
      "learning_rate": 1.0287261049878317e-06,
      "loss": 0.0001,
      "step": 3480
    },
    {
      "epoch": 1.889550622631294,
      "grad_norm": 0.028584564104676247,
      "learning_rate": 9.355487663119778e-07,
      "loss": 0.009,
      "step": 3490
    },
    {
      "epoch": 1.8949648077964265,
      "grad_norm": 0.0720379427075386,
      "learning_rate": 8.467541244506771e-07,
      "loss": 0.0013,
      "step": 3500
    },
    {
      "epoch": 1.9003789929615593,
      "grad_norm": 0.03005949594080448,
      "learning_rate": 7.623501110119446e-07,
      "loss": 0.0128,
      "step": 3510
    },
    {
      "epoch": 1.905793178126692,
      "grad_norm": 0.003077290253713727,
      "learning_rate": 6.823442654095679e-07,
      "loss": 0.0007,
      "step": 3520
    },
    {
      "epoch": 1.9112073632918247,
      "grad_norm": 0.00358296325430274,
      "learning_rate": 6.067437341896964e-07,
      "loss": 0.0022,
      "step": 3530
    },
    {
      "epoch": 1.9166215484569573,
      "grad_norm": 0.006437402684241533,
      "learning_rate": 5.35555270392446e-07,
      "loss": 0.0029,
      "step": 3540
    },
    {
      "epoch": 1.92203573362209,
      "grad_norm": 0.0994802862405777,
      "learning_rate": 4.6878523294868195e-07,
      "loss": 0.0012,
      "step": 3550
    },
    {
      "epoch": 1.9274499187872225,
      "grad_norm": 0.002565504051744938,
      "learning_rate": 4.064395861120118e-07,
      "loss": 0.0023,
      "step": 3560
    },
    {
      "epoch": 1.932864103952355,
      "grad_norm": 0.48547863960266113,
      "learning_rate": 3.4852389892603974e-07,
      "loss": 0.0033,
      "step": 3570
    },
    {
      "epoch": 1.9382782891174877,
      "grad_norm": 0.003349961480125785,
      "learning_rate": 2.950433447268752e-07,
      "loss": 0.0006,
      "step": 3580
    },
    {
      "epoch": 1.9436924742826205,
      "grad_norm": 0.6246022582054138,
      "learning_rate": 2.4600270068105833e-07,
      "loss": 0.0024,
      "step": 3590
    },
    {
      "epoch": 1.949106659447753,
      "grad_norm": 1.4567086696624756,
      "learning_rate": 2.014063473588179e-07,
      "loss": 0.014,
      "step": 3600
    },
    {
      "epoch": 1.9545208446128859,
      "grad_norm": 0.004240727983415127,
      "learning_rate": 1.6125826834278435e-07,
      "loss": 0.0051,
      "step": 3610
    },
    {
      "epoch": 1.9599350297780185,
      "grad_norm": 0.0023818009067326784,
      "learning_rate": 1.255620498721466e-07,
      "loss": 0.0013,
      "step": 3620
    },
    {
      "epoch": 1.965349214943151,
      "grad_norm": 0.016563531011343002,
      "learning_rate": 9.432088052232501e-08,
      "loss": 0.0042,
      "step": 3630
    },
    {
      "epoch": 1.9707634001082837,
      "grad_norm": 0.21583430469036102,
      "learning_rate": 6.753755092013814e-08,
      "loss": 0.0009,
      "step": 3640
    },
    {
      "epoch": 1.9761775852734162,
      "grad_norm": 0.0016139623476192355,
      "learning_rate": 4.5214453494524244e-08,
      "loss": 0.0031,
      "step": 3650
    },
    {
      "epoch": 1.9815917704385488,
      "grad_norm": 0.2847749888896942,
      "learning_rate": 2.7353582262862332e-08,
      "loss": 0.0003,
      "step": 3660
    },
    {
      "epoch": 1.9870059556036817,
      "grad_norm": 0.6400159597396851,
      "learning_rate": 1.3956532652820153e-08,
      "loss": 0.0047,
      "step": 3670
    },
    {
      "epoch": 1.9924201407688142,
      "grad_norm": 0.028468770906329155,
      "learning_rate": 5.024501359879308e-09,
      "loss": 0.0009,
      "step": 3680
    },
    {
      "epoch": 1.997834325933947,
      "grad_norm": 1.2700254917144775,
      "learning_rate": 5.582862404041045e-10,
      "loss": 0.0057,
      "step": 3690
    }
  ],
  "logging_steps": 10,
  "max_steps": 3694,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 2000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3087375146342154e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
