{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,4'\n",
    "    \n",
    "dataset = load_dataset('/data/home/wangys/DataSelection-IF/datasets/events_classification_biotech',trust_remote_code=True) \n",
    "    \n",
    "classes = [class_ for class_ in dataset['train'].features['label 1'].names if class_]\n",
    "class2id = {class_:id for id, class_ in enumerate(classes)}\n",
    "id2class = {id:class_ for class_, id in class2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(dataset,'../datasets/events_classification.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = '/data/home/wangys/model/deberta-v3-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('../datasets/events_classification_biotech/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "   text = f\"{example['title']}.\\n{example['content']}\"\n",
    "#    print(example['all_labels'])\n",
    "   if isinstance(example['all_labels'],str):\n",
    "       all_labels = example['all_labels'].split(', ')\n",
    "   else:\n",
    "       all_labels = example['all_labels'] \n",
    "   labels = [0. for i in range(len(classes))]\n",
    "   for label in all_labels:\n",
    "       label_id = class2id[label]\n",
    "       labels[label_id] = 1.\n",
    "  \n",
    "   example = tokenizer(text, truncation=True)\n",
    "   example['labels'] = labels\n",
    "   return example\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "clf_metrics = evaluate.combine([\"../../evaluate-main/metrics/accuracy\", \"../../evaluate-main/metrics/f1\", \"../../evaluate-main/metrics/precision\", \"../../evaluate-main/metrics/recall\"])\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1/(1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "   predictions, labels = eval_pred\n",
    "   predictions = sigmoid(predictions)\n",
    "   predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "   return clf_metrics.compute(predictions=predictions, \n",
    "                              references=labels.astype(int).reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at ../../model/ModernBERT-base/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\n",
    "   '../../model/ModernBERT-base/', num_labels=len(classes),\n",
    "           id2label=id2class, label2id=class2id,\n",
    "                       problem_type = \"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /data/home/wangys/model/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'TaskType' has no attribute 'SEQUENCE_CLASSIFICATION'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_config, get_peft_model, LoraConfig, TaskType\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m    model_path, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes),\n\u001b[1;32m      7\u001b[0m            id2label\u001b[38;5;241m=\u001b[39mid2class, label2id\u001b[38;5;241m=\u001b[39mclass2id,\n\u001b[1;32m      8\u001b[0m                        problem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m---> 10\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[43mTaskType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEQUENCE_CLASSIFICATION\u001b[49m,\n\u001b[1;32m     11\u001b[0m     inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# è®¾ç½®ä¸ºFalseç”¨äºŽè®­ç»ƒæ¨¡å¼ï¼Œè®­ç»ƒæ—¶å¯æ›´æ–°LoRAå‚æ•°\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,  \u001b[38;5;66;03m# LoRAçš„ç§©ï¼Œå¯æ ¹æ®å®žé™…æƒ…å†µè°ƒæ•´\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# å¯ä»¥è°ƒæ•´ï¼Œå½±å“è®­ç»ƒè¿‡ç¨‹\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,  \u001b[38;5;66;03m# å¯ä»¥è°ƒæ•´ï¼Œä¸€èˆ¬æ˜¯è¾ƒå°çš„å€¼ï¼Œå¦‚0.1ç­‰\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# è¦åº”ç”¨LoRAçš„æ¨¡å—ï¼Œéœ€æ ¹æ®æ¨¡åž‹ç»“æž„è°ƒæ•´ï¼Œæ¯”å¦‚é’ˆå¯¹DeBERTaçš„ç›¸å…³æ³¨æ„åŠ›æ¨¡å—ç­‰\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# åº”ç”¨LoRAé…ç½®åˆ°æ¨¡åž‹ä¸Š\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, peft_config)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TaskType' has no attribute 'SEQUENCE_CLASSIFICATION'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\n",
    "   model_path, num_labels=len(classes),\n",
    "           id2label=id2class, label2id=class2id,\n",
    "                       problem_type = \"multi_label_classification\")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQUENCE_CLASSIFICATION,\n",
    "    inference_mode=False,  # è®¾ç½®ä¸ºFalseç”¨äºŽè®­ç»ƒæ¨¡å¼ï¼Œè®­ç»ƒæ—¶å¯æ›´æ–°LoRAå‚æ•°\n",
    "    r=8,  # LoRAçš„ç§©ï¼Œå¯æ ¹æ®å®žé™…æƒ…å†µè°ƒæ•´\n",
    "    lora_alpha=16,  # å¯ä»¥è°ƒæ•´ï¼Œå½±å“è®­ç»ƒè¿‡ç¨‹\n",
    "    lora_dropout=0.1,  # å¯ä»¥è°ƒæ•´ï¼Œä¸€èˆ¬æ˜¯è¾ƒå°çš„å€¼ï¼Œå¦‚0.1ç­‰\n",
    "    target_modules=[\"query\", \"value\"]  # è¦åº”ç”¨LoRAçš„æ¨¡å—ï¼Œéœ€æ ¹æ®æ¨¡åž‹ç»“æž„è°ƒæ•´ï¼Œæ¯”å¦‚é’ˆå¯¹DeBERTaçš„ç›¸å…³æ³¨æ„åŠ›æ¨¡å—ç­‰\n",
    ")\n",
    "\n",
    "# åº”ç”¨LoRAé…ç½®åˆ°æ¨¡åž‹ä¸Š\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/transformers/training_args.py:1573: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3316013/3491357845.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-07 17:47:54,564] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_fwd\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_bwd\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 03:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.172532</td>\n",
       "      <td>0.945063</td>\n",
       "      <td>0.257038</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.160061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.160140</td>\n",
       "      <td>0.946330</td>\n",
       "      <td>0.316032</td>\n",
       "      <td>0.649289</td>\n",
       "      <td>0.208841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/wangys/anaconda3/envs/deepspeed/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=460, training_loss=0.2164324553116508, metrics={'train_runtime': 203.607, 'train_samples_per_second': 27.101, 'train_steps_per_second': 2.259, 'total_flos': 3656743226849076.0, 'train_loss': 0.2164324553116508, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "   output_dir=\"my_awesome_model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=3,\n",
    "   per_device_eval_batch_size=3,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    "   report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\t\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
