{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2768f438",
   "metadata": {},
   "source": [
    "# Mislabeled data detection - RoBERTa-MRPC\n",
    "\n",
    "This notebook demonstrates how to efficiently compute the influence functions using DataInf, showing its application to **mislabeled data detection** tasks.\n",
    "\n",
    "- Model: Robert-large (https://arxiv.org/abs/1907.11692; pretrained with publicly available datasets including BOOKCORPUS, WIKIPEDIA, and CC-NEWS)\n",
    "- Fine-tuning dataset: GLUE-mrpc\n",
    "    - What is MRPC? The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.\n",
    "\n",
    "References\n",
    "- https://github.com/huggingface/peft/blob/main/examples/sequence_classification/LoRA.ipynb\n",
    "- DataInf is available at this [ArXiv link](https://arxiv.org/abs/2310.00902)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "# # Load the cola dataset\n",
    "# train_datasets = MsDataset.load('glue', subset_name='cola', split='train')\n",
    "# eval_datasets = MsDataset.load('glue', subset_name='cola', split='validation')\n",
    "\n",
    "MsDataset.load(\n",
    "            \"glue\",\n",
    "            'qnli',\n",
    "            cache_dir=None,\n",
    "            use_auth_token=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625da945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "# from dataloader import create_dataloaders\n",
    "from dataloader_ER import create_dataloaders\n",
    "# from lora_model import LORAEngine,LORAEngineDeberta,LORAEngineDebertaMultiClass\n",
    "from lora_model_vmap import LORAEngineDebertaMultiClass\n",
    "\n",
    "# from influence import IFEngine\n",
    "from influence_hyperinf import IFEngine\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaeb117",
   "metadata": {},
   "source": [
    "## Set up hyperparameters and LoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path=\"roberta-large\"\n",
    "# model_name_or_path=\"/home/yanmy/roberta-base\"\n",
    "model_name_or_path=\"../../model/deberta-v3-base\"\n",
    "# model_name_or_path=\"../../model/roberta-large\"\n",
    "# task=\"mrpc\"\n",
    "task = \"ER\"\n",
    "noise_ratio=0\n",
    "batch_size=32\n",
    "# target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "# target_modules = ['value']\n",
    "target_modules=[\"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "\n",
    "device=\"cuda:3\"\n",
    "num_epochs=4\n",
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e778490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning models\n",
    "dataloader_outputs = create_dataloaders(model_name_or_path=model_name_or_path,\n",
    "                                           task=task,\n",
    "                                           noise_ratio=noise_ratio,\n",
    "                                           batch_size=batch_size)\n",
    "train_dataloader, eval_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "\n",
    "# lora_engine = LORAEngine(model_name_or_path=model_name_or_path,\n",
    "#                             target_modules=target_modules,\n",
    "#                             train_dataloader=train_dataloader,\n",
    "#                             eval_dataloader=eval_dataloader,\n",
    "#                             device=device,\n",
    "#                             num_epochs=num_epochs,\n",
    "#                             lr=lr,\n",
    "#                             low_rank=8, \n",
    "#                             task=task)\n",
    "\n",
    "lora_engine = LORAEngineDeberta(model_name_or_path=model_name_or_path,\n",
    "                            target_modules=target_modules,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            eval_dataloader=eval_dataloader,\n",
    "                            device=device,\n",
    "                            num_epochs=num_epochs,\n",
    "                            lr=lr,\n",
    "                            low_rank=8, \n",
    "                            task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19785d2",
   "metadata": {},
   "source": [
    "## Fine-tune a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning models\n",
    "# dataloader_outputs = create_dataloaders(model_name_or_path=model_name_or_path,\n",
    "#                                            task=task,\n",
    "#                                            noise_ratio=noise_ratio,\n",
    "#                                            batch_size=batch_size,\n",
    "#                                            train_file = '../ER/semi-text-c-FUSER/train.json',\n",
    "#                                            valid_file = '../ER/semi-text-c-FUSER/valid.json',\n",
    "#                                            test_file = '../ER/semi-text-c-FUSER/test.json')\n",
    "\n",
    "\n",
    "dataloader_outputs = create_dataloaders(model_name_or_path=model_name_or_path,\n",
    "                                           task=task,\n",
    "                                           noise_ratio=noise_ratio,\n",
    "                                           batch_size=batch_size,\n",
    "                                           train_file = '../ER/semi-text-c/train.json',\n",
    "                                           valid_file = '../ER/semi-text-c/valid.json',\n",
    "                                           test_file = '../ER/semi-text-c/test.json',\n",
    "                                           max_length=256)\n",
    "# train_dataloader, eval_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "train_dataloader, eval_dataloader,test_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "\n",
    "\n",
    "# lora_engine = LORAEngine(model_name_or_path=model_name_or_path,\n",
    "#                             target_modules=target_modules,\n",
    "#                             train_dataloader=train_dataloader,\n",
    "#                             eval_dataloader=eval_dataloader,\n",
    "#                             device=device,\n",
    "#                             num_epochs=num_epochs,\n",
    "#                             lr=lr,\n",
    "#                             low_rank=8, \n",
    "#                             task=task)\n",
    "\n",
    "lora_engine = LORAEngineDebertaMultiClass(model_name_or_path=model_name_or_path,\n",
    "                            target_modules=target_modules,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            # eval_dataloader=eval_dataloader,\n",
    "                            eval_dataloader=eval_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            device=device,\n",
    "                            num_epochs=num_epochs,\n",
    "                            lr=lr,\n",
    "                            low_rank=16, \n",
    "                            task=task,\n",
    "                            save_path = '../models/ER',\n",
    "                            valid_each_epoch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_engine.build_LORA_model()\n",
    "lora_engine.train_LORA_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06576452",
   "metadata": {},
   "source": [
    "## Compute the gradient\n",
    " - Influence function uses the first-order gradient of a loss function. Here we compute gradients using `compute_gradient`\n",
    " - `tr_grad_dict` has a nested structure of two Python dictionaries. The outer dictionary has `{an index of the training data: a dictionary of gradients}` and the inner dictionary has `{layer name: gradients}`. The `val_grad_dict` has the same structure but for the validationd data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5acb4bd",
   "metadata": {},
   "source": [
    "## Compute the influence function\n",
    " - We compute the inverse Hessian vector product first using `compute_hvps()`. With the argument `compute_accurate=True`, the exact influence function value will be computed. (it may take an hour to compute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf312d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict, val_grad_dict = lora_engine.compute_gradient(tokenized_datasets, collate_fn, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f21c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pickle, os\n",
    "import torch\n",
    "import numpy as np\n",
    "class IFEngine(object):\n",
    "        # 初始化存储时间、Hessian-Vector Product (HVP) 结果和影响函数（Influence Function）值的字典\n",
    "\n",
    "    def __init__(self):\n",
    "        self.time_dict=defaultdict(list)\n",
    "        self.hvp_dict=defaultdict(list)\n",
    "        self.IF_dict=defaultdict(list)\n",
    "\n",
    "    def preprocess_gradients(self, tr_grad_dict, val_grad_dict, noise_index=None): # 存储训练集和验证集的梯度字典，并计算验证集平均梯度\n",
    "        self.tr_grad_dict = tr_grad_dict # 训练集梯度字典\n",
    "        self.val_grad_dict = val_grad_dict # 验证集梯度字典\n",
    "        self.noise_index = noise_index # 可选的噪声索引（用于异常值处理）\n",
    "        # 存储训练集和验证集的样本数\n",
    "        self.n_train = len(self.tr_grad_dict.keys())\n",
    "        self.n_val = len(self.val_grad_dict.keys())\n",
    "        self.compute_val_grad_avg() # 计算验证集平均梯度\n",
    "\n",
    "    def compute_val_grad_avg(self):\n",
    "        # Compute the avg gradient on the validation dataset\n",
    "        self.val_grad_avg_dict={}\n",
    "        for weight_name in self.val_grad_dict[0]:\n",
    "            # 初始化为与梯度同设备的零向量\n",
    "            self.val_grad_avg_dict[weight_name]=torch.zeros(self.val_grad_dict[0][weight_name].shape).to(self.val_grad_dict[0][weight_name].device)\n",
    "            # 逐个样本累加梯度并取平均值\n",
    "            for val_id in self.val_grad_dict:\n",
    "                self.val_grad_avg_dict[weight_name] += self.val_grad_dict[val_id][weight_name] / self.n_val\n",
    "\n",
    "    def compute_hvps(self, lambda_const_param=10, compute_accurate=True, compute_LiSSA=True):\n",
    "        '''\n",
    "        Compute the influence function score under each method\n",
    "        '''\n",
    "        self.compute_hvp_iterative(lambda_const_param=lambda_const_param) ## HyperInf,使用 Schulz 迭代法计算 HVP 和近似 Hessian 逆矩阵\n",
    "        self.compute_hvp_identity() ## Baseline TracIn\n",
    "        self.compute_hvp_proposed(lambda_const_param=lambda_const_param) ## Datainf\n",
    "        if compute_LiSSA:\n",
    "            self.compute_hvp_LiSSA(lambda_const_param=lambda_const_param)\n",
    "        if compute_accurate:\n",
    "            self.compute_hvp_accurate(lambda_const_param=lambda_const_param)\n",
    "\n",
    "    def compute_hvp_identity(self):\n",
    "        '''\n",
    "        TracIN\n",
    "        '''\n",
    "        start_time = time()\n",
    "        self.hvp_dict['identity'] = self.val_grad_avg_dict.copy()\n",
    "        self.time_dict['identity'] = time()-start_time\n",
    "        print(\"Time taken for Hessian-free: \", self.time_dict['identity'])\n",
    "\n",
    "    def compute_hvp_iterative(self, lambda_const_param=10, n_iteration=30):\n",
    "        '''\n",
    "        Compute the influence funcion score by our method HyperINF\n",
    "        '''\n",
    "\n",
    "        def schulz_inverse_stable(A, damping_factor=0, max_iterations=20, tol=1e-6):\n",
    "            n = A.shape[0]\n",
    "            #I = np.eye(n)\n",
    "            I = torch.eye(n, device=A.device)\n",
    "            A_damped = A + damping_factor * I  # Apply damping for stable\n",
    "\n",
    "            #X = np.eye(n) * 0.00005  # Initial estimate of inverse matrix\n",
    "            X = torch.eye(n, device=A.device) * 0.00005  # Initial estimate of inverse matrix\n",
    "\n",
    "            for _ in range(max_iterations):\n",
    "                # 通过 Schulz 方法更新逆矩阵估计值\n",
    "                #X = X.dot(2 * I - A_damped.dot(X))\n",
    "                X = X @ (2 * I - A_damped @ X)\n",
    "\n",
    "                # # Check for convergence\n",
    "                # if np.linalg.norm(I - A.dot(X)) < tol:\n",
    "                #     break\n",
    "\n",
    "            return X\n",
    "\n",
    "        start_time = time()\n",
    "        hvp_iterative_dict={}\n",
    "\n",
    "        for _, weight_name in enumerate(tqdm(self.val_grad_avg_dict)):\n",
    "            # lambda_const computation = 0.1 x (n * d_l)^(-1) \\sum_{i=1}^{n} ||grad_i^l||_2^2, 计算 λ（正则化系数）：基于梯度的方差\n",
    "            S=torch.zeros(len(self.tr_grad_dict.keys())).to(self.val_grad_avg_dict[weight_name].device)\n",
    "            for tr_id in self.tr_grad_dict:\n",
    "                tmp_grad = self.tr_grad_dict[tr_id][weight_name]\n",
    "                S[tr_id]=torch.mean(tmp_grad**2)\n",
    "            lambda_const = torch.mean(S) / lambda_const_param # layer-wise lambda\n",
    "\n",
    "            # iterative hvp computation\n",
    "            # G_l: same shape of self.tr_grad_dict[0][weight_name].T @ self.tr_grad_dict[0][weight_name]\n",
    "            # 构造 Fisher 信息矩阵（近似 Hessian）\n",
    "            G_l = torch.zeros((self.tr_grad_dict[0][weight_name].T @ self.tr_grad_dict[0][weight_name]).shape).to(self.val_grad_avg_dict[weight_name].device)\n",
    "            for tr_id in self.tr_grad_dict:\n",
    "                tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device) # (grad_i^l)^T\n",
    "                G_l += tmp_grad.T @ tmp_grad / self.n_train\n",
    "                \n",
    "            # 加入正则化项以保证矩阵可逆\n",
    "            G_l = G_l + lambda_const * torch.eye(G_l.shape[0], device=G_l.device)\n",
    "           # G_l = G_l.cpu().detach().numpy()\n",
    "           # 使用 Schulz 方法近似计算逆矩阵\n",
    "            # G_l_inv = schulz_inverse_stable(G_l, damping_factor=0.001, max_iterations=n_iteration, tol=1e-6)\n",
    "            G_l_inv = torch.inverse(G_l)\n",
    "            print(G_l.shape,G_l_inv.shape)\n",
    "            # 计算 HVP: G_l_inv @ val_grad_avg\n",
    "            hvp_iterative_dict[weight_name] = torch.tensor(self.val_grad_avg_dict[weight_name] @ G_l_inv)\n",
    "            #print(hvp_iterative_dict[weight_name])\n",
    "        self.hvp_dict['iterative'] = hvp_iterative_dict\n",
    "        self.time_dict['iterative'] = time()-start_time\n",
    "        print(\"Time taken for HyperINF: \", self.time_dict['iterative'])\n",
    "\n",
    "\n",
    "\n",
    "    def compute_hvp_proposed(self, lambda_const_param=10):\n",
    "        '''\n",
    "        DataInf method\n",
    "        '''\n",
    "        start_time = time()\n",
    "        hvp_proposed_dict={}\n",
    "\n",
    "        for _ , weight_name in enumerate(tqdm(self.val_grad_avg_dict)):\n",
    "            # lambda_const computation = 0.1 x (n * d_l)^(-1) \\sum_{i=1}^{n} ||grad_i^l||_2^2\n",
    "            S=torch.zeros(len(self.tr_grad_dict.keys())).to(self.val_grad_avg_dict[weight_name].device)\n",
    "            for tr_id in self.tr_grad_dict:\n",
    "                tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device)\n",
    "                S[tr_id]=torch.mean(tmp_grad**2)\n",
    "            lambda_const = torch.mean(S) / lambda_const_param # layer-wise lambda\n",
    "\n",
    "            # hvp computation\n",
    "            hvp=torch.zeros(self.val_grad_avg_dict[weight_name].shape).to(self.val_grad_avg_dict[weight_name].device)\n",
    "            for tr_id in self.tr_grad_dict: # i\n",
    "                tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device) # grad_i^l\n",
    "                # L_(l,i) / (lambda + ||grad_i^l||_2^2) in Eqn. (5)\n",
    "                C_tmp = torch.sum(self.val_grad_avg_dict[weight_name] * tmp_grad) / (lambda_const + torch.sum(tmp_grad**2)).to(self.val_grad_avg_dict[weight_name].device)\n",
    "                # (v_l^T - C_tmp * (grad_i^l)^T ) / (n * lambda) in Eqn. (5)\n",
    "                hvp += (self.val_grad_avg_dict[weight_name] - C_tmp*tmp_grad) / (self.n_train*lambda_const)\n",
    "            hvp_proposed_dict[weight_name] = hvp\n",
    "        self.hvp_dict['proposed'] = hvp_proposed_dict\n",
    "        self.time_dict['proposed'] = time()-start_time\n",
    "        print(\"Time taken for Datainf: \", self.time_dict['proposed'])\n",
    "\n",
    "    def compute_hvp_accurate(self, lambda_const_param=10):\n",
    "        start_time = time()\n",
    "        hvp_accurate_dict={}\n",
    "        for _ , weight_name in enumerate(tqdm(self.val_grad_avg_dict)):\n",
    "\n",
    "            # lambda_const computation\n",
    "            S=torch.zeros(len(self.tr_grad_dict.keys()))\n",
    "            for tr_id in self.tr_grad_dict:\n",
    "                tmp_grad = self.tr_grad_dict[tr_id][weight_name]\n",
    "                S[tr_id]=torch.mean(tmp_grad**2)\n",
    "            lambda_const = torch.mean(S) / lambda_const_param # layer-wise lambda\n",
    "\n",
    "            # hvp computation (eigenvalue decomposition)\n",
    "            AAt_matrix = torch.zeros(torch.outer(self.tr_grad_dict[0][weight_name].reshape(-1),\n",
    "                                                 self.tr_grad_dict[0][weight_name].reshape(-1)).shape)\n",
    "            for tr_id in self.tr_grad_dict:\n",
    "\n",
    "                tmp_mat = torch.outer(self.tr_grad_dict[tr_id][weight_name].reshape(-1),\n",
    "                                      self.tr_grad_dict[tr_id][weight_name].reshape(-1))\n",
    "                AAt_matrix += tmp_mat\n",
    "\n",
    "\n",
    "            L, V = torch.linalg.eig(AAt_matrix)\n",
    "            L, V = L.float(), V.float()\n",
    "            hvp = self.val_grad_avg_dict[weight_name].reshape(-1) @ V\n",
    "            hvp = (hvp / (lambda_const + L/ self.n_train)) @ V.T\n",
    "\n",
    "            hvp_accurate_dict[weight_name] = hvp.reshape(len(self.tr_grad_dict[0][weight_name]), -1)\n",
    "            del tmp_mat, AAt_matrix, V # to save memory\n",
    "        self.hvp_dict['accurate'] = hvp_accurate_dict\n",
    "        self.time_dict['accurate'] = time()-start_time\n",
    "        print(\"Time taken for Accurate: \", self.time_dict['accurate'])\n",
    "\n",
    "    def compute_hvp_LiSSA(self, lambda_const_param=10, n_iteration=10, alpha_const=1.):\n",
    "        '''\n",
    "        LiSSA method\n",
    "        '''\n",
    "        start_time = time()\n",
    "        hvp_LiSSA_dict={}\n",
    "\n",
    "        for _, weight_name in enumerate(tqdm(self.val_grad_avg_dict)):\n",
    "            # lambda_const computation\n",
    "            S=torch.zeros(len(self.tr_grad_dict.keys())).to(self.val_grad_avg_dict[weight_name].device)\n",
    "            for tr_id in self.tr_grad_dict:\n",
    "                tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device)\n",
    "                S[tr_id]=torch.mean(tmp_grad**2)\n",
    "            lambda_const = torch.mean(S) / lambda_const_param # layer-wise lambda\n",
    "\n",
    "            # hvp computation\n",
    "            running_hvp=self.val_grad_avg_dict[weight_name]\n",
    "            for _ in range(n_iteration):\n",
    "                hvp_tmp=torch.zeros(self.val_grad_avg_dict[weight_name].shape).to(self.val_grad_avg_dict[weight_name].device)\n",
    "                for tr_id in self.tr_grad_dict:\n",
    "                    tmp_grad = self.tr_grad_dict[tr_id][weight_name].to(self.val_grad_avg_dict[weight_name].device)\n",
    "                    hvp_tmp += (torch.sum(tmp_grad*running_hvp)*tmp_grad - lambda_const*running_hvp) / self.n_train\n",
    "\n",
    "                running_hvp = self.val_grad_avg_dict[weight_name] + running_hvp - alpha_const*hvp_tmp\n",
    "            hvp_LiSSA_dict[weight_name] = running_hvp\n",
    "\n",
    "        self.hvp_dict['LiSSA'] = hvp_LiSSA_dict\n",
    "        self.time_dict['LiSSA'] = time()-start_time\n",
    "        print(\"Time taken for LiSSA: \", self.time_dict['LiSSA'])\n",
    "\n",
    "    def compute_IF(self):\n",
    "        for method_name in self.hvp_dict:\n",
    "            if_tmp_dict = {}\n",
    "            for tr_id in self.tr_grad_dict:\n",
    "                if_tmp_value = 0\n",
    "                for weight_name in self.val_grad_avg_dict:\n",
    "                    if_tmp_value += torch.sum(self.hvp_dict[method_name][weight_name]*self.tr_grad_dict[tr_id][weight_name])\n",
    "                if_tmp_dict[tr_id]= -if_tmp_value.cpu()\n",
    "               # print(-if_tmp_value)\n",
    "\n",
    "            self.IF_dict[method_name] = pd.Series(if_tmp_dict, dtype=float).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d01d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_engine = IFEngine()\n",
    "influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict, noise_index)\n",
    "\n",
    "influence_engine.compute_hvps(compute_accurate=False,compute_LiSSA=False)\n",
    "influence_engine.compute_IF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85318fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_engine.IF_dict['proposed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = torch.load('/home/yanmy/HyperINF-main/output/output_fp16_deberta_ER_v_proj.pkl')\n",
    "result['proposed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "IF_device = 'cuda:3' ## 避免爆显存\n",
    "for key in tqdm(tr_grad_dict): ## 尝试转为fp16计算\n",
    "    for kk in tr_grad_dict[key]:\n",
    "        tr_grad_dict[key][kk] = tr_grad_dict[key][kk].to(IF_device)\n",
    "for key in tqdm(val_grad_dict):\n",
    "    for kk in val_grad_dict[key]:\n",
    "        val_grad_dict[key][kk] = val_grad_dict[key][kk].to(IF_device)\n",
    "\n",
    "influence_engine = IFEngine()\n",
    "influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict, noise_index)\n",
    "\n",
    "influence_engine.compute_hvps(compute_accurate=False,compute_LiSSA=True)\n",
    "influence_engine.compute_IF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d426fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(influence_engine.IF_dict,'../../HyperINF-main/output/output_fp16_deberta_ER_v_proj_inv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ba796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3288995/2153823836.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tr_grad_dict = torch.load('../output/tr_grad_dict_all.pkl')\n",
      "/tmp/ipykernel_3288995/2153823836.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_grad_dict = torch.load('../output/val_grad_dict_all.pkl')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "tr_grad_dict = torch.load('../output/tr_grad_dict_all.pkl')\n",
    "val_grad_dict = torch.load('../output/val_grad_dict_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6612979",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grad_dict = tr_grad_dict[0]\n",
    "val_grad_dict = val_grad_dict[0]\n",
    "# tr_grad_dict.pop('ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(val_grad_dict):\n",
    "    val_grad_dict[key].pop('ids')\n",
    "for key in tqdm(tr_grad_dict):\n",
    "    tr_grad_dict[key].pop('ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba66074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1895768214bb4affa37da92724fc3e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a10bc2110f84a6fa07878d5cf068749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from influence_batch import IFEngine\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "IF_device = 'cuda:7' ## 避免爆显存\n",
    "# tr_grad_dict_IF = tr_grad_dict.copy()\n",
    "# val_grad_dict_IF = val_grad_dict.copy()\n",
    "# ## to cuda\n",
    "for key in tqdm(tr_grad_dict): ## 尝试转为fp16计算\n",
    "    for kk in tr_grad_dict[key]:\n",
    "        # if kk!='ids':\n",
    "            tr_grad_dict[key][kk] = tr_grad_dict[key][kk].to(IF_device)\n",
    "        # tr_grad_dict[key].pop('ids')\n",
    "\n",
    "for key in tqdm(val_grad_dict):\n",
    "    for kk in val_grad_dict[key]:\n",
    "        # if kk!='ids':\n",
    "            val_grad_dict[key][kk] = val_grad_dict[key][kk].to(IF_device)\n",
    "        # val_grad_dict[key].pop('ids')\n",
    "\n",
    "\n",
    "noise_index = None\n",
    "## compute influence function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7693cd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e463b7b555cd4fbb80b8708830e904a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for HyperINF:  0.09330606460571289\n",
      "Time taken for Hessian-free:  1.6689300537109375e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/wangys/DataSelection-IF/notebooks/../src/influence_batch.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hvp_iterative_dict[weight_name] = torch.tensor(self.val_grad_avg_dict[weight_name] @ G_l_inv)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0f86b6b83a4179bf8cc838558d97d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for Datainf:  0.06619572639465332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660e7eeb35534feba9803f61cf2eb158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for LiSSA:  0.24889206886291504\n"
     ]
    }
   ],
   "source": [
    "influence_engine = IFEngine(weight_list=list(val_grad_dict[0].keys())[22:])\n",
    "influence_engine.preprocess_gradients(tr_grad_dict, val_grad_dict, noise_index)\n",
    "\n",
    "influence_engine.compute_hvps(compute_accurate=False,compute_LiSSA=True)\n",
    "influence_engine.compute_IF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff411e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.10.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>4246.681152</td>\n",
       "      <td>-32049.476562</td>\n",
       "      <td>5386.741699</td>\n",
       "      <td>4397.683105</td>\n",
       "      <td>-4365.251953</td>\n",
       "      <td>-28306.96875</td>\n",
       "      <td>5716.95166</td>\n",
       "      <td>-24101.621094</td>\n",
       "      <td>-24783.480469</td>\n",
       "      <td>8573.176758</td>\n",
       "      <td>...</td>\n",
       "      <td>-10405.790039</td>\n",
       "      <td>8494.412109</td>\n",
       "      <td>-12377.030273</td>\n",
       "      <td>-4557.339355</td>\n",
       "      <td>-13784.908203</td>\n",
       "      <td>9479.004883</td>\n",
       "      <td>5446.173828</td>\n",
       "      <td>-13826.923828</td>\n",
       "      <td>708.128052</td>\n",
       "      <td>-10125.771484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.11.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>4468.546387</td>\n",
       "      <td>-28445.125</td>\n",
       "      <td>2920.133789</td>\n",
       "      <td>8445.088867</td>\n",
       "      <td>-86.586975</td>\n",
       "      <td>-39438.757812</td>\n",
       "      <td>8688.3125</td>\n",
       "      <td>-23035.714844</td>\n",
       "      <td>-23578.392578</td>\n",
       "      <td>8994.666016</td>\n",
       "      <td>...</td>\n",
       "      <td>-14895.918945</td>\n",
       "      <td>13628.555664</td>\n",
       "      <td>-13053.261719</td>\n",
       "      <td>-3069.782227</td>\n",
       "      <td>-2677.004883</td>\n",
       "      <td>12798.408203</td>\n",
       "      <td>20255.876953</td>\n",
       "      <td>-18925.603516</td>\n",
       "      <td>5608.026367</td>\n",
       "      <td>-14934.188477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.11.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>-18412.275391</td>\n",
       "      <td>-17422.791016</td>\n",
       "      <td>5109.820312</td>\n",
       "      <td>5936.246094</td>\n",
       "      <td>-6635.799805</td>\n",
       "      <td>-19055.220703</td>\n",
       "      <td>1715.612061</td>\n",
       "      <td>-21720.251953</td>\n",
       "      <td>-6955.642578</td>\n",
       "      <td>-480.379883</td>\n",
       "      <td>...</td>\n",
       "      <td>-8185.606934</td>\n",
       "      <td>5542.118652</td>\n",
       "      <td>-9518.250977</td>\n",
       "      <td>-21220.15625</td>\n",
       "      <td>-30517.861328</td>\n",
       "      <td>11464.677734</td>\n",
       "      <td>8343.212891</td>\n",
       "      <td>-4946.064941</td>\n",
       "      <td>10544.654297</td>\n",
       "      <td>-3580.837891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.classifier.modules_to_save.default.weight</th>\n",
       "      <td>1828.411377</td>\n",
       "      <td>-1521.543823</td>\n",
       "      <td>-438.234985</td>\n",
       "      <td>30.565327</td>\n",
       "      <td>90.412445</td>\n",
       "      <td>-1516.199341</td>\n",
       "      <td>250.946869</td>\n",
       "      <td>-874.574341</td>\n",
       "      <td>-1554.590332</td>\n",
       "      <td>985.943359</td>\n",
       "      <td>...</td>\n",
       "      <td>-898.345337</td>\n",
       "      <td>1036.105713</td>\n",
       "      <td>-694.516724</td>\n",
       "      <td>1367.785645</td>\n",
       "      <td>1204.49292</td>\n",
       "      <td>739.494812</td>\n",
       "      <td>1580.711304</td>\n",
       "      <td>-1676.055054</td>\n",
       "      <td>285.42157</td>\n",
       "      <td>-1256.068359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <td>[2041, 3032, 496, 1522, 2068, 2994, 1911, 535,...</td>\n",
       "      <td>[1189, 2592, 146, 588, 1106, 3043, 2499, 1623,...</td>\n",
       "      <td>[1943, 3503, 2542, 1033, 469, 951, 243, 3315, ...</td>\n",
       "      <td>[802, 2966, 2274, 99, 557, 2622, 3121, 349, 47...</td>\n",
       "      <td>[630, 1832, 75, 3001, 2569, 3508, 407, 1796, 1...</td>\n",
       "      <td>[1784, 650, 1013, 485, 2294, 1403, 2546, 2885,...</td>\n",
       "      <td>[2269, 1569, 2621, 589, 2466, 498, 165, 2962, ...</td>\n",
       "      <td>[44, 978, 548, 778, 492, 122, 3657, 3603, 2243...</td>\n",
       "      <td>[411, 1417, 2456, 136, 2566, 191, 1621, 1315, ...</td>\n",
       "      <td>[1161, 2547, 3322, 1420, 2909, 2362, 3098, 994...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1908, 1060, 747, 184, 1059, 3218, 2916, 1765,...</td>\n",
       "      <td>[2743, 2624, 1310, 1743, 2616, 1986, 3325, 321...</td>\n",
       "      <td>[347, 3115, 2064, 910, 1731, 766, 1174, 751, 3...</td>\n",
       "      <td>[659, 3630, 23, 3019, 5, 1857, 3000, 1709, 297...</td>\n",
       "      <td>[3078, 1950, 2646, 1828, 1179, 2989, 754, 607,...</td>\n",
       "      <td>[807, 1239, 187, 1864, 708, 281, 1815, 912, 25...</td>\n",
       "      <td>[2048, 1020, 3419, 3548, 2307, 1198, 3666, 374...</td>\n",
       "      <td>[2839, 42, 1474, 3438, 2714, 1782, 1800, 2654,...</td>\n",
       "      <td>[3169, 3622, 2114, 1759, 897, 1391, 2783, 1845...</td>\n",
       "      <td>[3093, 272, 1783, 58, 728, 2075, 820, 1912, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  0    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                        4246.681152   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        4468.546387   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -18412.275391   \n",
       "base_model.model.classifier.modules_to_save.def...                                        1828.411377   \n",
       "ids                                                 [2041, 3032, 496, 1522, 2068, 2994, 1911, 535,...   \n",
       "\n",
       "                                                                                                  1    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                      -32049.476562   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                         -28445.125   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -17422.791016   \n",
       "base_model.model.classifier.modules_to_save.def...                                       -1521.543823   \n",
       "ids                                                 [1189, 2592, 146, 588, 1106, 3043, 2499, 1623,...   \n",
       "\n",
       "                                                                                                  2    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                        5386.741699   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        2920.133789   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        5109.820312   \n",
       "base_model.model.classifier.modules_to_save.def...                                        -438.234985   \n",
       "ids                                                 [1943, 3503, 2542, 1033, 469, 951, 243, 3315, ...   \n",
       "\n",
       "                                                                                                  3    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                        4397.683105   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        8445.088867   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        5936.246094   \n",
       "base_model.model.classifier.modules_to_save.def...                                          30.565327   \n",
       "ids                                                 [802, 2966, 2274, 99, 557, 2622, 3121, 349, 47...   \n",
       "\n",
       "                                                                                                  4    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                       -4365.251953   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                         -86.586975   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -6635.799805   \n",
       "base_model.model.classifier.modules_to_save.def...                                          90.412445   \n",
       "ids                                                 [630, 1832, 75, 3001, 2569, 3508, 407, 1796, 1...   \n",
       "\n",
       "                                                                                                  5    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                       -28306.96875   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -39438.757812   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -19055.220703   \n",
       "base_model.model.classifier.modules_to_save.def...                                       -1516.199341   \n",
       "ids                                                 [1784, 650, 1013, 485, 2294, 1403, 2546, 2885,...   \n",
       "\n",
       "                                                                                                  6    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                         5716.95166   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          8688.3125   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        1715.612061   \n",
       "base_model.model.classifier.modules_to_save.def...                                         250.946869   \n",
       "ids                                                 [2269, 1569, 2621, 589, 2466, 498, 165, 2962, ...   \n",
       "\n",
       "                                                                                                  7    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                      -24101.621094   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -23035.714844   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -21720.251953   \n",
       "base_model.model.classifier.modules_to_save.def...                                        -874.574341   \n",
       "ids                                                 [44, 978, 548, 778, 492, 122, 3657, 3603, 2243...   \n",
       "\n",
       "                                                                                                  8    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                      -24783.480469   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -23578.392578   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -6955.642578   \n",
       "base_model.model.classifier.modules_to_save.def...                                       -1554.590332   \n",
       "ids                                                 [411, 1417, 2456, 136, 2566, 191, 1621, 1315, ...   \n",
       "\n",
       "                                                                                                  9    \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                        8573.176758   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        8994.666016   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        -480.379883   \n",
       "base_model.model.classifier.modules_to_save.def...                                         985.943359   \n",
       "ids                                                 [1161, 2547, 3322, 1420, 2909, 2362, 3098, 994...   \n",
       "\n",
       "                                                    ...  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...  ...   \n",
       "base_model.model.deberta.encoder.layer.11.atten...  ...   \n",
       "base_model.model.deberta.encoder.layer.11.atten...  ...   \n",
       "base_model.model.classifier.modules_to_save.def...  ...   \n",
       "ids                                                 ...   \n",
       "\n",
       "                                                                                                  105  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                      -10405.790039   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -14895.918945   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -8185.606934   \n",
       "base_model.model.classifier.modules_to_save.def...                                        -898.345337   \n",
       "ids                                                 [1908, 1060, 747, 184, 1059, 3218, 2916, 1765,...   \n",
       "\n",
       "                                                                                                  106  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                        8494.412109   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       13628.555664   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        5542.118652   \n",
       "base_model.model.classifier.modules_to_save.def...                                        1036.105713   \n",
       "ids                                                 [2743, 2624, 1310, 1743, 2616, 1986, 3325, 321...   \n",
       "\n",
       "                                                                                                  107  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                      -12377.030273   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -13053.261719   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -9518.250977   \n",
       "base_model.model.classifier.modules_to_save.def...                                        -694.516724   \n",
       "ids                                                 [347, 3115, 2064, 910, 1731, 766, 1174, 751, 3...   \n",
       "\n",
       "                                                                                                  108  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                       -4557.339355   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -3069.782227   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -21220.15625   \n",
       "base_model.model.classifier.modules_to_save.def...                                        1367.785645   \n",
       "ids                                                 [659, 3630, 23, 3019, 5, 1857, 3000, 1709, 297...   \n",
       "\n",
       "                                                                                                  109  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                      -13784.908203   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -2677.004883   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -30517.861328   \n",
       "base_model.model.classifier.modules_to_save.def...                                         1204.49292   \n",
       "ids                                                 [3078, 1950, 2646, 1828, 1179, 2989, 754, 607,...   \n",
       "\n",
       "                                                                                                  110  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                        9479.004883   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       12798.408203   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       11464.677734   \n",
       "base_model.model.classifier.modules_to_save.def...                                         739.494812   \n",
       "ids                                                 [807, 1239, 187, 1864, 708, 281, 1815, 912, 25...   \n",
       "\n",
       "                                                                                                  111  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                        5446.173828   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       20255.876953   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        8343.212891   \n",
       "base_model.model.classifier.modules_to_save.def...                                        1580.711304   \n",
       "ids                                                 [2048, 1020, 3419, 3548, 2307, 1198, 3666, 374...   \n",
       "\n",
       "                                                                                                  112  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                      -13826.923828   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -18925.603516   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -4946.064941   \n",
       "base_model.model.classifier.modules_to_save.def...                                       -1676.055054   \n",
       "ids                                                 [2839, 42, 1474, 3438, 2714, 1782, 1800, 2654,...   \n",
       "\n",
       "                                                                                                  113  \\\n",
       "base_model.model.deberta.encoder.layer.10.atten...                                         708.128052   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                        5608.026367   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       10544.654297   \n",
       "base_model.model.classifier.modules_to_save.def...                                          285.42157   \n",
       "ids                                                 [3169, 3622, 2114, 1759, 897, 1391, 2783, 1845...   \n",
       "\n",
       "                                                                                                  114  \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                      -10125.771484  \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                      -14934.188477  \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                       -3580.837891  \n",
       "base_model.model.classifier.modules_to_save.def...                                       -1256.068359  \n",
       "ids                                                 [3093, 272, 1783, 58, 728, 2075, 820, 1912, 15...  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = influence_engine.IF_dict['proposed']\n",
    "import pandas as pd\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a6b933f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>-0.002792</td>\n",
       "      <td>0.214461</td>\n",
       "      <td>-0.137952</td>\n",
       "      <td>0.092945</td>\n",
       "      <td>-0.113937</td>\n",
       "      <td>-0.248649</td>\n",
       "      <td>0.377446</td>\n",
       "      <td>-0.55323</td>\n",
       "      <td>-0.42092</td>\n",
       "      <td>-0.162539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256942</td>\n",
       "      <td>-0.060108</td>\n",
       "      <td>-0.30166</td>\n",
       "      <td>-0.587831</td>\n",
       "      <td>-0.019633</td>\n",
       "      <td>-0.041786</td>\n",
       "      <td>-0.036602</td>\n",
       "      <td>0.03769</td>\n",
       "      <td>0.091819</td>\n",
       "      <td>0.160528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.0.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.056509</td>\n",
       "      <td>-0.156315</td>\n",
       "      <td>-0.140347</td>\n",
       "      <td>0.101002</td>\n",
       "      <td>-0.337351</td>\n",
       "      <td>-0.438037</td>\n",
       "      <td>0.197004</td>\n",
       "      <td>-0.320979</td>\n",
       "      <td>-0.644424</td>\n",
       "      <td>-0.0436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297538</td>\n",
       "      <td>-0.254762</td>\n",
       "      <td>-0.707609</td>\n",
       "      <td>-0.628391</td>\n",
       "      <td>-0.296118</td>\n",
       "      <td>-0.033697</td>\n",
       "      <td>-0.340017</td>\n",
       "      <td>0.095167</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>0.054598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.178758</td>\n",
       "      <td>-0.66987</td>\n",
       "      <td>0.163777</td>\n",
       "      <td>0.060063</td>\n",
       "      <td>-0.190005</td>\n",
       "      <td>-0.300544</td>\n",
       "      <td>0.020764</td>\n",
       "      <td>-0.143777</td>\n",
       "      <td>-0.676488</td>\n",
       "      <td>0.024625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291554</td>\n",
       "      <td>0.210634</td>\n",
       "      <td>-0.045833</td>\n",
       "      <td>-0.283357</td>\n",
       "      <td>-0.402362</td>\n",
       "      <td>-0.40779</td>\n",
       "      <td>-0.39959</td>\n",
       "      <td>0.281333</td>\n",
       "      <td>0.205976</td>\n",
       "      <td>0.327814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.1.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.027288</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.038804</td>\n",
       "      <td>-0.003856</td>\n",
       "      <td>-0.204447</td>\n",
       "      <td>-0.210844</td>\n",
       "      <td>-0.103285</td>\n",
       "      <td>-0.035615</td>\n",
       "      <td>-0.329397</td>\n",
       "      <td>-0.14137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157061</td>\n",
       "      <td>-0.352993</td>\n",
       "      <td>-0.375491</td>\n",
       "      <td>-0.329159</td>\n",
       "      <td>-0.029132</td>\n",
       "      <td>-0.038246</td>\n",
       "      <td>-0.058219</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>-0.036041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.055865</td>\n",
       "      <td>-0.081093</td>\n",
       "      <td>-0.01777</td>\n",
       "      <td>-0.029478</td>\n",
       "      <td>-0.237979</td>\n",
       "      <td>-0.286873</td>\n",
       "      <td>0.074688</td>\n",
       "      <td>-0.318219</td>\n",
       "      <td>-0.379533</td>\n",
       "      <td>0.151725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264797</td>\n",
       "      <td>0.023683</td>\n",
       "      <td>-0.265076</td>\n",
       "      <td>-0.063408</td>\n",
       "      <td>-0.145172</td>\n",
       "      <td>-0.09048</td>\n",
       "      <td>-0.029833</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>-0.004159</td>\n",
       "      <td>0.069041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.2.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.14476</td>\n",
       "      <td>-0.074117</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>-0.003116</td>\n",
       "      <td>-0.344114</td>\n",
       "      <td>-0.356342</td>\n",
       "      <td>0.184545</td>\n",
       "      <td>-0.23669</td>\n",
       "      <td>-0.592648</td>\n",
       "      <td>-0.005764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277656</td>\n",
       "      <td>-0.132468</td>\n",
       "      <td>-0.196689</td>\n",
       "      <td>-0.199692</td>\n",
       "      <td>-0.262067</td>\n",
       "      <td>-0.134191</td>\n",
       "      <td>-0.168306</td>\n",
       "      <td>-0.028317</td>\n",
       "      <td>-0.111756</td>\n",
       "      <td>-0.037405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.136803</td>\n",
       "      <td>-0.192393</td>\n",
       "      <td>0.091636</td>\n",
       "      <td>-0.044098</td>\n",
       "      <td>-0.244484</td>\n",
       "      <td>-0.471105</td>\n",
       "      <td>0.359509</td>\n",
       "      <td>-0.169028</td>\n",
       "      <td>-0.858913</td>\n",
       "      <td>-0.062963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473988</td>\n",
       "      <td>-0.036099</td>\n",
       "      <td>-0.369804</td>\n",
       "      <td>-0.006071</td>\n",
       "      <td>-0.265601</td>\n",
       "      <td>-0.087226</td>\n",
       "      <td>-0.213986</td>\n",
       "      <td>-0.179511</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.200177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.3.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.139233</td>\n",
       "      <td>-0.073114</td>\n",
       "      <td>0.120961</td>\n",
       "      <td>-0.123164</td>\n",
       "      <td>-0.346416</td>\n",
       "      <td>-0.051335</td>\n",
       "      <td>0.070784</td>\n",
       "      <td>-0.128176</td>\n",
       "      <td>-0.906164</td>\n",
       "      <td>-0.005602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260905</td>\n",
       "      <td>-0.276575</td>\n",
       "      <td>-0.198805</td>\n",
       "      <td>-0.185212</td>\n",
       "      <td>-0.193549</td>\n",
       "      <td>-0.247312</td>\n",
       "      <td>-0.362922</td>\n",
       "      <td>-0.083613</td>\n",
       "      <td>0.069094</td>\n",
       "      <td>0.139744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.19205</td>\n",
       "      <td>-0.590967</td>\n",
       "      <td>0.032571</td>\n",
       "      <td>-0.005729</td>\n",
       "      <td>-0.235471</td>\n",
       "      <td>-0.84766</td>\n",
       "      <td>0.188607</td>\n",
       "      <td>-0.500709</td>\n",
       "      <td>-1.083541</td>\n",
       "      <td>-0.238263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329472</td>\n",
       "      <td>-0.047762</td>\n",
       "      <td>-0.617092</td>\n",
       "      <td>-0.29318</td>\n",
       "      <td>-0.246804</td>\n",
       "      <td>-0.166606</td>\n",
       "      <td>-0.106733</td>\n",
       "      <td>-0.248479</td>\n",
       "      <td>0.15717</td>\n",
       "      <td>-0.134625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.4.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.392455</td>\n",
       "      <td>-0.44349</td>\n",
       "      <td>0.188913</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>-0.391593</td>\n",
       "      <td>-0.448656</td>\n",
       "      <td>0.582309</td>\n",
       "      <td>0.05325</td>\n",
       "      <td>-0.966565</td>\n",
       "      <td>0.250896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244405</td>\n",
       "      <td>0.020122</td>\n",
       "      <td>-0.476967</td>\n",
       "      <td>-0.314586</td>\n",
       "      <td>-0.023449</td>\n",
       "      <td>-0.238637</td>\n",
       "      <td>-0.594619</td>\n",
       "      <td>-0.246472</td>\n",
       "      <td>0.107189</td>\n",
       "      <td>-0.046298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.098254</td>\n",
       "      <td>-0.332135</td>\n",
       "      <td>0.220692</td>\n",
       "      <td>-0.053722</td>\n",
       "      <td>-0.198889</td>\n",
       "      <td>-0.476128</td>\n",
       "      <td>0.045115</td>\n",
       "      <td>-0.07363</td>\n",
       "      <td>-0.418666</td>\n",
       "      <td>0.072858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137125</td>\n",
       "      <td>0.196084</td>\n",
       "      <td>-0.538379</td>\n",
       "      <td>0.246166</td>\n",
       "      <td>0.278526</td>\n",
       "      <td>0.227817</td>\n",
       "      <td>0.132502</td>\n",
       "      <td>-0.352544</td>\n",
       "      <td>0.047346</td>\n",
       "      <td>-0.428009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.5.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.198159</td>\n",
       "      <td>-0.015774</td>\n",
       "      <td>0.16524</td>\n",
       "      <td>-0.123532</td>\n",
       "      <td>-0.235988</td>\n",
       "      <td>-0.341662</td>\n",
       "      <td>0.164594</td>\n",
       "      <td>0.01735</td>\n",
       "      <td>-0.971057</td>\n",
       "      <td>0.036761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036196</td>\n",
       "      <td>-0.115006</td>\n",
       "      <td>-0.434189</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>-0.134519</td>\n",
       "      <td>-0.161426</td>\n",
       "      <td>-0.569493</td>\n",
       "      <td>-0.029103</td>\n",
       "      <td>0.055847</td>\n",
       "      <td>-0.218948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.6.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.292771</td>\n",
       "      <td>-0.775282</td>\n",
       "      <td>-0.022499</td>\n",
       "      <td>0.046982</td>\n",
       "      <td>-0.275537</td>\n",
       "      <td>-0.774497</td>\n",
       "      <td>0.547008</td>\n",
       "      <td>-0.199132</td>\n",
       "      <td>-1.248056</td>\n",
       "      <td>-0.251957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226381</td>\n",
       "      <td>-0.086881</td>\n",
       "      <td>-0.84264</td>\n",
       "      <td>-0.15316</td>\n",
       "      <td>0.147685</td>\n",
       "      <td>-0.258888</td>\n",
       "      <td>-0.275184</td>\n",
       "      <td>-0.216572</td>\n",
       "      <td>0.291677</td>\n",
       "      <td>-0.409051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.6.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.215827</td>\n",
       "      <td>-0.413406</td>\n",
       "      <td>0.295059</td>\n",
       "      <td>-0.145862</td>\n",
       "      <td>-0.37301</td>\n",
       "      <td>-0.810405</td>\n",
       "      <td>0.613199</td>\n",
       "      <td>0.18622</td>\n",
       "      <td>-1.093206</td>\n",
       "      <td>0.175242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238817</td>\n",
       "      <td>-0.233841</td>\n",
       "      <td>-0.572529</td>\n",
       "      <td>-0.330001</td>\n",
       "      <td>-0.359169</td>\n",
       "      <td>-0.289662</td>\n",
       "      <td>-0.482529</td>\n",
       "      <td>-0.30142</td>\n",
       "      <td>-0.002135</td>\n",
       "      <td>-0.293762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.7.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.332904</td>\n",
       "      <td>-0.57791</td>\n",
       "      <td>0.129567</td>\n",
       "      <td>-0.163199</td>\n",
       "      <td>-0.474915</td>\n",
       "      <td>-0.852885</td>\n",
       "      <td>0.222831</td>\n",
       "      <td>-0.323512</td>\n",
       "      <td>-1.061247</td>\n",
       "      <td>-0.132374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015765</td>\n",
       "      <td>-0.594157</td>\n",
       "      <td>-1.257513</td>\n",
       "      <td>-0.883947</td>\n",
       "      <td>-0.500247</td>\n",
       "      <td>-0.6013</td>\n",
       "      <td>-0.734604</td>\n",
       "      <td>-0.290117</td>\n",
       "      <td>-0.102028</td>\n",
       "      <td>-0.37198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.7.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.112797</td>\n",
       "      <td>-0.256702</td>\n",
       "      <td>0.094027</td>\n",
       "      <td>-0.178481</td>\n",
       "      <td>-0.349706</td>\n",
       "      <td>-1.095097</td>\n",
       "      <td>0.310162</td>\n",
       "      <td>0.234712</td>\n",
       "      <td>-1.082799</td>\n",
       "      <td>0.218409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298758</td>\n",
       "      <td>-0.119643</td>\n",
       "      <td>-1.133333</td>\n",
       "      <td>-0.385864</td>\n",
       "      <td>-0.523531</td>\n",
       "      <td>-0.261873</td>\n",
       "      <td>-0.558597</td>\n",
       "      <td>-0.17902</td>\n",
       "      <td>-0.098232</td>\n",
       "      <td>-0.274255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.8.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.19416</td>\n",
       "      <td>-0.047386</td>\n",
       "      <td>0.230465</td>\n",
       "      <td>-0.254186</td>\n",
       "      <td>-0.142909</td>\n",
       "      <td>-0.618639</td>\n",
       "      <td>0.431471</td>\n",
       "      <td>-0.782424</td>\n",
       "      <td>-0.549205</td>\n",
       "      <td>0.089142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.396075</td>\n",
       "      <td>-0.23969</td>\n",
       "      <td>-1.491969</td>\n",
       "      <td>-0.714956</td>\n",
       "      <td>-0.459515</td>\n",
       "      <td>-0.475683</td>\n",
       "      <td>-0.784999</td>\n",
       "      <td>-0.588387</td>\n",
       "      <td>-0.25317</td>\n",
       "      <td>-0.401181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.8.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.046904</td>\n",
       "      <td>-0.511408</td>\n",
       "      <td>0.272433</td>\n",
       "      <td>0.156861</td>\n",
       "      <td>-0.342042</td>\n",
       "      <td>-0.847866</td>\n",
       "      <td>1.089048</td>\n",
       "      <td>-0.798935</td>\n",
       "      <td>-0.72829</td>\n",
       "      <td>0.621343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.659621</td>\n",
       "      <td>0.154978</td>\n",
       "      <td>-1.130791</td>\n",
       "      <td>-0.003262</td>\n",
       "      <td>0.291454</td>\n",
       "      <td>-0.144082</td>\n",
       "      <td>-0.714478</td>\n",
       "      <td>-0.657897</td>\n",
       "      <td>0.350278</td>\n",
       "      <td>-0.294913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.9.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>-0.270864</td>\n",
       "      <td>0.040701</td>\n",
       "      <td>0.186763</td>\n",
       "      <td>-0.231008</td>\n",
       "      <td>-0.341176</td>\n",
       "      <td>-0.82451</td>\n",
       "      <td>0.575566</td>\n",
       "      <td>-0.044544</td>\n",
       "      <td>-0.825899</td>\n",
       "      <td>0.493065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234266</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>-1.032613</td>\n",
       "      <td>-0.820275</td>\n",
       "      <td>-0.548061</td>\n",
       "      <td>-0.417693</td>\n",
       "      <td>-1.067562</td>\n",
       "      <td>-0.77619</td>\n",
       "      <td>-0.131101</td>\n",
       "      <td>-0.757706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.9.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.181064</td>\n",
       "      <td>-0.579111</td>\n",
       "      <td>0.146162</td>\n",
       "      <td>-0.02647</td>\n",
       "      <td>-0.184742</td>\n",
       "      <td>-0.917341</td>\n",
       "      <td>0.250982</td>\n",
       "      <td>-0.722402</td>\n",
       "      <td>-1.16531</td>\n",
       "      <td>0.232613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414435</td>\n",
       "      <td>0.446336</td>\n",
       "      <td>-0.540872</td>\n",
       "      <td>0.034245</td>\n",
       "      <td>-0.372402</td>\n",
       "      <td>0.040043</td>\n",
       "      <td>0.00375</td>\n",
       "      <td>-0.500791</td>\n",
       "      <td>0.162658</td>\n",
       "      <td>-0.446702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.10.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>0.061355</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.079674</td>\n",
       "      <td>-0.026072</td>\n",
       "      <td>-0.219239</td>\n",
       "      <td>-0.520825</td>\n",
       "      <td>0.200233</td>\n",
       "      <td>-0.439619</td>\n",
       "      <td>-0.660791</td>\n",
       "      <td>0.301812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425284</td>\n",
       "      <td>-0.114967</td>\n",
       "      <td>-1.079712</td>\n",
       "      <td>-0.553524</td>\n",
       "      <td>-0.536183</td>\n",
       "      <td>-0.282182</td>\n",
       "      <td>-0.594605</td>\n",
       "      <td>-0.542457</td>\n",
       "      <td>0.106237</td>\n",
       "      <td>-0.552031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.10.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>0.044053</td>\n",
       "      <td>-1.360904</td>\n",
       "      <td>0.301462</td>\n",
       "      <td>0.172283</td>\n",
       "      <td>-0.321999</td>\n",
       "      <td>-1.233153</td>\n",
       "      <td>0.350924</td>\n",
       "      <td>-1.241081</td>\n",
       "      <td>-1.040351</td>\n",
       "      <td>0.49249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.548766</td>\n",
       "      <td>0.374585</td>\n",
       "      <td>-0.698491</td>\n",
       "      <td>-0.433522</td>\n",
       "      <td>-0.986309</td>\n",
       "      <td>0.392311</td>\n",
       "      <td>0.213531</td>\n",
       "      <td>-0.799849</td>\n",
       "      <td>0.096928</td>\n",
       "      <td>-0.403259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.11.attention.self.value_proj.lora_A.default.weight</th>\n",
       "      <td>-0.088828</td>\n",
       "      <td>-0.398544</td>\n",
       "      <td>0.425401</td>\n",
       "      <td>0.180796</td>\n",
       "      <td>-0.367283</td>\n",
       "      <td>-1.405324</td>\n",
       "      <td>0.772277</td>\n",
       "      <td>-0.839124</td>\n",
       "      <td>-0.889908</td>\n",
       "      <td>0.230208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.45745</td>\n",
       "      <td>-0.057926</td>\n",
       "      <td>-1.041163</td>\n",
       "      <td>-0.479463</td>\n",
       "      <td>-0.679992</td>\n",
       "      <td>0.140148</td>\n",
       "      <td>-0.382713</td>\n",
       "      <td>-0.512454</td>\n",
       "      <td>0.129889</td>\n",
       "      <td>-0.168923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.deberta.encoder.layer.11.attention.self.value_proj.lora_B.default.weight</th>\n",
       "      <td>-0.669642</td>\n",
       "      <td>-0.360336</td>\n",
       "      <td>0.113424</td>\n",
       "      <td>0.119894</td>\n",
       "      <td>-0.294114</td>\n",
       "      <td>-0.396758</td>\n",
       "      <td>0.061735</td>\n",
       "      <td>-0.573565</td>\n",
       "      <td>-0.068463</td>\n",
       "      <td>-0.040185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.313851</td>\n",
       "      <td>0.113545</td>\n",
       "      <td>-0.25877</td>\n",
       "      <td>-0.691484</td>\n",
       "      <td>-0.986623</td>\n",
       "      <td>0.264475</td>\n",
       "      <td>0.204267</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.305324</td>\n",
       "      <td>-0.14024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.classifier.modules_to_save.default.weight</th>\n",
       "      <td>-0.275683</td>\n",
       "      <td>-0.026677</td>\n",
       "      <td>-0.115425</td>\n",
       "      <td>-0.021991</td>\n",
       "      <td>-0.575016</td>\n",
       "      <td>0.094797</td>\n",
       "      <td>0.273732</td>\n",
       "      <td>-0.343231</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>-0.00229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.321151</td>\n",
       "      <td>-0.182191</td>\n",
       "      <td>-0.392172</td>\n",
       "      <td>-0.423644</td>\n",
       "      <td>-0.668879</td>\n",
       "      <td>0.167471</td>\n",
       "      <td>-0.071013</td>\n",
       "      <td>-0.110323</td>\n",
       "      <td>-0.154488</td>\n",
       "      <td>-0.363421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <td>[2041, 3032, 496, 1522, 2068, 2994, 1911, 535,...</td>\n",
       "      <td>[1189, 2592, 146, 588, 1106, 3043, 2499, 1623,...</td>\n",
       "      <td>[1943, 3503, 2542, 1033, 469, 951, 243, 3315, ...</td>\n",
       "      <td>[802, 2966, 2274, 99, 557, 2622, 3121, 349, 47...</td>\n",
       "      <td>[630, 1832, 75, 3001, 2569, 3508, 407, 1796, 1...</td>\n",
       "      <td>[1784, 650, 1013, 485, 2294, 1403, 2546, 2885,...</td>\n",
       "      <td>[2269, 1569, 2621, 589, 2466, 498, 165, 2962, ...</td>\n",
       "      <td>[44, 978, 548, 778, 492, 122, 3657, 3603, 2243...</td>\n",
       "      <td>[411, 1417, 2456, 136, 2566, 191, 1621, 1315, ...</td>\n",
       "      <td>[1161, 2547, 3322, 1420, 2909, 2362, 3098, 994...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1908, 1060, 747, 184, 1059, 3218, 2916, 1765,...</td>\n",
       "      <td>[2743, 2624, 1310, 1743, 2616, 1986, 3325, 321...</td>\n",
       "      <td>[347, 3115, 2064, 910, 1731, 766, 1174, 751, 3...</td>\n",
       "      <td>[659, 3630, 23, 3019, 5, 1857, 3000, 1709, 297...</td>\n",
       "      <td>[3078, 1950, 2646, 1828, 1179, 2989, 754, 607,...</td>\n",
       "      <td>[807, 1239, 187, 1864, 708, 281, 1815, 912, 25...</td>\n",
       "      <td>[2048, 1020, 3419, 3548, 2307, 1198, 3666, 374...</td>\n",
       "      <td>[2839, 42, 1474, 3438, 2714, 1782, 1800, 2654,...</td>\n",
       "      <td>[3169, 3622, 2114, 1759, 897, 1391, 2783, 1845...</td>\n",
       "      <td>[3093, 272, 1783, 58, 728, 2075, 820, 1912, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  0    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.002792   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.056509   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.178758   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.027288   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           0.055865   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                            0.14476   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.136803   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.139233   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                            0.19205   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.392455   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.098254   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.198159   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.292771   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.215827   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.332904   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.112797   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                            0.19416   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.046904   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.270864   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.181064   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.061355   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.044053   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.088828   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.669642   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.275683   \n",
       "ids                                                 [2041, 3032, 496, 1522, 2068, 2994, 1911, 535,...   \n",
       "\n",
       "                                                                                                  1    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.214461   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.156315   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           -0.66987   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.010155   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.081093   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.074117   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.192393   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.073114   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.590967   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           -0.44349   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.332135   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.015774   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.775282   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.413406   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           -0.57791   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.256702   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.047386   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.511408   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.040701   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.579111   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                             -0.425   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -1.360904   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.398544   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.360336   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.026677   \n",
       "ids                                                 [1189, 2592, 146, 588, 1106, 3043, 2499, 1623,...   \n",
       "\n",
       "                                                                                                  2    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.137952   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.140347   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.163777   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.038804   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           -0.01777   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           0.040006   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.091636   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.120961   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.032571   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.188913   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.220692   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                            0.16524   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.022499   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.295059   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.129567   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.094027   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.230465   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.272433   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.186763   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.146162   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.079674   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.301462   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.425401   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.113424   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.115425   \n",
       "ids                                                 [1943, 3503, 2542, 1033, 469, 951, 243, 3315, ...   \n",
       "\n",
       "                                                                                                  3    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.092945   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.101002   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.060063   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.003856   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.029478   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.003116   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.044098   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.123164   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.005729   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.018775   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.053722   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.123532   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.046982   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.145862   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.163199   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.178481   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.254186   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.156861   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.231008   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           -0.02647   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.026072   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.172283   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.180796   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.119894   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.021991   \n",
       "ids                                                 [802, 2966, 2274, 99, 557, 2622, 3121, 349, 47...   \n",
       "\n",
       "                                                                                                  4    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.113937   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.337351   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.190005   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.204447   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.237979   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.344114   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.244484   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.346416   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.235471   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.391593   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.198889   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.235988   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.275537   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           -0.37301   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.474915   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.349706   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.142909   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.342042   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.341176   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.184742   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.219239   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.321999   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.367283   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.294114   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.575016   \n",
       "ids                                                 [630, 1832, 75, 3001, 2569, 3508, 407, 1796, 1...   \n",
       "\n",
       "                                                                                                  5    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.248649   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.438037   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.300544   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.210844   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.286873   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.356342   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.471105   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.051335   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           -0.84766   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.448656   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.476128   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.341662   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.774497   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.810405   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.852885   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -1.095097   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.618639   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.847866   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           -0.82451   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.917341   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.520825   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -1.233153   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -1.405324   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.396758   \n",
       "base_model.model.classifier.modules_to_save.def...                                           0.094797   \n",
       "ids                                                 [1784, 650, 1013, 485, 2294, 1403, 2546, 2885,...   \n",
       "\n",
       "                                                                                                  6    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.377446   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.197004   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.020764   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.103285   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           0.074688   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           0.184545   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.359509   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.070784   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.188607   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.582309   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.045115   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.164594   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.547008   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.613199   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.222831   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.310162   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.431471   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           1.089048   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.575566   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.250982   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.200233   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.350924   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.772277   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.061735   \n",
       "base_model.model.classifier.modules_to_save.def...                                           0.273732   \n",
       "ids                                                 [2269, 1569, 2621, 589, 2466, 498, 165, 2962, ...   \n",
       "\n",
       "                                                                                                  7    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           -0.55323   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.320979   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.143777   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.035615   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.318219   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           -0.23669   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.169028   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.128176   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.500709   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                            0.05325   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           -0.07363   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                            0.01735   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.199132   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                            0.18622   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.323512   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.234712   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.782424   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.798935   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.044544   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.722402   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.439619   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -1.241081   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.839124   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.573565   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.343231   \n",
       "ids                                                 [44, 978, 548, 778, 492, 122, 3657, 3603, 2243...   \n",
       "\n",
       "                                                                                                  8    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           -0.42092   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.644424   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.676488   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.329397   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.379533   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.592648   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.858913   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.906164   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -1.083541   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.966565   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.418666   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.971057   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -1.248056   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -1.093206   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -1.061247   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -1.082799   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.549205   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           -0.72829   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.825899   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           -1.16531   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.660791   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -1.040351   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.889908   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.068463   \n",
       "base_model.model.classifier.modules_to_save.def...                                           0.009197   \n",
       "ids                                                 [411, 1417, 2456, 136, 2566, 191, 1621, 1315, ...   \n",
       "\n",
       "                                                                                                  9    \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.162539   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                            -0.0436   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.024625   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           -0.14137   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           0.151725   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.005764   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.062963   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.005602   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.238263   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.250896   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.072858   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.036761   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.251957   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.175242   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.132374   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.218409   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.089142   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.621343   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.493065   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.232613   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.301812   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                            0.49249   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.230208   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.040185   \n",
       "base_model.model.classifier.modules_to_save.def...                                           -0.00229   \n",
       "ids                                                 [1161, 2547, 3322, 1420, 2909, 2362, 3098, 994...   \n",
       "\n",
       "                                                    ...  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.0.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.1.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.1.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.2.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.2.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.3.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.3.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.4.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.4.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.5.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.5.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.6.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.6.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.7.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.7.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.8.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.8.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.9.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.9.attent...  ...   \n",
       "base_model.model.deberta.encoder.layer.10.atten...  ...   \n",
       "base_model.model.deberta.encoder.layer.10.atten...  ...   \n",
       "base_model.model.deberta.encoder.layer.11.atten...  ...   \n",
       "base_model.model.deberta.encoder.layer.11.atten...  ...   \n",
       "base_model.model.classifier.modules_to_save.def...  ...   \n",
       "ids                                                 ...   \n",
       "\n",
       "                                                                                                  105  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.256942   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.297538   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.291554   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.157061   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.264797   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.277656   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.473988   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.260905   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.329472   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.244405   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.137125   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.036196   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.226381   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.238817   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           0.015765   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.298758   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.396075   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.659621   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.234266   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.414435   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.425284   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.548766   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           -0.45745   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.313851   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.321151   \n",
       "ids                                                 [1908, 1060, 747, 184, 1059, 3218, 2916, 1765,...   \n",
       "\n",
       "                                                                                                  106  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.060108   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.254762   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.210634   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.352993   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           0.023683   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.132468   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.036099   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.276575   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.047762   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.020122   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.196084   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.115006   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.086881   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.233841   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.594157   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.119643   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           -0.23969   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.154978   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.000444   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.446336   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.114967   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.374585   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.057926   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.113545   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.182191   \n",
       "ids                                                 [2743, 2624, 1310, 1743, 2616, 1986, 3325, 321...   \n",
       "\n",
       "                                                                                                  107  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           -0.30166   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.707609   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.045833   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.375491   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.265076   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.196689   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.369804   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.198805   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.617092   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.476967   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.538379   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.434189   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           -0.84264   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.572529   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -1.257513   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -1.133333   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -1.491969   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -1.130791   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -1.032613   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.540872   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -1.079712   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.698491   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -1.041163   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           -0.25877   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.392172   \n",
       "ids                                                 [347, 3115, 2064, 910, 1731, 766, 1174, 751, 3...   \n",
       "\n",
       "                                                                                                  108  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.587831   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.628391   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.283357   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.329159   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.063408   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.199692   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.006071   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.185212   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           -0.29318   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.314586   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.246166   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.004751   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           -0.15316   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.330001   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.883947   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.385864   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.714956   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.003262   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.820275   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.034245   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.553524   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.433522   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.479463   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.691484   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.423644   \n",
       "ids                                                 [659, 3630, 23, 3019, 5, 1857, 3000, 1709, 297...   \n",
       "\n",
       "                                                                                                  109  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.019633   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.296118   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.402362   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.029132   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.145172   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.262067   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.265601   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.193549   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.246804   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.023449   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.278526   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.134519   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.147685   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.359169   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.500247   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.523531   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.459515   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.291454   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.548061   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.372402   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.536183   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.986309   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.679992   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.986623   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.668879   \n",
       "ids                                                 [3078, 1950, 2646, 1828, 1179, 2989, 754, 607,...   \n",
       "\n",
       "                                                                                                  110  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.041786   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.033697   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           -0.40779   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.038246   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           -0.09048   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.134191   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.087226   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.247312   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.166606   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.238637   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.227817   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.161426   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.258888   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.289662   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                            -0.6013   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.261873   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.475683   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.144082   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.417693   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.040043   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.282182   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.392311   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.140148   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.264475   \n",
       "base_model.model.classifier.modules_to_save.def...                                           0.167471   \n",
       "ids                                                 [807, 1239, 187, 1864, 708, 281, 1815, 912, 25...   \n",
       "\n",
       "                                                                                                  111  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.036602   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                          -0.340017   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           -0.39959   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.058219   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.029833   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.168306   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.213986   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.362922   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.106733   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.594619   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.132502   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.569493   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.275184   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.482529   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.734604   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.558597   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.784999   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.714478   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -1.067562   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                            0.00375   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.594605   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.213531   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.382713   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.204267   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.071013   \n",
       "ids                                                 [2048, 1020, 3419, 3548, 2307, 1198, 3666, 374...   \n",
       "\n",
       "                                                                                                  112  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                            0.03769   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.095167   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.281333   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.000214   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           0.017699   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.028317   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.179511   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                          -0.083613   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.248479   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.246472   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.352544   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.029103   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.216572   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           -0.30142   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.290117   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           -0.17902   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.588387   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.657897   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           -0.77619   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.500791   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.542457   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.799849   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.512454   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.015437   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.110323   \n",
       "ids                                                 [2839, 42, 1474, 3438, 2714, 1782, 1800, 2654,...   \n",
       "\n",
       "                                                                                                  113  \\\n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.091819   \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.067727   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.205976   \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.007177   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.004159   \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.111756   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.010667   \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.069094   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                            0.15717   \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                           0.107189   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.047346   \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                           0.055847   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                           0.291677   \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.002135   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.102028   \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.098232   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           -0.25317   \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                           0.350278   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.131101   \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                           0.162658   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.106237   \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                           0.096928   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.129889   \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           0.305324   \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.154488   \n",
       "ids                                                 [3169, 3622, 2114, 1759, 897, 1391, 2783, 1845...   \n",
       "\n",
       "                                                                                                  114  \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.160528  \n",
       "base_model.model.deberta.encoder.layer.0.attent...                                           0.054598  \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                           0.327814  \n",
       "base_model.model.deberta.encoder.layer.1.attent...                                          -0.036041  \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                           0.069041  \n",
       "base_model.model.deberta.encoder.layer.2.attent...                                          -0.037405  \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.200177  \n",
       "base_model.model.deberta.encoder.layer.3.attent...                                           0.139744  \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.134625  \n",
       "base_model.model.deberta.encoder.layer.4.attent...                                          -0.046298  \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.428009  \n",
       "base_model.model.deberta.encoder.layer.5.attent...                                          -0.218948  \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.409051  \n",
       "base_model.model.deberta.encoder.layer.6.attent...                                          -0.293762  \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                           -0.37198  \n",
       "base_model.model.deberta.encoder.layer.7.attent...                                          -0.274255  \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.401181  \n",
       "base_model.model.deberta.encoder.layer.8.attent...                                          -0.294913  \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.757706  \n",
       "base_model.model.deberta.encoder.layer.9.attent...                                          -0.446702  \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.552031  \n",
       "base_model.model.deberta.encoder.layer.10.atten...                                          -0.403259  \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                          -0.168923  \n",
       "base_model.model.deberta.encoder.layer.11.atten...                                           -0.14024  \n",
       "base_model.model.classifier.modules_to_save.def...                                          -0.363421  \n",
       "ids                                                 [3093, 272, 1783, 58, 728, 2075, 820, 1912, 15...  \n",
       "\n",
       "[26 rows x 115 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d6cca0c",
   "metadata": {},
   "source": [
    "## Attributes of influence_engine\n",
    "There are a couple of useful attributes in `influence_engine`. For intance, to compare the runtime, one case use `time_dict`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9f7b3",
   "metadata": {},
   "source": [
    "`IF_dict` includes all the computed influence function values. Here, `identity` indicates the `Hessian-free` influence computation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1371a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding:utf-8 -*-\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "print(torch.__version__)  # 1.7.1\n",
    "print(transformers.__version__) # 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa121b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    DebertaV2ForSequenceClassification, DebertaV2Tokenizer, AdamW, get_linear_schedule_with_warmup,AutoTokenizer\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/yanmy/model/deberta-v3-base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(influence_engine.IF_dict,'../../HyperINF-main/output/output_fp16_deberta_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_engine.IF_dict['identity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c866c",
   "metadata": {},
   "source": [
    "## retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "result = torch.load('/home/yanmy/HyperINF-main/output/output_fp16_deberta.pkl')\n",
    "index = np.argsort(result['proposed'])[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_json('../ER/semi-text-c/train.json').iloc[index,-1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.argsort(result['proposed'])[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.load('/home/yanmy/HyperINF-main/output/output_deberta_fp16_ER.pkl')\n",
    "b = np.argsort(result['proposed'])[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(a).intersection(set(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "result = torch.load('/home/yanmy/HyperINF-main/output/output_fp16_deberta_ER_v_proj_inv.pkl')\n",
    "index = np.argsort(result['iterative'])[:1500]\n",
    "import pandas as pd\n",
    "pd.read_json('../ER/semi-text-c/train.json').iloc[index,-1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60952b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"/home/yanmy/model/deberta-v3-base\" \n",
    "# task=\"mrpc\"\n",
    "task = \"ER\"\n",
    "noise_ratio=0\n",
    "batch_size=32\n",
    "# target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "target_modules=[\"value_proj\"] ## deberta, for roberta, use [\"value\"]\n",
    "\n",
    "device=\"cuda:3\"\n",
    "num_epochs=10\n",
    "lr=3e-4\n",
    "\n",
    "\n",
    "dataloader_outputs = create_dataloaders(model_name_or_path=model_name_or_path,\n",
    "                                           task=task,\n",
    "                                           noise_ratio=noise_ratio,\n",
    "                                           batch_size=batch_size,\n",
    "                                           train_file = '../ER/semi-text-c/train.json',\n",
    "                                           valid_file = '../ER/semi-text-c/valid.json',\n",
    "                                           test_file = '../ER/semi-text-c/test.json',\n",
    "                                           max_length=256,\n",
    "                                           select_index=index)\n",
    "# train_dataloader, eval_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "train_dataloader, eval_dataloader,test_dataloader, noise_index, tokenized_datasets, collate_fn=dataloader_outputs\n",
    "\n",
    "\n",
    "# lora_engine = LORAEngine(model_name_or_path=model_name_or_path,\n",
    "#                             target_modules=target_modules,\n",
    "#                             train_dataloader=train_dataloader,\n",
    "#                             eval_dataloader=eval_dataloader,\n",
    "#                             device=device,\n",
    "#                             num_epochs=num_epochs,\n",
    "#                             lr=lr,\n",
    "#                             low_rank=8, \n",
    "#                             task=task)\n",
    "\n",
    "lora_engine = LORAEngineDebertaMultiClass(model_name_or_path=model_name_or_path,\n",
    "                            target_modules=target_modules,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            # eval_dataloader=eval_dataloader,\n",
    "                            eval_dataloader=eval_dataloader,\n",
    "                            test_dataloader = test_dataloader,\n",
    "                            device=device,\n",
    "                            num_epochs=num_epochs,\n",
    "                            lr=lr,\n",
    "                            low_rank=8, \n",
    "                            task=task,\n",
    "                            save_path = '../models/ER',\n",
    "                            valid_each_epoch = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a18ce",
   "metadata": {},
   "source": [
    "`output/output_fp16_deberta.pkl`\n",
    "- 1000: {'f1': 0.7783063748810657}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66adb5a",
   "metadata": {},
   "source": [
    "## Application to mislabeled data detection task\n",
    "- We compare the mislabeled data detection ability of different influence computation methods. Given that large influence function values are likely to increase the validation loss, data points with large influence fucntion values are desired to be mislabeled. \n",
    "- We inspect data points from the largest to lowest influence function values and evaluate the detection rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ab5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=influence_engine.n_train\n",
    "true_label=np.zeros(n_train)\n",
    "true_label[noise_index]=1\n",
    "\n",
    "method_dict={'identity': 'Hessian-free',\n",
    "            'proposed': 'DataInf',\n",
    "            'LiSSA': 'LiSSA'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0335e",
   "metadata": {},
   "source": [
    "Requirement already satisfied: torch in /home/yanmy/anaconda3/lib/python3.9/site-packages (1.13.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44833374",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "for method in influence_engine.IF_dict:\n",
    "    detection_rate_list=[]\n",
    "    low_quality_to_high_quality=np.argsort(influence_engine.IF_dict[method])[::-1]\n",
    "    for ind in range(1, len(low_quality_to_high_quality)+1):\n",
    "        detected_samples = set(low_quality_to_high_quality[:ind]).intersection(noise_index)\n",
    "        detection_rate = 100*len(detected_samples)/len(noise_index)\n",
    "        detection_rate_list.append(detection_rate)\n",
    "    plt.plot(100*np.arange(len(low_quality_to_high_quality))/n_train, \n",
    "             detection_rate_list,\n",
    "             label=method_dict[method])\n",
    "plt.xlabel('Data inspected (%)', fontsize=18)\n",
    "plt.ylabel('Detection Rate (%)', fontsize=18)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('Mislabeled Data Detection', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e60ae-368c-4bc1-977f-5deeaf5fe862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3332f3a-f5e0-4ce4-9748-3d51f161bb72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
