{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "def process_datasets(config_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    遍历给定文件夹内的 YAML 配置文件，处理相应的 JSON 文件，并生成 JSONL 格式的输出。\n",
    "\n",
    "    Args:\n",
    "        config_folder (str): 包含 YAML 配置文件的文件夹路径。\n",
    "        output_folder (str): 用于保存新生成的 JSONL 文件的文件夹路径。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    for root, _, files in os.walk(config_folder):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.yaml') and 'config_' in file_name:\n",
    "                yaml_path = os.path.join(root, file_name)\n",
    "                \n",
    "                print(f\"\\nProcessing YAML file: {yaml_path}\")\n",
    "                try:\n",
    "                    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "                        yaml_data = yaml.safe_load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading YAML file {yaml_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if 'train_file_path' in yaml_data:\n",
    "                    json_path = yaml_data['train_file_path']\n",
    "                    dataset_name = yaml_data.get('dataset', 'unknown_dataset')\n",
    "                    task_name = yaml_data.get('task', 'unknown_task')\n",
    "                    \n",
    "                    full_dataset_name = f\"{task_name}_{dataset_name}\"\n",
    "                    \n",
    "                    if not os.path.exists(json_path):\n",
    "                        print(f\"Warning: JSON file not found at {json_path}. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    print(f\"  -> Reading JSON file: {json_path}\")\n",
    "                    try:\n",
    "                        df = pd.read_json(json_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading JSON file {json_path}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 检查所需的列是否存在\n",
    "                    if 'instruction' not in df.columns or 'output' not in df.columns:\n",
    "                        print(f\"Warning: JSON file {json_path} is missing 'instruction' or 'output' column. Skipping.\")\n",
    "                        continue\n",
    "                        \n",
    "                    # 创建新的 JSONL 文件名\n",
    "                    full_output_file = os.path.join(output_folder, f\"{full_dataset_name}_full.jsonl\")\n",
    "                    subset_output_file = os.path.join(output_folder, f\"{full_dataset_name}_subset.jsonl\")\n",
    "                    \n",
    "                    # 生成完整数据集\n",
    "                    full_data_list = []\n",
    "                    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"  -> Formatting data\"):\n",
    "                        new_entry = {\n",
    "                            \"dataset\": full_dataset_name,\n",
    "                            \"id\": f\"{full_dataset_name}_{index}\",\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": row['instruction']\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"assistant\",\n",
    "                                    \"content\": row['output']\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                        full_data_list.append(new_entry)\n",
    "                    \n",
    "                    # 保存完整数据集\n",
    "                    with open(full_output_file, 'w', encoding='utf-8') as f:\n",
    "                        for item in full_data_list:\n",
    "                            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                    print(f\"  -> Successfully created full JSONL file: {full_output_file}\")\n",
    "                    \n",
    "                    # 随机抽取1%的数据样本\n",
    "                    sample_size = max(1, int(len(full_data_list) * 0.3))\n",
    "                    subset_data_list = random.sample(full_data_list, sample_size)\n",
    "                    \n",
    "                    # 保存子集\n",
    "                    with open(subset_output_file, 'w', encoding='utf-8') as f:\n",
    "                        for item in subset_data_list:\n",
    "                            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                    print(f\"  -> Successfully created 1% subset JSONL file: {subset_output_file}\")\n",
    "def merge_jsonl_files(input_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    将指定文件夹内的所有 full.jsonl 和 subset.jsonl 文件合并。\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): 包含要合并的 JSONL 文件的文件夹路径。\n",
    "        output_folder (str): 用于保存合并后的文件的文件夹路径。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    # 合并 full.jsonl 文件\n",
    "    full_output_path = os.path.join(output_folder, 'full.jsonl')\n",
    "    full_file_list = [f for f in os.listdir(input_folder) if f.endswith('_full.jsonl')]\n",
    "    \n",
    "    if not full_file_list:\n",
    "        print(\"No files ending with '_full.jsonl' were found. Skipping full file merge.\")\n",
    "    else:\n",
    "        print(f\"Found {len(full_file_list)} full files to merge.\")\n",
    "        with open(full_output_path, 'w', encoding='utf-8') as outfile:\n",
    "            for file_name in full_file_list:\n",
    "                file_path = os.path.join(input_folder, file_name)\n",
    "                print(f\"  -> Merging: {file_name}\")\n",
    "                with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                    for line in infile:\n",
    "                        # 逐行读取并写入到新的文件中\n",
    "                        outfile.write(line)\n",
    "        print(f\"Successfully merged all full files into: {full_output_path}\")\n",
    "        \n",
    "    # ---\n",
    "\n",
    "    # 合并 subset.jsonl 文件\n",
    "    subset_output_path = os.path.join(output_folder, 'subset.jsonl')\n",
    "    subset_file_list = [f for f in os.listdir(input_folder) if f.endswith('_subset.jsonl')]\n",
    "\n",
    "    if not subset_file_list:\n",
    "        print(\"No files ending with '_subset.jsonl' were found. Skipping subset file merge.\")\n",
    "    else:\n",
    "        print(f\"Found {len(subset_file_list)} subset files to merge.\")\n",
    "        with open(subset_output_path, 'w', encoding='utf-8') as outfile:\n",
    "            for file_name in subset_file_list:\n",
    "                file_path = os.path.join(input_folder, file_name)\n",
    "                print(f\"  -> Merging: {file_name}\")\n",
    "                with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                    for line in infile:\n",
    "                        # 逐行读取并写入到新的文件中\n",
    "                        outfile.write(line)\n",
    "        print(f\"Successfully merged all subset files into: {subset_output_path}\")\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_datasets(config_folder: str, output_folder: str):\n",
    "    \"\"\"\n",
    "    Traverses YAML configuration files in a given folder, processes the corresponding\n",
    "    JSON test files, and generates a 1% random subset in JSONL format,\n",
    "    following the 'task-dataset-test-subset.jsonl' naming convention.\n",
    "\n",
    "    Args:\n",
    "        config_folder (str): The path to the folder containing the YAML configuration files.\n",
    "        output_folder (str): The path to the folder where the new JSONL files will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "\n",
    "    for root, _, files in os.walk(config_folder):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.yaml') and 'config_' in file_name:\n",
    "                yaml_path = os.path.join(root, file_name)\n",
    "\n",
    "                print(f\"\\nProcessing YAML file: {yaml_path}\")\n",
    "                try:\n",
    "                    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "                        yaml_data = yaml.safe_load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading YAML file {yaml_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if 'test_file_path' in yaml_data:\n",
    "                    json_path = yaml_data['test_file_path']\n",
    "                    dataset_name = yaml_data.get('dataset', 'unknown_dataset')\n",
    "                    task_name = yaml_data.get('task', 'unknown_task')\n",
    "\n",
    "                    # Create a unified dataset name from task and dataset\n",
    "                    unified_dataset_name = f\"{task_name}_{dataset_name}\"\n",
    "                    \n",
    "                    if not os.path.exists(json_path):\n",
    "                        print(f\"Warning: JSON file not found at {json_path}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    print(f\"  -> Reading JSON file: {json_path}\")\n",
    "                    try:\n",
    "                        df = pd.read_json(json_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading JSON file {json_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # Check for required columns\n",
    "                    if 'instruction' not in df.columns or 'output' not in df.columns:\n",
    "                        print(f\"Warning: JSON file {json_path} is missing 'instruction' or 'output' column. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Generate the output file name according to the specified standard\n",
    "                    output_file_name = f\"{task_name}-{dataset_name}-test-subset.jsonl\"\n",
    "                    output_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "                    # Generate the full data list for sampling\n",
    "                    full_data_list = []\n",
    "                    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"  -> Formatting data\"):\n",
    "                        new_entry = {\n",
    "                            \"dataset\": unified_dataset_name,\n",
    "                            \"id\": f\"{unified_dataset_name}_{index}\",\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": row['instruction']\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"assistant\",\n",
    "                                    \"content\": row['output']\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                        full_data_list.append(new_entry)\n",
    "\n",
    "                    # Randomly sample 1% of the data\n",
    "                    if not full_data_list:\n",
    "                        print(f\"Warning: No data found in {json_path}. Skipping subset creation.\")\n",
    "                        continue\n",
    "\n",
    "                    sample_size = max(1, int(len(full_data_list) * 0.01))\n",
    "                    subset_data_list = random.sample(full_data_list, sample_size)\n",
    "\n",
    "                    # Save the subset to the new file\n",
    "                    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                        for item in subset_data_list:\n",
    "                            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                    print(f\"  -> Successfully created 1% subset JSONL file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1524727/3849334575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'script/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../LESS/LESS_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'process_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "process_datasets('script/','../LESS/LESS_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing YAML file: script/config_CTA_WebTable.yaml\n",
      "  -> Reading JSON file: train/CTA/WebTable/WebTable-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e769c5d08d40409e86a0dfa67c9982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/17709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/CTA_WebTable_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/CTA_WebTable_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_RE_RE.yaml\n",
      "  -> Reading JSON file: train/RE/RE/RE-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483d4066798841d19f23176dd55e7a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/2072 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/RE_RE_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/RE_RE_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_wdc.yaml\n",
      "  -> Reading JSON file: train/ER/wdc/wdc-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb81eb03a0b74bd5b9095b5c3a7fcb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/4398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/ER_wdc_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER_wdc_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_semi-text-w.yaml\n",
      "  -> Reading JSON file: train/ER/semi-text-w/semi-text-w-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcb7b0afc82427a99f421deb6babca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/1846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/ER_semi-text-w_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER_semi-text-w_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_semi-text-c.yaml\n",
      "  -> Reading JSON file: train/ER/semi-text-c/semi-text-c-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99a3bb545ae44688923ef03fbf9ab2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/4179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/ER_semi-text-c_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER_semi-text-c_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_abt-buy.yaml\n",
      "  -> Reading JSON file: train/ER/abt-buy/abt-buy-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d215979fac464a52b79d31eb2a1755b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/1916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/ER_abt-buy_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER_abt-buy_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_amazon-google.yaml\n",
      "  -> Reading JSON file: train/ER/amazon-google/amazon-google-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f954c468fedd4ba19af80ad4d92c7a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/2289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/ER_amazon-google_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER_amazon-google_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_ER_walmart-amazon.yaml\n",
      "  -> Reading JSON file: train/ER/walmart-amazon/walmart-amazon-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b644f69fb0748b1acf2c2990e13e0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/2049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/ER_walmart-amazon_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/ER_walmart-amazon_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DC_hospital.yaml\n",
      "  -> Reading JSON file: train/DC/hospital/hospital-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853d8b66f31249b2b219aefecc9d8e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/DC_hospital_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DC_hospital_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DC_rayyan.yaml\n",
      "  -> Reading JSON file: train/DC/rayyan/rayyan-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d227ceddedd44dbb1f82d5b91a0fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/1117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/DC_rayyan_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DC_rayyan_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DC_beer.yaml\n",
      "  -> Reading JSON file: train/DC/beer/beer-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82c7bb410a049fbbcdacec05e4c1d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/3364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/DC_beer_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DC_beer_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DI_walmart.yaml\n",
      "  -> Reading JSON file: train/DI/walmart/walmart-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d5297c63b2484bb1ea1aa720b83942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/DI_walmart_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DI_walmart_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_DI_amazon.yaml\n",
      "  -> Reading JSON file: train/DI/amazon/amazon-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dc2d3a93684858b19ca0b1a32568d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/DI_amazon_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/DI_amazon_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_SM_CMS.yaml\n",
      "  -> Reading JSON file: train/SM/CMS/CMS-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14402c5d69144c0923705f7cebb2b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/5127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/SM_CMS_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/SM_CMS_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_AVE_oa_mine.yaml\n",
      "  -> Reading JSON file: train/AVE/oa_mine/oa_mine-test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ad9745143f4352a72b3ca2d94da92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/2451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/AVE_oa_mine_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/AVE_oa_mine_test_subset.jsonl\n",
      "\n",
      "Processing YAML file: script/config_CTA_SimTab.yaml\n",
      "  -> Reading JSON file: /data/home/wangys/MELD/dataset/CTA/SimTab_test_few.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492c024f0e2e41318117696bb3e66458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  -> Formatting data:   0%|          | 0/7610 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully created full JSONL file: ../LESS/LESS_data/CTA_SimTab_test_full.jsonl\n",
      "  -> Successfully created 1% subset JSONL file: ../LESS/LESS_data/CTA_SimTab_test_subset.jsonl\n"
     ]
    }
   ],
   "source": [
    "process_datasets_test('script/','../LESS/LESS_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_jsonl_files('LESS_data','LESS_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_0</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_1</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_2</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_3</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTA_WebTable</td>\n",
       "      <td>CTA_WebTable_4</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99777</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_6364</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99778</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_6365</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99779</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_6366</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99780</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_6367</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99781</th>\n",
       "      <td>CTA_SimTab</td>\n",
       "      <td>CTA_SimTab_6368</td>\n",
       "      <td>[{'role': 'user', 'content': 'You are an exper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99782 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dataset               id  \\\n",
       "0      CTA_WebTable   CTA_WebTable_0   \n",
       "1      CTA_WebTable   CTA_WebTable_1   \n",
       "2      CTA_WebTable   CTA_WebTable_2   \n",
       "3      CTA_WebTable   CTA_WebTable_3   \n",
       "4      CTA_WebTable   CTA_WebTable_4   \n",
       "...             ...              ...   \n",
       "99777    CTA_SimTab  CTA_SimTab_6364   \n",
       "99778    CTA_SimTab  CTA_SimTab_6365   \n",
       "99779    CTA_SimTab  CTA_SimTab_6366   \n",
       "99780    CTA_SimTab  CTA_SimTab_6367   \n",
       "99781    CTA_SimTab  CTA_SimTab_6368   \n",
       "\n",
       "                                                messages  \n",
       "0      [{'role': 'user', 'content': 'You are an exper...  \n",
       "1      [{'role': 'user', 'content': 'You are an exper...  \n",
       "2      [{'role': 'user', 'content': 'You are an exper...  \n",
       "3      [{'role': 'user', 'content': 'You are an exper...  \n",
       "4      [{'role': 'user', 'content': 'You are an exper...  \n",
       "...                                                  ...  \n",
       "99777  [{'role': 'user', 'content': 'You are an exper...  \n",
       "99778  [{'role': 'user', 'content': 'You are an exper...  \n",
       "99779  [{'role': 'user', 'content': 'You are an exper...  \n",
       "99780  [{'role': 'user', 'content': 'You are an exper...  \n",
       "99781  [{'role': 'user', 'content': 'You are an exper...  \n",
       "\n",
       "[99782 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('LESS_data/full.jsonl',lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
